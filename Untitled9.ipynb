{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHKQRdbQQHRGSCmus85Ot5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nbilasals/air_quality_data/blob/main/Untitled9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLonJCBNX7CK"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FEATURE SELECTION FOR MODELING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get all encoded features and numerical features\n",
        "encoded_features = [col for col in df_encoded.columns if col.endswith('_encoded')]\n",
        "original_numerical = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak', 'FastingBS']\n",
        "engineered_numerical = ['HR_Achievement_Pct', 'Simple_Risk_Score']\n",
        "\n",
        "# Combine features\n",
        "feature_cols = [col for col in original_numerical if col in df_encoded.columns]\n",
        "feature_cols.extend([col for col in encoded_features if col in df_encoded.columns])\n",
        "feature_cols.extend([col for col in engineered_numerical if col in df_encoded.columns])\n",
        "\n",
        "# Remove target if accidentally included\n",
        "if target_col in feature_cols:\n",
        "    feature_cols.remove(target_col)\n",
        "\n",
        "print(f\"Selected {len(feature_cols)} features for modeling:\")\n",
        "for i, col in enumerate(feature_cols, 1):\n",
        "    print(f\"  {i:2d}. {col}\")\n",
        "\n",
        "# Create feature matrix and target vector\n",
        "X = df_encoded[feature_cols].fillna(0)  # Fill any remaining NaN with 0\n",
        "y = df_encoded[target_col]\n",
        "\n",
        "print(f\"\\nâœ“ Feature matrix (X): {X.shape[0]} patients Ã— {X.shape[1]} features\")\n",
        "print(f\"âœ“ Target vector (y): {y.shape[0]} patients\")\n",
        "print(f\"âœ“ Class distribution:\")\n",
        "print(f\"   - Class 0 (No Disease): {(y == 0).sum()} ({(y == 0).sum()/len(y)*100:.1f}%)\")\n",
        "print(f\"   - Class 1 (Disease):    {(y == 1).sum()} ({(y == 1).sum()/len(y)*100:.1f}%)\")\n",
        "\n",
        "# ### 5.2 Feature Importance Analysis (Preliminary)\n",
        "\n",
        "\"\"\"\n",
        "Use Random Forest to identify most important features before modeling\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PRELIMINARY FEATURE IMPORTANCE ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Train Random Forest for feature importance\n",
        "rf_temp = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "rf_temp.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_cols,\n",
        "    'Importance': rf_temp.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 15 Most Important Features:\")\n",
        "print(feature_importance_df.head(15).to_string(index=False))\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(12, 8))\n",
        "top_15 = feature_importance_df.head(15)\n",
        "plt.barh(range(len(top_15)), top_15['Importance'], color='steelblue',\n",
        "        edgecolor='black', alpha=0.8)\n",
        "plt.yticks(range(len(top_15)), top_15['Feature'])\n",
        "plt.xlabel('Importance Score')\n",
        "plt.title('Top 15 Most Important Features (Random Forest)', fontsize=14, fontweight='bold')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---\n",
        "# ## 6. Data Splitting and Scaling\n",
        "# ### 6.1 Train-Test Split\n",
        "\n",
        "\"\"\"\n",
        "Split data into training and testing sets with stratification\n",
        "to maintain class distribution\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAIN-TEST SPLIT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"âœ“ Training set: {len(X_train)} patients ({len(X_train)/len(X)*100:.1f}%)\")\n",
        "print(f\"âœ“ Test set: {len(X_test)} patients ({len(X_test)/len(X)*100:.1f}%)\")\n",
        "print(f\"âœ“ Features: {X_train.shape[1]}\")\n",
        "\n",
        "print(f\"\\nClass distribution in training set:\")\n",
        "print(f\"   - Class 0: {(y_train == 0).sum()} ({(y_train == 0).sum()/len(y_train)*100:.1f}%)\")\n",
        "print(f\"   - Class 1: {(y_train == 1).sum()} ({(y_train == 1).sum()/len(y_train)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nClass distribution in test set:\")\n",
        "print(f\"   - Class 0: {(y_test == 0).sum()} ({(y_test == 0).sum()/len(y_test)*100:.1f}%)\")\n",
        "print(f\"   - Class 1: {(y_test == 1).sum()} ({(y_test == 1).sum()/len(y_test)*100:.1f}%)\")\n",
        "\n",
        "# ### 6.2 Feature Scaling\n",
        "\n",
        "\"\"\"\n",
        "Standardize features to have zero mean and unit variance.\n",
        "Critical for distance-based algorithms like SVM and KNN.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FEATURE SCALING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"âœ“ Features scaled using StandardScaler\")\n",
        "print(f\"  Training set: Mean â‰ˆ 0, Std â‰ˆ 1 for all features\")\n",
        "\n",
        "# Convert back to DataFrame for easier handling\n",
        "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=feature_cols, index=X_train.index)\n",
        "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=feature_cols, index=X_test.index)\n",
        "\n",
        "# ---\n",
        "# ## 7. Model Training and Evaluation\n",
        "# ### 7.1 Model 1: Logistic Regression\n",
        "\n",
        "\"\"\"\n",
        "Baseline Model: Logistic Regression\n",
        "- Simple, interpretable, fast\n",
        "- Good for linear relationships\n",
        "- Provides probability estimates\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL 1: LOGISTIC REGRESSION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "lr_model = LogisticRegression(random_state=42, max_iter=1000, solver='lbfgs')\n",
        "lr_model.fit(X_train_scaled_df, y_train)\n",
        "\n",
        "y_pred_lr = lr_model.predict(X_test_scaled_df)\n",
        "y_pred_proba_lr = lr_model.predict_proba(X_test_scaled_df)[:, 1]\n",
        "\n",
        "# Evaluate\n",
        "lr_accuracy = accuracy_score(y_test, y_pred_lr)\n",
        "lr_precision = precision_score(y_test, y_pred_lr)\n",
        "lr_recall = recall_score(y_test, y_pred_lr)\n",
        "lr_f1 = f1_score(y_test, y_pred_lr)\n",
        "lr_auc = roc_auc_score(y_test, y_pred_proba_lr)\n",
        "\n",
        "print(f\"\\nPerformance Metrics:\")\n",
        "print(f\"  â€¢ Accuracy:  {lr_accuracy:.4f} ({lr_accuracy*100:.2f}%)\")\n",
        "print(f\"  â€¢ Precision: {lr_precision:.4f} (of predicted positives, {lr_precision*100:.1f}% are correct)\")\n",
        "print(f\"  â€¢ Recall:    {lr_recall:.4f} (detected {lr_recall*100:.1f}% of actual disease cases)\")\n",
        "print(f\"  â€¢ F1-Score:  {lr_f1:.4f} (harmonic mean of precision and recall)\")\n",
        "print(f\"  â€¢ ROC-AUC:   {lr_auc:.4f} (area under ROC curve)\")\n",
        "\n",
        "# ### 7.2 Model 2: Random Forest Classifier\n",
        "\n",
        "\"\"\"\n",
        "Ensemble Model: Random Forest\n",
        "- Handles non-linear relationships\n",
        "- Robust to outliers\n",
        "- Provides feature importance\n",
        "- Less prone to overfitting\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL 2: RANDOM FOREST CLASSIFIER\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    min_samples_split=10,\n",
        "    min_samples_leaf=4,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate\n",
        "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
        "rf_precision = precision_score(y_test, y_pred_rf)\n",
        "rf_recall = recall_score(y_test, y_pred_rf)\n",
        "rf_f1 = f1_score(y_test, y_pred_rf)\n",
        "rf_auc = roc_auc_score(y_test, y_pred_proba_rf)\n",
        "\n",
        "print(f\"\\nPerformance Metrics:\")\n",
        "print(f\"  â€¢ Accuracy:  {rf_accuracy:.4f} ({rf_accuracy*100:.2f}%)\")\n",
        "print(f\"  â€¢ Precision: {rf_precision:.4f} (of predicted positives, {rf_precision*100:.1f}% are correct)\")\n",
        "print(f\"  â€¢ Recall:    {rf_recall:.4f} (detected {rf_recall*100:.1f}% of actual disease cases)\")\n",
        "print(f\"  â€¢ F1-Score:  {rf_f1:.4f}\")\n",
        "print(f\"  â€¢ ROC-AUC:   {rf_auc:.4f}\")\n",
        "\n",
        "# ### 7.3 Model 3: Support Vector Machine (SVM)\n",
        "\n",
        "\"\"\"\n",
        "Support Vector Machine with RBF Kernel\n",
        "- Effective in high-dimensional spaces\n",
        "- Good for non-linear classification\n",
        "- Memory efficient\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL 3: SUPPORT VECTOR MACHINE (SVM)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "svm_model = SVC(kernel='rbf', probability=True, random_state=42, C=1.0, gamma='scale')\n",
        "svm_model.fit(X_train_scaled_df, y_train)\n",
        "\n",
        "y_pred_svm = svm_model.predict(X_test_scaled_df)\n",
        "y_pred_proba_svm = svm_model.predict_proba(X_test_scaled_df)[:, 1]\n",
        "\n",
        "# Evaluate\n",
        "svm_accuracy = accuracy_score(y_test, y_pred_svm)\n",
        "svm_precision = precision_score(y_test, y_pred_svm)\n",
        "svm_recall = recall_score(y_test, y_pred_svm)\n",
        "svm_f1 = f1_score(y_test, y_pred_svm)\n",
        "svm_auc = roc_auc_score(y_test, y_pred_proba_svm)\n",
        "\n",
        "print(f\"\\nPerformance Metrics:\")\n",
        "print(f\"  â€¢ Accuracy:  {svm_accuracy:.4f} ({svm_accuracy*100:.2f}%)\")\n",
        "print(f\"  â€¢ Precision: {svm_precision:.4f} (of predicted positives, {svm_precision*100:.1f}% are correct)\")\n",
        "print(f\"  â€¢ Recall:    {svm_recall:.4f} (detected {svm_recall*100:.1f}% of actual disease cases)\")\n",
        "print(f\"  â€¢ F1-Score:  {svm_f1:.4f}\")\n",
        "print(f\"  â€¢ ROC-AUC:   {svm_auc:.4f}\")\n",
        "\n",
        "# ### 7.4 Model 4: Gradient Boosting Classifier\n",
        "\n",
        "\"\"\"\n",
        "Gradient Boosting: Advanced Ensemble Method\n",
        "- Sequential learning\n",
        "- Often achieves high accuracy\n",
        "- Handles complex patterns\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL 4: GRADIENT BOOSTING CLASSIFIER\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "gb_model = GradientBoostingClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=5,\n",
        "    min_samples_split=10,\n",
        "    random_state=42\n",
        ")\n",
        "gb_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_gb = gb_model.predict(X_test)\n",
        "y_pred_proba_gb = gb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate\n",
        "gb_accuracy = accuracy_score(y_test, y_pred_gb)\n",
        "gb_precision = precision_score(y_test, y_pred_gb)\n",
        "gb_recall = recall_score(y_test, y_pred_gb)\n",
        "gb_f1 = f1_score(y_test, y_pred_gb)\n",
        "gb_auc = roc_auc_score(y_test, y_pred_proba_gb)\n",
        "\n",
        "print(f\"\\nPerformance Metrics:\")\n",
        "print(f\"  â€¢ Accuracy:  {gb_accuracy:.4f} ({gb_accuracy*100:.2f}%)\")\n",
        "print(f\"  â€¢ Precision: {gb_precision:.4f} (of predicted positives, {gb_precision*100:.1f}% are correct)\")\n",
        "print(f\"  â€¢ Recall:    {gb_recall:.4f} (detected {gb_recall*100:.1f}% of actual disease cases)\")\n",
        "print(f\"  â€¢ F1-Score:  {gb_f1:.4f}\")\n",
        "print(f\"  â€¢ ROC-AUC:   {gb_auc:.4f}\")\n",
        "\n",
        "# ### 7.5 Model 5: K-Nearest Neighbors (KNN)\n",
        "\n",
        "\"\"\"\n",
        "K-Nearest Neighbors\n",
        "- Non-parametric, instance-based learning\n",
        "- Simple intuition: similar patients have similar outcomes\n",
        "- No training phase (lazy learning)\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL 5: K-NEAREST NEIGHBORS (KNN)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5, weights='distance', metric='euclidean')\n",
        "knn_model.fit(X_train_scaled_df, y_train)\n",
        "\n",
        "y_pred_knn = knn_model.predict(X_test_scaled_df)\n",
        "y_pred_proba_knn = knn_model.predict_proba(X_test_scaled_df)[:, 1]\n",
        "\n",
        "# Evaluate\n",
        "knn_accuracy = accuracy_score(y_test, y_pred_knn)\n",
        "knn_precision = precision_score(y_test, y_pred_knn)\n",
        "knn_recall = recall_score(y_test, y_pred_knn)\n",
        "knn_f1 = f1_score(y_test, y_pred_knn)\n",
        "knn_auc = roc_auc_score(y_test, y_pred_proba_knn)\n",
        "\n",
        "print(f\"\\nPerformance Metrics:\")\n",
        "print(f\"  â€¢ Accuracy:  {knn_accuracy:.4f} ({knn_accuracy*100:.2f}%)\")\n",
        "print(f\"  â€¢ Precision: {knn_precision:.4f} (of predicted positives, {knn_precision*100:.1f}% are correct)\")\n",
        "print(f\"  â€¢ Recall:    {knn_recall:.4f} (detected {knn_recall*100:.1f}% of actual disease cases)\")\n",
        "print(f\"  â€¢ F1-Score:  {knn_f1:.4f}\")\n",
        "print(f\"  â€¢ ROC-AUC:   {knn_auc:.4f}\")\n",
        "\n",
        "# ### 7.6 Model 6: Naive Bayes\n",
        "\n",
        "\"\"\"\n",
        "Gaussian Naive Bayes\n",
        "- Probabilistic classifier\n",
        "- Fast and efficient\n",
        "- Works well with small datasets\n",
        "- Assumes feature independence\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL 6: NAIVE BAYES (GAUSSIAN)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train_scaled_df, y_train)\n",
        "\n",
        "y_pred_nb = nb_model.predict(X_test_scaled_df)\n",
        "y_pred_proba_nb = nb_model.predict_proba(X_test_scaled_df)[:, 1]\n",
        "\n",
        "# Evaluate\n",
        "nb_accuracy = accuracy_score(y_test, y_pred_nb)\n",
        "nb_precision = precision_score(y_test, y_pred_nb)\n",
        "nb_recall = recall_score(y_test, y_pred_nb)\n",
        "nb_f1 = f1_score(y_test, y_pred_nb)\n",
        "nb_auc = roc_auc_score(y_test, y_pred_proba_nb)\n",
        "\n",
        "print(f\"\\nPerformance Metrics:\")\n",
        "print(f\"  â€¢ Accuracy:  {nb_accuracy:.4f} ({nb_accuracy*100:.2f}%)\")\n",
        "print(f\"  â€¢ Precision: {nb_precision:.4f} (of predicted positives, {nb_precision*100:.1f}% are correct)\")\n",
        "print(f\"  â€¢ Recall:    {nb_recall:.4f} (detected {nb_recall*100:.1f}% of actual disease cases)\")\n",
        "print(f\"  â€¢ F1-Score:  {nb_f1:.4f}\")\n",
        "print(f\"  â€¢ ROC-AUC:   {nb_auc:.4f}\")\n",
        "\n",
        "# ---\n",
        "# ## 8. Model Comparison and Analysis\n",
        "# ### 8.1 Comprehensive Model Comparison\n",
        "\n",
        "\"\"\"\n",
        "Compare all models across multiple metrics\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPREHENSIVE MODEL PERFORMANCE COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': ['Logistic Regression', 'Random Forest', 'SVM',\n",
        "              'Gradient Boosting', 'KNN', 'Naive Bayes'],\n",
        "    'Accuracy': [lr_accuracy, rf_accuracy, svm_accuracy, gb_accuracy, knn_accuracy, nb_accuracy],\n",
        "    'Precision': [lr_precision, rf_precision, svm_precision, gb_precision, knn_precision, nb_precision],\n",
        "    'Recall': [lr_recall, rf_recall, svm_recall, gb_recall, knn_recall, nb_recall],\n",
        "    'F1-Score': [lr_f1, rf_f1, svm_f1, gb_f1, knn_f1, nb_f1],\n",
        "    'ROC-AUC': [lr_auc, rf_auc, svm_auc, gb_auc, knn_auc, nb_auc]\n",
        "})\n",
        "\n",
        "# Sort by F1-Score\n",
        "comparison_df = comparison_df.sort_values('F1-Score', ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"\\nModel Performance Summary (Ranked by F1-Score):\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
        "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6', '#1abc9c']\n",
        "\n",
        "for idx, metric in enumerate(metrics):\n",
        "    axes[idx].bar(range(len(comparison_df)), comparison_df[metric],\n",
        "                 color=colors, edgecolor='black', alpha=0.8)\n",
        "    axes[idx].set_xticks(range(len(comparison_df)))\n",
        "    axes[idx].set_xticklabels(comparison_df['Model'], rotation=45, ha='right')\n",
        "    axes[idx].set_title(f'{metric} Comparison', fontweight='bold', fontsize=12)\n",
        "    axes[idx].set_ylabel(metric)\n",
        "    axes[idx].set_ylim([0, 1.1])\n",
        "    axes[idx].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Add value labels\n",
        "    for i, v in enumerate(comparison_df[metric]):\n",
        "        axes[idx].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold', fontsize=9)\n",
        "\n",
        "# Overall performance radar chart\n",
        "axes[5].remove()\n",
        "ax_radar = fig.add_subplot(2, 3, 6, projection='polar')\n",
        "\n",
        "# Get best model for radar chart\n",
        "best_model_idx = comparison_df['F1-Score'].idxmax()\n",
        "best_model_data = comparison_df.iloc[best_model_idx]\n",
        "\n",
        "categories = metrics\n",
        "values = best_model_data[metrics].values\n",
        "angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
        "values = np.concatenate((values, [values[0]]))\n",
        "angles += angles[:1]\n",
        "\n",
        "ax_radar.plot(angles, values, 'o-', linewidth=2, color='red', label=best_model_data['Model'])\n",
        "ax_radar.fill(angles, values, alpha=0.25, color='red')\n",
        "ax_radar.set_xticks(angles[:-1])\n",
        "ax_radar.set_xticklabels(categories)\n",
        "ax_radar.set_ylim(0, 1)\n",
        "ax_radar.set_title('Best Model Performance\\nRadar Chart', fontweight='bold', pad=20)\n",
        "ax_radar.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
        "ax_radar.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Select best model\n",
        "best_model_name = comparison_df.iloc[0]['Model']\n",
        "best_f1 = comparison_df.iloc[0]['F1-Score']\n",
        "best_auc = comparison_df.iloc[0]['ROC-AUC']\n",
        "\n",
        "print(f\"\\nðŸ† BEST PERFORMING MODEL: {best_model_name}\")\n",
        "print(f\"   â€¢ F1-Score: {best_f1:.4f}\")\n",
        "print(f\"   â€¢ ROC-AUC:  {best_auc:.4f}\")\n",
        "print(f\"   â€¢ Accuracy: {comparison_df.iloc[0]['Accuracy']:.4f}\")\n",
        "\n",
        "# Map to actual model and predictions\n",
        "model_map = {\n",
        "    'Logistic Regression': (lr_model, y_pred_lr, y_pred_proba_lr),\n",
        "    'Random Forest': (rf_model, y_pred_rf, y_pred_proba_rf),\n",
        "    'SVM': (svm_model, y_pred_svm, y_pred_proba_svm),\n",
        "    'Gradient Boosting': (gb_model, y_pred_gb, y_pred_proba_gb),\n",
        "    'KNN': (knn_model, y_pred_knn, y_pred_proba_knn),\n",
        "    'Naive Bayes': (nb_model, y_pred_nb, y_pred_proba_nb)\n",
        "}\n",
        "best_model, y_pred_best, y_pred_proba_best = model_map[best_model_name]\n",
        "\n",
        "# ### 8.2 Confusion Matrix Analysis\n",
        "\n",
        "\"\"\"\n",
        "Detailed breakdown of predictions for all models\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CONFUSION MATRIX ANALYSIS - ALL MODELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "predictions = [\n",
        "    ('Logistic Regression', y_pred_lr),\n",
        "    ('Random Forest', y_pred_rf),\n",
        "    ('SVM', y_pred_svm),\n",
        "    ('Gradient Boosting', y_pred_gb),\n",
        "    ('KNN', y_pred_knn),\n",
        "    ('Naive Bayes', y_pred_nb)\n",
        "]\n",
        "\n",
        "for idx, (model_name, y_pred) in enumerate(predictions):\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[idx],\n",
        "               xticklabels=['No Disease', 'Disease'],\n",
        "               yticklabels=['No Disease', 'Disease'])\n",
        "    axes[idx].set_title(f'{model_name}\\nConfusion Matrix', fontweight='bold')\n",
        "    axes[idx].set_ylabel('Actual')\n",
        "    axes[idx].set_xlabel('Predicted')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Detailed metrics for best model\n",
        "cm_best = confusion_matrix(y_test, y_pred_best)\n",
        "tn, fp, fn, tp = cm_best.ravel()\n",
        "\n",
        "print(f\"\\nDetailed Analysis - {best_model_name}:\")\n",
        "print(f\"  â€¢ True Positives (TP):  {tp} - Correctly identified disease cases\")\n",
        "print(f\"  â€¢ True Negatives (TN):  {tn} - Correctly identified healthy patients\")\n",
        "print(f\"  â€¢ False Positives (FP): {fp} - Healthy patients incorrectly flagged\")\n",
        "print(f\"  â€¢ False Negatives (FN): {fn} - Disease cases missed\")\n",
        "print(f\"\\n  â€¢ Sensitivity (Recall): {tp/(tp+fn):.4f} - {tp/(tp+fn)*100:.1f}% of disease cases detected\")\n",
        "print(f\"  â€¢ Specificity:          {tn/(tn+fp):.4f} - {tn/(tn+fp)*100:.1f}% of healthy correctly identified\")\n",
        "\n",
        "# ### 8.3 ROC Curve Comparison\n",
        "\n",
        "\"\"\"\n",
        "ROC curves show trade-off between true positive and false positive rates\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ROC CURVE ANALYSIS - ALL MODELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plot ROC curves\n",
        "roc_data = [\n",
        "    ('Logistic Regression', y_pred_proba_lr, lr_auc),\n",
        "    ('Random Forest', y_pred_proba_rf, rf_auc),\n",
        "    ('SVM', y_pred_proba_svm, svm_auc),\n",
        "    ('Gradient Boosting', y_pred_proba_gb, gb_auc),\n",
        "    ('KNN', y_pred_proba_knn, knn_auc),\n",
        "    ('Naive Bayes', y_pred_proba_nb, nb_auc)\n",
        "]\n",
        "\n",
        "for model_name, y_proba, auc_score in roc_data:\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "    plt.plot(fpr, tpr, linewidth=2, label=f'{model_name} (AUC = {auc_score:.4f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier (AUC = 0.5000)')\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate (Sensitivity)', fontsize=12)\n",
        "plt.title('ROC Curves - All Models', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right', fontsize=10)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"ðŸ“Š ROC-AUC Interpretation:\")\n",
        "print(f\"   â€¢ AUC = 1.0: Perfect classifier\")\n",
        "print(f\"   â€¢ AUC = 0.9-1.0: Excellent\")\n",
        "print(f\"   â€¢ AUC = 0.8-0.9: Good\")\n",
        "print(f\"   â€¢ AUC = 0.7-0.8: Fair\")\n",
        "print(f\"   â€¢ AUC = 0.5: Random guess\")\n",
        "\n",
        "# ### 8.4 Precision-Recall Curve\n",
        "\n",
        "\"\"\"\n",
        "Precision-Recall curves are useful for imbalanced datasets\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PRECISION-RECALL CURVE ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "for model_name, y_proba, _ in roc_data:\n",
        "    precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
        "    pr_auc = auc(recall, precision)\n",
        "    plt.plot(recall, precision, linewidth=2, label=f'{model_name} (AP = {pr_auc:.4f})')\n",
        "\n",
        "plt.xlabel('Recall (Sensitivity)', fontsize=12)\n",
        "plt.ylabel('Precision', fontsize=12)\n",
        "plt.title('Precision-Recall Curves - All Models', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='best', fontsize=10)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ### 8.5 Classification Report - Best Model\n",
        "\n",
        "\"\"\"\n",
        "Detailed classification metrics for the best model\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"CLASSIFICATION REPORT - {best_model_name.upper()}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(classification_report(y_test, y_pred_best,\n",
        "                          target_names=['No Disease', 'Heart Disease'],\n",
        "                          digits=4))\n",
        "\n",
        "# ---\n",
        "# ## 9. Cross-Validation\n",
        "# ### 9.1 K-Fold Cross-Validation for All Models\n",
        "\n",
        "\"\"\"\n",
        "Perform k-fold cross-validation to assess model stability\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"5-FOLD CROSS-VALIDATION ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "cv_results = {}\n",
        "\n",
        "# Models to cross-validate\n",
        "models_cv = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
        "    'SVM': SVC(kernel='rbf', probability=True, random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
        "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
        "    'Naive Bayes': GaussianNB()\n",
        "}\n",
        "\n",
        "for model_name, model in models_cv.items():\n",
        "    # Use scaled data for distance-based models\n",
        "    if model_name in ['Logistic Regression', 'SVM', 'KNN', 'Naive Bayes']:\n",
        "        X_cv = X_train_scaled_df\n",
        "    else:\n",
        "        X_cv = X_train\n",
        "\n",
        "    cv_scores = cross_val_score(model, X_cv, y_train, cv=cv, scoring='f1')\n",
        "    cv_results[model_name] = cv_scores\n",
        "\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    print(f\"  CV Scores: {cv_scores}\")\n",
        "    print(f\"  Mean F1:   {cv_scores.mean():.4f}\")\n",
        "    print(f\"  Std Dev:   {cv_scores.std():.4f}\")\n",
        "\n",
        "# Visualize CV results\n",
        "plt.figure(figsize=(14, 6))\n",
        "cv_df = pd.DataFrame(cv_results)\n",
        "\n",
        "bp = plt.boxplot([cv_df[col] for col in cv_df.columns],\n",
        "                labels=cv_df.columns,\n",
        "                patch_artist=True,\n",
        "                showmeans=True)\n",
        "\n",
        "# Color the boxes\n",
        "for patch, color in zip(bp['boxes'], colors):\n",
        "    patch.set_facecolor(color)\n",
        "    patch.set_alpha(0.7)\n",
        "\n",
        "plt.xlabel('Model', fontsize=12)\n",
        "plt.ylabel('F1-Score', fontsize=12)\n",
        "plt.title('5-Fold Cross-Validation Results', fontsize=14, fontweight='bold')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---\n",
        "# ## 10. Key Insights and Clinical Findings\n",
        "# ### 10.1 Feature Importance - Clinical Insights\n",
        "\n",
        "\"\"\"\n",
        "Analyze which clinical factors are most predictive of heart disease\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)# Heart Failure Prediction Using Machine Learning\n",
        "# ================================================\n",
        "# Data Exploration, Model Comparison, and Insights\n",
        "#\n",
        "# Objective: Develop a machine learning pipeline to predict heart failure\n",
        "# using clinical data, compare multiple models, and derive actionable insights\n",
        "\n",
        "# ## 1. Project Setup and Environment Configuration\n",
        "# ### 1.1 Import Required Libraries\n",
        "\n",
        "\"\"\"\n",
        "This notebook implements a complete machine learning pipeline for heart failure prediction.\n",
        "We'll use:\n",
        "- Data manipulation: pandas, numpy\n",
        "- Visualization: matplotlib, seaborn\n",
        "- Machine Learning: scikit-learn\n",
        "- Statistical analysis: scipy\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "\n",
        "# Machine Learning Libraries\n",
        "from sklearn.model_selection import (train_test_split, cross_val_score,\n",
        "                                    StratifiedKFold, GridSearchCV, learning_curve)\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# ML Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                            f1_score, roc_auc_score, roc_curve, confusion_matrix,\n",
        "                            classification_report, precision_recall_curve, auc)\n",
        "\n",
        "# Feature Selection\n",
        "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
        "\n",
        "# Configuration\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette(\"Set2\")\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"HEART FAILURE PREDICTION USING MACHINE LEARNING\")\n",
        "print(\"Data Exploration, Model Comparison, and Clinical Insights\")\n",
        "print(\"=\"*80)\n",
        "print(\"âœ“ All libraries imported successfully\")\n",
        "print(f\"âœ“ Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# ### 1.2 Configure Display Settings\n",
        "\n",
        "\"\"\"\n",
        "Set up display options for better readability and professional visualizations\n",
        "\"\"\"\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['font.size'] = 10\n",
        "plt.rcParams['axes.titlesize'] = 14\n",
        "plt.rcParams['axes.labelsize'] = 11\n",
        "\n",
        "print(\"âœ“ Display and visualization settings configured\")\n",
        "\n",
        "# ---\n",
        "# ## 2. Data Loading and Initial Exploration\n",
        "# ### 2.1 Load the Heart Failure Dataset\n",
        "\n",
        "\"\"\"\n",
        "Load the clinical heart failure dataset.\n",
        "This dataset contains various medical and demographic features\n",
        "used to predict the presence of heart disease.\n",
        "\"\"\"\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('heart_failure_data.csv')\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DATASET LOADED SUCCESSFULLY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Total Patients: {len(df):,}\")\n",
        "print(f\"Total Features: {len(df.columns)}\")\n",
        "print(f\"Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "# ### 2.2 Display Sample Data\n",
        "\n",
        "\"\"\"\n",
        "Examine the first and last few rows to understand data structure\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FIRST 10 PATIENT RECORDS\")\n",
        "print(\"=\"*80)\n",
        "print(df.head(10))\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"LAST 5 PATIENT RECORDS\")\n",
        "print(\"=\"*80)\n",
        "print(df.tail())\n",
        "\n",
        "# ### 2.3 Dataset Structure and Information\n",
        "\n",
        "\"\"\"\n",
        "Comprehensive overview of dataset structure, data types, and completeness\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DATASET STRUCTURE AND INFORMATION\")\n",
        "print(\"=\"*80)\n",
        "df.info()\n",
        "\n",
        "# ### 2.4 Feature Descriptions\n",
        "\n",
        "\"\"\"\n",
        "Understanding each clinical feature in the dataset:\n",
        "\"\"\"\n",
        "\n",
        "feature_descriptions = {\n",
        "    'Age': 'Age of the patient (years)',\n",
        "    'Sex': 'Sex of the patient (M: Male, F: Female)',\n",
        "    'ChestPainType': 'Type of chest pain (TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic)',\n",
        "    'RestingBP': 'Resting blood pressure (mm Hg)',\n",
        "    'Cholesterol': 'Serum cholesterol (mm/dl)',\n",
        "    'FastingBS': 'Fasting blood sugar > 120 mg/dl (1: Yes, 0: No)',\n",
        "    'RestingECG': 'Resting electrocardiogram results (Normal, ST, LVH)',\n",
        "    'MaxHR': 'Maximum heart rate achieved (60-202)',\n",
        "    'ExerciseAngina': 'Exercise-induced angina (Y: Yes, N: No)',\n",
        "    'Oldpeak': 'ST depression induced by exercise relative to rest',\n",
        "    'ST_Slope': 'Slope of peak exercise ST segment (Up, Flat, Down)',\n",
        "    'HeartDisease': 'Target variable - Presence of heart disease (1: Yes, 0: No)'\n",
        "}\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CLINICAL FEATURE DESCRIPTIONS\")\n",
        "print(\"=\"*80)\n",
        "for feature, description in feature_descriptions.items():\n",
        "    # Check for similar column names (case-insensitive, space handling)\n",
        "    matching_cols = [col for col in df.columns if feature.lower().replace('_', '').replace(' ', '')\n",
        "                    == col.lower().replace('_', '').replace(' ', '')]\n",
        "    if matching_cols:\n",
        "        print(f\"  â€¢ {matching_cols[0]:20s}: {description}\")\n",
        "\n",
        "# ### 2.5 Statistical Summary\n",
        "\n",
        "\"\"\"\n",
        "Statistical overview of numerical features:\n",
        "- Central tendency (mean, median)\n",
        "- Spread (std, min, max)\n",
        "- Distribution characteristics\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STATISTICAL SUMMARY OF NUMERICAL FEATURES\")\n",
        "print(\"=\"*80)\n",
        "print(df.describe())\n",
        "\n",
        "# ### 2.6 Target Variable Analysis\n",
        "\n",
        "\"\"\"\n",
        "Analyze the distribution of heart disease (target variable).\n",
        "Understanding class balance is critical for model development.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TARGET VARIABLE ANALYSIS: HEART DISEASE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Find the target column (might be named differently)\n",
        "target_col = None\n",
        "for col in df.columns:\n",
        "    if 'heart' in col.lower() and 'disease' in col.lower():\n",
        "        target_col = col\n",
        "        break\n",
        "    elif col.lower() in ['target', 'output', 'class']:\n",
        "        target_col = col\n",
        "        break\n",
        "\n",
        "if target_col:\n",
        "    target_counts = df[target_col].value_counts().sort_index()\n",
        "    target_pct = (target_counts / len(df) * 100).round(2)\n",
        "\n",
        "    print(f\"Target Variable: '{target_col}'\")\n",
        "    print(\"\\nClass Distribution:\")\n",
        "    print(f\"  â€¢ No Heart Disease (0): {target_counts.get(0, 0):,} patients ({target_pct.get(0, 0)}%)\")\n",
        "    print(f\"  â€¢ Heart Disease (1):    {target_counts.get(1, 0):,} patients ({target_pct.get(1, 0)}%)\")\n",
        "\n",
        "    # Check for class imbalance\n",
        "    if len(target_counts) == 2:\n",
        "        imbalance_ratio = max(target_counts) / min(target_counts)\n",
        "        print(f\"\\n  Class Balance Ratio: {imbalance_ratio:.2f}:1\")\n",
        "\n",
        "        if imbalance_ratio > 1.5:\n",
        "            print(f\"  âš  Moderate class imbalance detected\")\n",
        "        elif imbalance_ratio > 2:\n",
        "            print(f\"  âš  Significant class imbalance detected\")\n",
        "        else:\n",
        "            print(f\"  âœ“ Classes are well balanced\")\n",
        "\n",
        "    # Visualize target distribution\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Bar chart\n",
        "    axes[0].bar(['No Disease', 'Heart Disease'], target_counts.values,\n",
        "               color=['#2ecc71', '#e74c3c'], edgecolor='black', alpha=0.8)\n",
        "    axes[0].set_title('Heart Disease Distribution', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_ylabel('Number of Patients')\n",
        "    axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Add value labels\n",
        "    for i, v in enumerate(target_counts.values):\n",
        "        axes[0].text(i, v + 10, f'{v:,}\\n({target_pct.values[i]}%)',\n",
        "                    ha='center', fontweight='bold')\n",
        "\n",
        "    # Pie chart\n",
        "    axes[1].pie(target_counts.values, labels=['No Disease', 'Heart Disease'],\n",
        "               autopct='%1.1f%%', colors=['#2ecc71', '#e74c3c'],\n",
        "               startangle=90, explode=[0, 0.1])\n",
        "    axes[1].set_title('Class Distribution', fontsize=14, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"âš  Warning: Target variable not found. Please specify the correct column name.\")\n",
        "\n",
        "# ---\n",
        "# ## 3. Data Preprocessing\n",
        "# ### 3.1 Missing Values Analysis\n",
        "\n",
        "\"\"\"\n",
        "Identify and quantify missing data across all features.\n",
        "Missing data can significantly impact model performance.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MISSING VALUES ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "missing_data = pd.DataFrame({\n",
        "    'Column': df.columns,\n",
        "    'Missing_Count': df.isnull().sum(),\n",
        "    'Missing_Percentage': (df.isnull().sum() / len(df) * 100).round(2),\n",
        "    'Data_Type': df.dtypes\n",
        "})\n",
        "\n",
        "missing_data = missing_data[missing_data['Missing_Count'] > 0].sort_values(\n",
        "    'Missing_Percentage', ascending=False)\n",
        "\n",
        "if len(missing_data) > 0:\n",
        "    print(\"Columns with Missing Values:\")\n",
        "    print(missing_data.to_string(index=False))\n",
        "\n",
        "    # Visualize missing data\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.barh(missing_data['Column'], missing_data['Missing_Percentage'],\n",
        "            color='coral', edgecolor='black', alpha=0.8)\n",
        "    plt.xlabel('Missing Percentage (%)')\n",
        "    plt.title('Missing Data by Feature', fontsize=14, fontweight='bold')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.grid(axis='x', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"âœ“ Excellent! No missing values found in the dataset\")\n",
        "\n",
        "# ### 3.2 Handle Missing Values\n",
        "\n",
        "\"\"\"\n",
        "Strategy for handling missing values:\n",
        "1. Drop columns with >50% missing data\n",
        "2. Impute numerical features with median\n",
        "3. Impute categorical features with mode\n",
        "4. Remove rows with missing target variable\n",
        "\"\"\"\n",
        "\n",
        "df_clean = df.copy()\n",
        "initial_rows = len(df_clean)\n",
        "initial_cols = len(df_clean.columns)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"HANDLING MISSING VALUES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Remove columns with >50% missing\n",
        "if len(missing_data) > 0:\n",
        "    high_missing = missing_data[missing_data['Missing_Percentage'] > 50]['Column'].tolist()\n",
        "    if len(high_missing) > 0:\n",
        "        df_clean = df_clean.drop(columns=high_missing)\n",
        "        print(f\"âœ“ Dropped {len(high_missing)} columns with >50% missing: {high_missing}\")\n",
        "\n",
        "# Remove rows with missing target\n",
        "if target_col and df_clean[target_col].isnull().sum() > 0:\n",
        "    df_clean = df_clean.dropna(subset=[target_col])\n",
        "    print(f\"âœ“ Removed {initial_rows - len(df_clean)} rows with missing target variable\")\n",
        "\n",
        "# Impute numerical features with median\n",
        "numerical_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
        "if target_col in numerical_cols:\n",
        "    numerical_cols.remove(target_col)\n",
        "\n",
        "for col in numerical_cols:\n",
        "    if df_clean[col].isnull().sum() > 0:\n",
        "        median_val = df_clean[col].median()\n",
        "        df_clean[col].fillna(median_val, inplace=True)\n",
        "        print(f\"âœ“ Imputed '{col}' missing values with median: {median_val:.2f}\")\n",
        "\n",
        "# Impute categorical features with mode\n",
        "categorical_cols = df_clean.select_dtypes(include=['object']).columns.tolist()\n",
        "for col in categorical_cols:\n",
        "    if df_clean[col].isnull().sum() > 0:\n",
        "        mode_val = df_clean[col].mode()[0]\n",
        "        df_clean[col].fillna(mode_val, inplace=True)\n",
        "        print(f\"âœ“ Imputed '{col}' missing values with mode: {mode_val}\")\n",
        "\n",
        "print(f\"\\nâœ“ Final dataset: {len(df_clean):,} patients Ã— {len(df_clean.columns)} features\")\n",
        "print(f\"âœ“ Data retention: {len(df_clean)/initial_rows*100:.1f}% of original rows\")\n",
        "\n",
        "# ### 3.3 Check for Duplicates\n",
        "\n",
        "\"\"\"\n",
        "Identify and remove duplicate patient records\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DUPLICATE RECORDS CHECK\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "duplicates = df_clean.duplicated().sum()\n",
        "print(f\"Duplicate rows found: {duplicates}\")\n",
        "\n",
        "if duplicates > 0:\n",
        "    df_clean = df_clean.drop_duplicates()\n",
        "    print(f\"âœ“ Removed {duplicates} duplicate records\")\n",
        "    print(f\"âœ“ Dataset now has {len(df_clean):,} unique patient records\")\n",
        "else:\n",
        "    print(\"âœ“ No duplicate records found\")\n",
        "\n",
        "# ### 3.4 Data Type Corrections\n",
        "\n",
        "\"\"\"\n",
        "Ensure all features have appropriate data types\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DATA TYPE VALIDATION AND CORRECTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Binary features should be 0/1 integers\n",
        "binary_features = ['FastingBS', target_col] if target_col else ['FastingBS']\n",
        "for col in binary_features:\n",
        "    if col in df_clean.columns:\n",
        "        df_clean[col] = df_clean[col].astype(int)\n",
        "        print(f\"âœ“ Converted '{col}' to integer type\")\n",
        "\n",
        "# Ensure numerical features are numeric\n",
        "for col in numerical_cols:\n",
        "    if col in df_clean.columns and df_clean[col].dtype == 'object':\n",
        "        try:\n",
        "            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
        "            print(f\"âœ“ Converted '{col}' to numeric type\")\n",
        "        except:\n",
        "            print(f\"âš  Could not convert '{col}' to numeric\")\n",
        "\n",
        "print(f\"\\nâœ“ Data type validation completed\")\n",
        "\n",
        "# ### 3.5 Encode Categorical Variables\n",
        "\n",
        "\"\"\"\n",
        "Convert categorical text features to numerical format for machine learning.\n",
        "We'll use Label Encoding for binary categories and create dummy variables for multi-class.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ENCODING CATEGORICAL VARIABLES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "df_encoded = df_clean.copy()\n",
        "label_encoders = {}\n",
        "\n",
        "# Get categorical columns\n",
        "categorical_columns = df_encoded.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "print(f\"Found {len(categorical_columns)} categorical columns: {categorical_columns}\")\n",
        "\n",
        "for col in categorical_columns:\n",
        "    unique_values = df_encoded[col].nunique()\n",
        "    print(f\"\\n  â€¢ {col}: {unique_values} unique values\")\n",
        "    print(f\"    Values: {df_encoded[col].unique().tolist()}\")\n",
        "\n",
        "    # Use Label Encoding\n",
        "    le = LabelEncoder()\n",
        "    df_encoded[col + '_encoded'] = le.fit_transform(df_encoded[col].astype(str))\n",
        "    label_encoders[col] = le\n",
        "\n",
        "    # Create mapping\n",
        "    mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "    print(f\"    Mapping: {mapping}\")\n",
        "\n",
        "print(f\"\\nâœ“ Categorical encoding completed\")\n",
        "print(f\"âœ“ Total features: {len(df_encoded.columns)}\")\n",
        "\n",
        "# ### 3.6 Feature Engineering\n",
        "\n",
        "\"\"\"\n",
        "Create additional features that might improve model performance:\n",
        "1. Age groups\n",
        "2. Blood pressure categories\n",
        "3. Cholesterol categories\n",
        "4. Heart rate zones\n",
        "5. Risk scores\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FEATURE ENGINEERING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. Age groups\n",
        "if 'Age' in df_encoded.columns:\n",
        "    df_encoded['Age_Group'] = pd.cut(df_encoded['Age'],\n",
        "                                     bins=[0, 40, 55, 70, 100],\n",
        "                                     labels=['Young', 'Middle', 'Senior', 'Elderly'])\n",
        "    df_encoded['Age_Group_encoded'] = LabelEncoder().fit_transform(df_encoded['Age_Group'].astype(str))\n",
        "    print(\"âœ“ Created Age_Group: Young (<40), Middle (40-55), Senior (55-70), Elderly (70+)\")\n",
        "\n",
        "# 2. Blood Pressure categories\n",
        "if 'RestingBP' in df_encoded.columns:\n",
        "    def categorize_bp(bp):\n",
        "        if bp < 120:\n",
        "            return 'Normal'\n",
        "        elif bp < 130:\n",
        "            return 'Elevated'\n",
        "        elif bp < 140:\n",
        "            return 'High_Stage1'\n",
        "        else:\n",
        "            return 'High_Stage2'\n",
        "\n",
        "    df_encoded['BP_Category'] = df_encoded['RestingBP'].apply(categorize_bp)\n",
        "    df_encoded['BP_Category_encoded'] = LabelEncoder().fit_transform(df_encoded['BP_Category'])\n",
        "    print(\"âœ“ Created BP_Category: Normal, Elevated, High_Stage1, High_Stage2\")\n",
        "\n",
        "# 3. Cholesterol categories\n",
        "if 'Cholesterol' in df_encoded.columns:\n",
        "    # Filter out zero values (often missing data coded as 0)\n",
        "    df_encoded['Cholesterol_Valid'] = df_encoded['Cholesterol'].replace(0, np.nan)\n",
        "\n",
        "    def categorize_cholesterol(chol):\n",
        "        if pd.isna(chol):\n",
        "            return 'Unknown'\n",
        "        elif chol < 200:\n",
        "            return 'Desirable'\n",
        "        elif chol < 240:\n",
        "            return 'Borderline'\n",
        "        else:\n",
        "            return 'High'\n",
        "\n",
        "    df_encoded['Chol_Category'] = df_encoded['Cholesterol_Valid'].apply(categorize_cholesterol)\n",
        "    df_encoded['Chol_Category_encoded'] = LabelEncoder().fit_transform(df_encoded['Chol_Category'])\n",
        "    print(\"âœ“ Created Chol_Category: Desirable (<200), Borderline (200-240), High (>240)\")\n",
        "\n",
        "# 4. Heart Rate zones\n",
        "if 'MaxHR' in df_encoded.columns and 'Age' in df_encoded.columns:\n",
        "    df_encoded['Max_HR_Expected'] = 220 - df_encoded['Age']\n",
        "    df_encoded['HR_Achievement_Pct'] = (df_encoded['MaxHR'] / df_encoded['Max_HR_Expected']) * 100\n",
        "    print(\"âœ“ Created HR_Achievement_Pct: Percentage of maximum heart rate achieved\")\n",
        "\n",
        "# 5. Risk score (simple composite)\n",
        "if all(col in df_encoded.columns for col in ['Age', 'RestingBP', 'Cholesterol', 'FastingBS']):\n",
        "    df_encoded['Simple_Risk_Score'] = (\n",
        "        (df_encoded['Age'] > 55).astype(int) +\n",
        "        (df_encoded['RestingBP'] > 130).astype(int) +\n",
        "        (df_encoded['Cholesterol'] > 200).astype(int) +\n",
        "        df_encoded['FastingBS']\n",
        "    )\n",
        "    print(\"âœ“ Created Simple_Risk_Score: Composite cardiovascular risk indicator (0-4)\")\n",
        "\n",
        "print(f\"\\nâœ“ Feature engineering completed\")\n",
        "print(f\"âœ“ Total features now: {len(df_encoded.columns)}\")\n",
        "\n",
        "# ---\n",
        "# ## 4. Exploratory Data Analysis (EDA)\n",
        "# ### 4.1 Univariate Analysis - Numerical Features\n",
        "\n",
        "\"\"\"\n",
        "Analyze the distribution of numerical features\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EXPLORATORY DATA ANALYSIS: NUMERICAL FEATURES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "numerical_features = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']\n",
        "available_num_features = [f for f in numerical_features if f in df_encoded.columns]\n",
        "\n",
        "if len(available_num_features) > 0:\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, feature in enumerate(available_num_features[:6]):\n",
        "        axes[idx].hist(df_encoded[feature], bins=30, color='steelblue',\n",
        "                      edgecolor='black', alpha=0.7)\n",
        "        axes[idx].set_title(f'Distribution of {feature}', fontweight='bold')\n",
        "        axes[idx].set_xlabel(feature)\n",
        "        axes[idx].set_ylabel('Frequency')\n",
        "        axes[idx].axvline(df_encoded[feature].mean(), color='red', linestyle='--',\n",
        "                         linewidth=2, label=f'Mean: {df_encoded[feature].mean():.1f}')\n",
        "        axes[idx].axvline(df_encoded[feature].median(), color='green', linestyle='--',\n",
        "                         linewidth=2, label=f'Median: {df_encoded[feature].median():.1f}')\n",
        "        axes[idx].legend(fontsize=8)\n",
        "        axes[idx].grid(alpha=0.3)\n",
        "\n",
        "    # Hide extra subplots\n",
        "    for idx in range(len(available_num_features), 6):\n",
        "        axes[idx].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ### 4.2 Univariate Analysis - Categorical Features\n",
        "\n",
        "\"\"\"\n",
        "Analyze the distribution of categorical features\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EXPLORATORY DATA ANALYSIS: CATEGORICAL FEATURES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "categorical_features = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n",
        "# Find matching columns (handle different naming conventions)\n",
        "available_cat_features = []\n",
        "for feat in categorical_features:\n",
        "    matching = [col for col in df_clean.columns if feat.lower().replace('_', '') in col.lower().replace('_', '')]\n",
        "    if matching:\n",
        "        available_cat_features.append(matching[0])\n",
        "\n",
        "if len(available_cat_features) > 0:\n",
        "    n_features = len(available_cat_features)\n",
        "    n_cols = min(3, n_features)\n",
        "    n_rows = (n_features + n_cols - 1) // n_cols\n",
        "\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(6*n_cols, 5*n_rows))\n",
        "    if n_features == 1:\n",
        "        axes = [axes]\n",
        "    else:\n",
        "        axes = axes.flatten()\n",
        "\n",
        "    for idx, feature in enumerate(available_cat_features):\n",
        "        feature_counts = df_clean[feature].value_counts()\n",
        "        axes[idx].bar(range(len(feature_counts)), feature_counts.values,\n",
        "                     color='coral', edgecolor='black', alpha=0.8)\n",
        "        axes[idx].set_xticks(range(len(feature_counts)))\n",
        "        axes[idx].set_xticklabels(feature_counts.index, rotation=45, ha='right')\n",
        "        axes[idx].set_title(f'Distribution of {feature}', fontweight='bold')\n",
        "        axes[idx].set_ylabel('Count')\n",
        "        axes[idx].grid(axis='y', alpha=0.3)\n",
        "\n",
        "        # Add value labels\n",
        "        for i, v in enumerate(feature_counts.values):\n",
        "            axes[idx].text(i, v + 5, str(v), ha='center', fontweight='bold')\n",
        "\n",
        "    # Hide extra subplots\n",
        "    for idx in range(n_features, len(axes)):\n",
        "        axes[idx].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ### 4.3 Bivariate Analysis - Features vs Heart Disease\n",
        "\n",
        "\"\"\"\n",
        "Analyze how each feature relates to heart disease outcome\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BIVARIATE ANALYSIS: FEATURES VS HEART DISEASE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if target_col:\n",
        "    # Numerical features vs target\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, feature in enumerate(available_num_features[:6]):\n",
        "        df_encoded.boxplot(column=feature, by=target_col, ax=axes[idx])\n",
        "        axes[idx].set_title(f'{feature} by Heart Disease Status', fontweight='bold')\n",
        "        axes[idx].set_xlabel('Heart Disease (0=No, 1=Yes)')\n",
        "        axes[idx].set_ylabel(feature)\n",
        "        plt.suptitle('')\n",
        "        axes[idx].grid(alpha=0.3)\n",
        "\n",
        "    for idx in range(len(available_num_features), 6):\n",
        "        axes[idx].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Statistical comparison\n",
        "    print(\"\\nMean Values by Heart Disease Status:\")\n",
        "    for feature in available_num_features:\n",
        "        no_disease_mean = df_encoded[df_encoded[target_col] == 0][feature].mean()\n",
        "        disease_mean = df_encoded[df_encoded[target_col] == 1][feature].mean()\n",
        "        difference = disease_mean - no_disease_mean\n",
        "        print(f\"  â€¢ {feature:15s}: No Disease={no_disease_mean:6.1f}, Disease={disease_mean:6.1f}, Diff={difference:+6.1f}\")\n",
        "\n",
        "# ### 4.4 Categorical Features vs Heart Disease\n",
        "\n",
        "\"\"\"\n",
        "Analyze relationship between categorical features and heart disease\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CATEGORICAL FEATURES VS HEART DISEASE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if target_col and len(available_cat_features) > 0:\n",
        "    n_features = min(4, len(available_cat_features))\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, feature in enumerate(available_cat_features[:4]):\n",
        "        crosstab = pd.crosstab(df_clean[feature], df_clean[target_col])\n",
        "        crosstab_pct = pd.crosstab(df_clean[feature], df_clean[target_col], normalize='index') * 100\n",
        "\n",
        "        crosstab.plot(kind='bar', ax=axes[idx], color=['#2ecc71', '#e74c3c'],\n",
        "                     edgecolor='black', alpha=0.8)\n",
        "        axes[idx].set_title(f'{feature} vs Heart Disease', fontweight='bold')\n",
        "        axes[idx].set_xlabel(feature)\n",
        "        axes[idx].set_ylabel('Count')\n",
        "        axes[idx].legend(['No Disease', 'Disease'])\n",
        "        axes[idx].tick_params(axis='x', rotation=45)\n",
        "        axes[idx].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ### 4.5 Correlation Analysis\n",
        "\n",
        "\"\"\"\n",
        "Analyze correlations between all numerical features\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CORRELATION ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Select numerical columns including encoded categoricals\n",
        "numerical_for_corr = df_encoded.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Remove non-predictive columns\n",
        "exclude_cols = [col for col in numerical_for_corr if 'Group' in col and '_encoded' not in col]\n",
        "numerical_for_corr = [col for col in numerical_for_corr if col not in exclude_cols]\n",
        "\n",
        "if len(numerical_for_corr) > 2:\n",
        "    # Calculate correlation matrix\n",
        "    corr_matrix = df_encoded[numerical_for_corr].corr()\n",
        "\n",
        "    # Visualize\n",
        "    plt.figure(figsize=(16, 14))\n",
        "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='RdYlGn', center=0,\n",
        "                square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8},\n",
        "                annot_kws={'size': 8})\n",
        "    plt.title('Correlation Matrix - All Numerical Features', fontsize=16, fontweight='bold', pad=20)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Features most correlated with target\n",
        "    if target_col in corr_matrix.columns:\n",
        "        target_corr = corr_matrix[target_col].drop(target_col).sort_values(ascending=False)\n",
        "\n",
        "        print(\"\\nTop 15 Features Correlated with Heart Disease:\")\n",
        "        print(\"\\nPositive Correlations (increase disease risk):\")\n",
        "        for feature, corr in target_corr[target_corr > 0].head(10).items():\n",
        "            print(f\"   â€¢ {feature:30s}: {corr:.4f}\")\n",
        "\n",
        "        print(\"\\nNegative Correlations (decrease disease risk):\")\n",
        "        for feature, corr in target_corr[target_corr < 0].head(10).items():\n",
        "            print(f\"   â€¢ {feature:30s}: {corr:.4f}\")\n",
        "\n",
        "        # Visualize top correlations\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        top_15_corr = pd.concat([target_corr.head(8), target_corr.tail(7)])\n",
        "        colors = ['green' if x > 0 else 'red' for x in top_15_corr.values]\n",
        "        plt.barh(range(len(top_15_corr)), top_15_corr.values, color=colors,\n",
        "                edgecolor='black', alpha=0.8)\n",
        "        plt.yticks(range(len(top_15_corr)), top_15_corr.index)\n",
        "        plt.xlabel('Correlation Coefficient')\n",
        "        plt.title('Top 15 Features Correlated with Heart Disease', fontsize=14, fontweight='bold')\n",
        "        plt.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
        "        plt.gca().invert_yaxis()\n",
        "        plt.grid(axis='x', alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# ---\n",
        "# ## 5. Feature Selection and Preparation\n",
        "# ### 5.1 Prepare Features for Modeling\n",
        "\n",
        "\"\"\"\n",
        "Select final features for machine learning models:\n",
        "- Use encoded versions of categorical features\n",
        "- Remove original categorical columns\n",
        "- Remove intermediate feature engineering columns\n",
        "- Separate features (X) and target (y)\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \""
      ]
    }
  ]
}