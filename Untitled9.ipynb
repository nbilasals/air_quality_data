{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIkjSj5anJkijYRSLTiieu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nbilasals/air_quality_data/blob/main/Untitled9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# ü©∫ BREAST CANCER PREDICTION: MALIGNANT VS BENIGN CLASSIFICATION üß¨\n",
        "# ============================================================================\n",
        "#\n",
        "# PROJECT OVERVIEW:\n",
        "# -----------------\n",
        "# This comprehensive notebook demonstrates a complete machine learning workflow\n",
        "# for predicting whether a breast tumor is malignant (cancerous) or benign\n",
        "# (non-cancerous) based on physical characteristics extracted from biopsy images.\n",
        "#\n",
        "# DATASET: Breast Cancer Wisconsin (Diagnostic) Dataset\n",
        "# - Source: UCI Machine Learning Repository\n",
        "# - Samples: 569 breast tumor cases\n",
        "# - Features: 30 numeric measurements of tumor characteristics\n",
        "# - Target: Binary classification (Malignant vs Benign)\n",
        "#\n",
        "# WORKFLOW STAGES:\n",
        "# 1. Data Loading & Exploration\n",
        "# 2. Exploratory Data Analysis (EDA)\n",
        "# 3. Data Preprocessing & Feature Engineering\n",
        "# 4. Multiple ML Model Training\n",
        "# 5. Model Evaluation & Comparison\n",
        "# 6. Final Recommendations\n",
        "#\n",
        "# CLINICAL SIGNIFICANCE:\n",
        "# Early and accurate detection of breast cancer is crucial for successful\n",
        "# treatment. This machine learning approach can assist medical professionals\n",
        "# in making more informed diagnostic decisions.\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 1: IMPORT NECESSARY LIBRARIES\n",
        "# ============================================================================\n",
        "#\n",
        "# SECTION PURPOSE:\n",
        "# ----------------\n",
        "# In this cell, we import all the essential Python libraries needed for our\n",
        "# complete machine learning pipeline. Each library serves a specific purpose\n",
        "# in our analysis workflow.\n",
        "#\n",
        "# LIBRARIES BREAKDOWN:\n",
        "# --------------------\n",
        "#\n",
        "# üìä DATA MANIPULATION:\n",
        "# - numpy: Numerical computing, array operations, mathematical functions\n",
        "# - pandas: Data manipulation, DataFrame operations, data analysis\n",
        "#\n",
        "# üìà DATA VISUALIZATION:\n",
        "# - matplotlib.pyplot: Creating static, animated, and interactive visualizations\n",
        "# - seaborn: Statistical data visualization built on matplotlib, prettier plots\n",
        "#\n",
        "# ü§ñ MACHINE LEARNING - CORE:\n",
        "# - sklearn.datasets: Access to built-in datasets including breast cancer data\n",
        "# - sklearn.model_selection: Tools for splitting data and cross-validation\n",
        "# - sklearn.preprocessing: Data preprocessing tools like scaling and encoding\n",
        "#\n",
        "# üéØ MACHINE LEARNING - ALGORITHMS:\n",
        "# - LogisticRegression: Linear model for binary classification\n",
        "# - DecisionTreeClassifier: Tree-based model with interpretable rules\n",
        "# - RandomForestClassifier: Ensemble of decision trees for robust predictions\n",
        "# - SVC: Support Vector Classifier for complex decision boundaries\n",
        "# - KNeighborsClassifier: Instance-based learning using nearest neighbors\n",
        "#\n",
        "# üìè MACHINE LEARNING - EVALUATION:\n",
        "# - Various metrics: accuracy, precision, recall, F1-score for model assessment\n",
        "# - confusion_matrix: Visual representation of classification results\n",
        "# - ROC curves & AUC: Evaluate model performance across different thresholds\n",
        "#\n",
        "# WHY EACH METRIC MATTERS:\n",
        "# - Accuracy: Overall correctness of predictions\n",
        "# - Precision: Of all positive predictions, how many were actually positive?\n",
        "# - Recall: Of all actual positives, how many did we correctly identify?\n",
        "# - F1-Score: Harmonic mean of precision and recall (balanced metric)\n",
        "# - ROC-AUC: Model's ability to distinguish between classes\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, confusion_matrix, classification_report,\n",
        "                             roc_curve, roc_auc_score)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set visualization style for better-looking plots\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"‚úÖ ALL LIBRARIES IMPORTED SUCCESSFULLY!\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nüì¶ Libraries loaded:\")\n",
        "print(\"   ‚úì NumPy - Numerical computing\")\n",
        "print(\"   ‚úì Pandas - Data manipulation\")\n",
        "print(\"   ‚úì Matplotlib & Seaborn - Data visualization\")\n",
        "print(\"   ‚úì Scikit-learn - Machine learning algorithms and tools\")\n",
        "print(\"\\nüöÄ Ready to begin breast cancer prediction analysis!\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 2: LOAD AND PREPARE THE BREAST CANCER DATASET\n",
        "# ============================================================================\n",
        "#\n",
        "# SECTION PURPOSE:\n",
        "# ----------------\n",
        "# This cell loads the Breast Cancer Wisconsin dataset from scikit-learn's\n",
        "# built-in datasets and prepares it for analysis by converting it into a\n",
        "# pandas DataFrame with proper column names and labels.\n",
        "#\n",
        "# ABOUT THE DATASET:\n",
        "# ------------------\n",
        "# The Breast Cancer Wisconsin (Diagnostic) Dataset contains features computed\n",
        "# from digitized images of fine needle aspirate (FNA) of breast masses. These\n",
        "# features describe characteristics of cell nuclei present in the images.\n",
        "#\n",
        "# DATA COLLECTION METHOD:\n",
        "# 1. A fine needle aspirate (FNA) is taken from a breast mass\n",
        "# 2. The sample is digitized and processed\n",
        "# 3. Computer vision algorithms extract features from cell nuclei\n",
        "# 4. Medical experts provide the diagnosis (malignant or benign)\n",
        "#\n",
        "# DATASET STRUCTURE:\n",
        "# - 569 instances (patient cases)\n",
        "# - 30 real-valued features (measurements)\n",
        "# - 2 classes: Malignant (0) and Benign (1)\n",
        "# - No missing values (complete dataset)\n",
        "#\n",
        "# TARGET VARIABLE ENCODING:\n",
        "# - 0 = Malignant (M) ‚Üí Cancer is present, requires treatment\n",
        "# - 1 = Benign (B) ‚Üí No cancer, but monitoring may be needed\n",
        "#\n",
        "# WHY THIS MATTERS:\n",
        "# In medical diagnosis, correctly identifying malignant tumors (high recall)\n",
        "# is critical to ensure patients receive timely treatment. However, we also\n",
        "# want to avoid false positives (high precision) to prevent unnecessary\n",
        "# stress and medical procedures.\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìä LOADING BREAST CANCER WISCONSIN DATASET\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Load the dataset from scikit-learn's built-in datasets\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Create a pandas DataFrame for easier manipulation and analysis\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "\n",
        "# Add the target variable (diagnosis) to our DataFrame\n",
        "df['diagnosis'] = data.target\n",
        "\n",
        "# Create human-readable labels for better interpretation\n",
        "# Map 0 ‚Üí 'Malignant' (cancerous), 1 ‚Üí 'Benign' (non-cancerous)\n",
        "df['diagnosis_label'] = df['diagnosis'].map({0: 'Malignant', 1: 'Benign'})\n",
        "\n",
        "print(\"\\n‚úÖ Dataset loaded successfully!\")\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìã DATASET OVERVIEW:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"   ‚Ä¢ Total number of samples: {len(df)}\")\n",
        "print(f\"   ‚Ä¢ Number of features: {len(data.feature_names)}\")\n",
        "print(f\"   ‚Ä¢ Dataset dimensions: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
        "print(f\"   ‚Ä¢ Number of malignant cases: {(df['diagnosis'] == 0).sum()}\")\n",
        "print(f\"   ‚Ä¢ Number of benign cases: {(df['diagnosis'] == 1).sum()}\")\n",
        "print(f\"   ‚Ä¢ Class balance ratio: {(df['diagnosis'] == 1).sum() / len(df) * 100:.1f}% benign\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìù DATASET DESCRIPTION:\")\n",
        "print(\"-\" * 80)\n",
        "print(data.DESCR[:500] + \"...\")  # Print first 500 characters of description\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 3: INITIAL DATA EXPLORATION - VIEWING THE DATA\n",
        "# ============================================================================\n",
        "#\n",
        "# SECTION PURPOSE:\n",
        "# ----------------\n",
        "# This cell performs initial exploration of our dataset to understand its\n",
        "# structure, data types, and basic characteristics. This is a critical first\n",
        "# step in any data science project.\n",
        "#\n",
        "# WHY DATA EXPLORATION MATTERS:\n",
        "# -----------------------------\n",
        "# Before building any machine learning model, we need to:\n",
        "# 1. Understand what data we're working with\n",
        "# 2. Identify data types and potential issues\n",
        "# 3. Get familiar with feature names and values\n",
        "# 4. Check for any obvious problems or anomalies\n",
        "#\n",
        "# WHAT WE'RE EXAMINING:\n",
        "# ---------------------\n",
        "# ‚Ä¢ First few rows: Get a feel for the actual data values\n",
        "# ‚Ä¢ Data types: Ensure all features are numeric (required for ML)\n",
        "# ‚Ä¢ Memory usage: Understand dataset size\n",
        "# ‚Ä¢ Feature names: Familiarize ourselves with what we're measuring\n",
        "#\n",
        "# THINGS TO LOOK FOR:\n",
        "# -------------------\n",
        "# ‚úì Are all features numeric? (Yes, required for our ML models)\n",
        "# ‚úì Do values seem reasonable? (No obvious errors)\n",
        "# ‚úì Are there any unexpected patterns?\n",
        "# ‚úì Do feature names make clinical sense?\n",
        "#\n",
        "# UNDERSTANDING THE OUTPUT:\n",
        "# -------------------------\n",
        "# - .head(): Shows first 5 rows of data\n",
        "# - .info(): Provides data types, non-null counts, memory usage\n",
        "# - .describe(): Statistical summary (mean, std, min, max, quartiles)\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üîç INITIAL DATA EXPLORATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìã FIRST 5 ROWS OF THE DATASET\")\n",
        "print(\"-\" * 80)\n",
        "print(\"\\nThis gives us a glimpse of the actual data values:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìä DATASET INFORMATION (DATA TYPES & STRUCTURE)\")\n",
        "print(\"-\" * 80)\n",
        "print(\"\\nDetailed information about each column:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìà STATISTICAL SUMMARY OF ALL FEATURES\")\n",
        "print(\"-\" * 80)\n",
        "print(\"\\nDescriptive statistics for each numeric feature:\")\n",
        "print(df.describe().round(2))\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìù INTERPRETATION GUIDE:\")\n",
        "print(\"-\" * 80)\n",
        "print(\"\"\"\n",
        "‚Ä¢ count: Number of non-null values (should be 569 for all)\n",
        "‚Ä¢ mean: Average value - center of the distribution\n",
        "‚Ä¢ std: Standard deviation - measure of spread/variability\n",
        "‚Ä¢ min: Minimum value observed\n",
        "‚Ä¢ 25%: First quartile (25% of data is below this value)\n",
        "‚Ä¢ 50%: Median (middle value when sorted)\n",
        "‚Ä¢ 75%: Third quartile (75% of data is below this value)\n",
        "‚Ä¢ max: Maximum value observed\n",
        "\n",
        "KEY OBSERVATIONS:\n",
        "‚úì All features have 569 non-null values ‚Üí No missing data!\n",
        "‚úì All features are numeric (float64) ‚Üí Ready for machine learning\n",
        "‚úì Different features have vastly different scales ‚Üí Will need scaling\n",
        "‚úì Some features show high variability (large std) ‚Üí Normal for medical data\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 4: DATA QUALITY CHECK - MISSING VALUES ANALYSIS\n",
        "# ============================================================================\n",
        "#\n",
        "# SECTION PURPOSE:\n",
        "# ----------------\n",
        "# This cell thoroughly checks for missing values in our dataset. Missing data\n",
        "# is one of the most common problems in real-world datasets and can significantly\n",
        "# impact model performance if not handled properly.\n",
        "#\n",
        "# WHY MISSING VALUES MATTER:\n",
        "# --------------------------\n",
        "# Missing values can occur due to:\n",
        "# ‚Ä¢ Data collection errors\n",
        "# ‚Ä¢ Equipment malfunction\n",
        "# ‚Ä¢ Human error during data entry\n",
        "# ‚Ä¢ Privacy concerns (data intentionally omitted)\n",
        "# ‚Ä¢ Technical issues during data transfer\n",
        "#\n",
        "# IMPACT ON MACHINE LEARNING:\n",
        "# ----------------------------\n",
        "# Most ML algorithms cannot handle missing values and will either:\n",
        "# 1. Throw an error and refuse to run\n",
        "# 2. Produce incorrect results\n",
        "# 3. Automatically drop rows/columns with missing values\n",
        "#\n",
        "# COMMON STRATEGIES FOR HANDLING MISSING DATA:\n",
        "# ---------------------------------------------\n",
        "# If we find missing values, we can:\n",
        "# 1. DELETE: Remove rows or columns with missing values\n",
        "#    - Use when: Very few missing values (<5% of data)\n",
        "#    - Pros: Simple, no assumptions made\n",
        "#    - Cons: Lose potentially valuable data\n",
        "#\n",
        "# 2. IMPUTE - MEAN/MEDIAN/MODE:\n",
        "#    - Use when: Data is missing at random\n",
        "#    - Pros: Retains all samples\n",
        "#    - Cons: Can distort distributions\n",
        "#\n",
        "# 3. IMPUTE - ADVANCED METHODS:\n",
        "#    - Use algorithms like KNN or regression to predict missing values\n",
        "#    - Pros: More accurate than simple imputation\n",
        "#    - Cons: More complex, computationally expensive\n",
        "#\n",
        "# 4. CREATE INDICATOR VARIABLES:\n",
        "#    - Add binary column indicating if value was missing\n",
        "#    - Use when: Missingness itself is informative\n",
        "#\n",
        "# GOOD NEWS FOR OUR DATASET:\n",
        "# ---------------------------\n",
        "# The Wisconsin Breast Cancer dataset is a well-curated research dataset\n",
        "# with NO missing values! This is rare in real-world scenarios but makes\n",
        "# our analysis cleaner and more straightforward.\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üîç DATA QUALITY CHECK: MISSING VALUES ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Check for missing values in each column\n",
        "missing_values = df.isnull().sum()\n",
        "total_cells = np.product(df.shape)\n",
        "total_missing = missing_values.sum()\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìä MISSING VALUES SUMMARY:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"\\n   ‚Ä¢ Total cells in dataset: {total_cells:,}\")\n",
        "print(f\"   ‚Ä¢ Total missing values: {total_missing}\")\n",
        "print(f\"   ‚Ä¢ Percentage of missing data: {(total_missing / total_cells) * 100:.2f}%\")\n",
        "\n",
        "if total_missing == 0:\n",
        "    print(\"\\n   ‚úÖ EXCELLENT! No missing values found in any column!\")\n",
        "    print(\"   ‚úÖ Dataset is complete and ready for machine learning!\")\n",
        "    print(\"\\n   This is a high-quality dataset - no imputation needed!\")\n",
        "else:\n",
        "    print(\"\\n   ‚ö†Ô∏è WARNING: Missing values detected!\")\n",
        "    print(\"\\n   Columns with missing values:\")\n",
        "    print(\"-\" * 80)\n",
        "    missing_df = pd.DataFrame({\n",
        "        'Column': missing_values[missing_values > 0].index,\n",
        "        'Missing Count': missing_values[missing_values > 0].values,\n",
        "        'Percentage': (missing_values[missing_values > 0].values / len(df) * 100).round(2)\n",
        "    })\n",
        "    print(missing_df.to_string(index=False))\n",
        "\n",
        "    print(\"\\n   üìù RECOMMENDED ACTIONS:\")\n",
        "    print(\"   \" + \"-\" * 76)\n",
        "    for col, missing_pct in zip(missing_df['Column'], missing_df['Percentage']):\n",
        "        if missing_pct < 5:\n",
        "            print(f\"   ‚Ä¢ {col}: {missing_pct}% missing ‚Üí Consider removing rows\")\n",
        "        elif missing_pct < 30:\n",
        "            print(f\"   ‚Ä¢ {col}: {missing_pct}% missing ‚Üí Consider imputation\")\n",
        "        else:\n",
        "            print(f\"   ‚Ä¢ {col}: {missing_pct}% missing ‚Üí Consider removing column\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üí° DATA QUALITY ASSESSMENT:\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\"\"\n",
        "‚úì All 569 samples are complete\n",
        "‚úì All 30 features have valid measurements\n",
        "‚úì No data cleaning required for missing values\n",
        "‚úì Ready to proceed with exploratory data analysis\n",
        "\n",
        "This clean dataset allows us to focus on feature engineering and\n",
        "model building without worrying about data imputation strategies!\n",
        "\"\"\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 5: TARGET VARIABLE DISTRIBUTION ANALYSIS\n",
        "# ============================================================================\n",
        "#\n",
        "# SECTION PURPOSE:\n",
        "# ----------------\n",
        "# This cell analyzes the distribution of our target variable (diagnosis).\n",
        "# Understanding class distribution is crucial because it affects:\n",
        "# 1. Model training and performance\n",
        "# 2. Evaluation metric selection\n",
        "# 3. Potential need for balancing techniques\n",
        "#\n",
        "# WHY CLASS DISTRIBUTION MATTERS:\n",
        "# -------------------------------\n",
        "# BALANCED DATASET (50:50 ratio):\n",
        "# ‚Ä¢ Models learn both classes equally well\n",
        "# ‚Ä¢ Standard metrics (accuracy) work well\n",
        "# ‚Ä¢ No special techniques needed\n",
        "#\n",
        "# IMBALANCED DATASET (e.g., 90:10 ratio):\n",
        "# ‚Ä¢ Models may become biased toward majority class\n",
        "# ‚Ä¢ Accuracy can be misleading (e.g., 90% by always predicting majority)\n",
        "# ‚Ä¢ May need: oversampling, undersampling, SMOTE, or class weights\n",
        "# ‚Ä¢ Should focus on: precision, recall, F1-score, not just accuracy\n",
        "#\n",
        "# CLASS IMBALANCE IN MEDICAL DIAGNOSIS:\n",
        "# --------------------------------------\n",
        "# In medical datasets, imbalance is common because:\n",
        "# ‚Ä¢ Diseases are often rare in the general population\n",
        "# ‚Ä¢ More benign cases than malignant in screening programs\n",
        "# ‚Ä¢ Cost of false negatives (missing cancer) is very high\n",
        "#\n",
        "# WHAT TO LOOK FOR:\n",
        "# -----------------\n",
        "# ‚Ä¢ Ratio between classes (is one significantly larger?)\n",
        "# ‚Ä¢ Absolute numbers (do we have enough samples of minority class?)\n",
        "# ‚Ä¢ Consider if imbalance reflects real-world prevalence\n",
        "#\n",
        "# EVALUATION METRIC IMPLICATIONS:\n",
        "# --------------------------------\n",
        "# ‚Ä¢ Balanced dataset ‚Üí Accuracy is fine\n",
        "# ‚Ä¢ Imbalanced dataset ‚Üí Focus on precision/recall/F1-score\n",
        "# ‚Ä¢ Medical context ‚Üí Prioritize RECALL (don't miss cancer cases)\n",
        "#\n",
        "# VISUALIZATIONS INCLUDED:\n",
        "# ------------------------\n",
        "# 1. Count plot: Shows absolute numbers of each class\n",
        "# 2. Pie chart: Shows proportional distribution\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üéØ TARGET VARIABLE DISTRIBUTION ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Calculate class distribution\n",
        "diagnosis_counts = df['diagnosis_label'].value_counts()\n",
        "diagnosis_percentages = df['diagnosis_label'].value_counts(normalize=True) * 100\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìä CLASS DISTRIBUTION (ABSOLUTE COUNTS):\")\n",
        "print(\"-\" * 80)\n",
        "for label, count in diagnosis_counts.items():\n",
        "    print(f\"   ‚Ä¢ {label:12s}: {count:3d} cases\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìä CLASS DISTRIBUTION (PERCENTAGES):\")\n",
        "print(\"-\" * 80)\n",
        "for label, pct in diagnosis_percentages.items():\n",
        "    print(f\"   ‚Ä¢ {label:12s}: {pct:5.2f}%\")\n",
        "\n",
        "# Calculate imbalance ratio\n",
        "majority_count = diagnosis_counts.values[0]\n",
        "minority_count = diagnosis_counts.values[1]\n",
        "imbalance_ratio = majority_count / minority_count\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"‚öñÔ∏è CLASS BALANCE ANALYSIS:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"   ‚Ä¢ Majority class: {diagnosis_counts.index[0]} ({majority_count} samples)\")\n",
        "print(f\"   ‚Ä¢ Minority class: {diagnosis_counts.index[1]} ({minority_count} samples)\")\n",
        "print(f\"   ‚Ä¢ Imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
        "\n",
        "if imbalance_ratio < 1.5:\n",
        "    balance_status = \"‚úÖ WELL BALANCED\"\n",
        "    recommendation = \"Standard ML algorithms will work well without modifications.\"\n",
        "elif imbalance_ratio < 3:\n",
        "    balance_status = \"‚ö†Ô∏è SLIGHT IMBALANCE\"\n",
        "    recommendation = \"Consider monitoring precision and recall separately. May use class weights.\"\n",
        "else:\n",
        "    balance_status = \"‚ùå SIGNIFICANT IMBALANCE\"\n",
        "    recommendation = \"Should use: class weights, SMOTE, or focus on F1-score/AUC metrics.\"\n",
        "\n",
        "print(f\"\\n   Status: {balance_status}\")\n",
        "print(f\"   Recommendation: {recommendation}\")\n",
        "\n",
        "# Visualization\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìä GENERATING VISUALIZATIONS...\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Count plot with detailed annotations\n",
        "bars = axes[0].bar(range(len(diagnosis_counts)), diagnosis_counts.values,\n",
        "                   color=['#FF6B6B', '#4ECDC4'], alpha=0.8, edgecolor='black', linewidth=2)\n",
        "axes[0].set_xticks(range(len(diagnosis_counts)))\n",
        "axes[0].set_xticklabels(diagnosis_counts.index, fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('Number of Cases', fontsize=13, fontweight='bold')\n",
        "axes[0].set_title('Distribution of Breast Cancer Diagnosis\\n(Absolute Counts)',\n",
        "                  fontsize=14, fontweight='bold', pad=20)\n",
        "axes[0].grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, (bar, count, pct) in enumerate(zip(bars, diagnosis_counts.values, diagnosis_percentages.values)):\n",
        "    height = bar.get_height()\n",
        "    axes[0].text(bar.get_x() + bar.get_width()/2., height + 5,\n",
        "                f'{count}\\n({pct:.1f}%)',\n",
        "                ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Pie chart with enhanced styling\n",
        "colors = ['#FF6B6B', '#4ECDC4']\n",
        "explode = (0.05, 0.05)  # Slightly separate both slices\n",
        "wedges, texts, autotexts = axes[1].pie(diagnosis_counts.values,\n",
        "                                        labels=diagnosis_counts.index,\n",
        "                                        autopct='%1.1f%%',\n",
        "                                        colors=colors,\n",
        "                                        explode=explode,\n",
        "                                        startangle=90,\n",
        "                                        textprops={'fontsize': 12, 'fontweight': 'bold'},\n",
        "                                        shadow=True)\n",
        "axes[1].set_title('Proportion of Malignant vs Benign Cases\\n(Percentage Distribution)',\n",
        "                  fontsize=14, fontweight='bold', pad=20)\n",
        "\n",
        "# Enhance autotext\n",
        "for autotext in autotexts:\n",
        "    autotext.set_color('white')\n",
        "    autotext.set_fontsize(13)\n",
        "    autotext.set_weight('bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üîç FEATURE SEPARATION ANALYSIS:\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nHow well can each mean feature distinguish between malignant and benign?\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Calculate separation metrics for each mean feature\n",
        "separation_metrics = []\n",
        "for feature in mean_features:\n",
        "    mal_mean = df[df['diagnosis'] == 0][feature].mean()\n",
        "    ben_mean = df[df['diagnosis'] == 1][feature].mean()\n",
        "    pooled_std = df[feature].std()\n",
        "    separation = abs(mal_mean - ben_mean) / pooled_std\n",
        "    separation_metrics.append({\n",
        "        'Feature': feature.replace('mean ', ''),\n",
        "        'Separation Score': separation,\n",
        "        'Malignant Mean': mal_mean,\n",
        "        'Benign Mean': ben_mean\n",
        "    })\n",
        "\n",
        "separation_df = pd.DataFrame(separation_metrics).sort_values('Separation Score', ascending=False)\n",
        "print(separation_df.to_string(index=False))\n",
        "\n",
        "print(\"\\nüí° Interpretation:\")\n",
        "print(\"   ‚Ä¢ Higher separation score = Better at distinguishing classes\")\n",
        "print(\"   ‚Ä¢ Features with score > 1.0 are particularly discriminative\")\n",
        "print(f\"   ‚Ä¢ Top 3 features: {', '.join(separation_df.head(3)['Feature'].values)}\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 7: CORRELATION ANALYSIS - UNDERSTANDING FEATURE RELATIONSHIPS\n",
        "# ============================================================================\n",
        "#\n",
        "# SECTION PURPOSE:\n",
        "# ----------------\n",
        "# This cell analyzes correlations between features to understand:\n",
        "# 1. Which features are related to each other (multicollinearity)\n",
        "# 2. Which features are most related to our target (diagnosis)\n",
        "# 3. Potential redundancy in our feature set\n",
        "#\n",
        "# WHAT IS CORRELATION?\n",
        "# --------------------\n",
        "# Correlation measures the linear relationship between two variables:\n",
        "# ‚Ä¢ +1.0: Perfect positive correlation (as one increases, other increases)\n",
        "# ‚Ä¢  0.0: No linear relationship\n",
        "# ‚Ä¢ -1.0: Perfect negative correlation (as one increases, other decreases)\n",
        "#\n",
        "# CORRELATION RANGES INTERPRETATION:\n",
        "# -----------------------------------\n",
        "# ‚Ä¢ 0.00 - 0.19: Very weak correlation\n",
        "# ‚Ä¢ 0.20 - 0.39: Weak correlation\n",
        "# ‚Ä¢ 0.40 - 0.59: Moderate correlation\n",
        "# ‚Ä¢ 0.60 - 0.79: Strong correlation\n",
        "# ‚Ä¢ 0.80 - 1.00: Very strong correlation\n",
        "#\n",
        "# WHY CORRELATION ANALYSIS MATTERS FOR ML:\n",
        "# -----------------------------------------\n",
        "#\n",
        "# 1. FEATURE SELECTION:\n",
        "#    ‚Ä¢ Highly correlated features provide redundant information\n",
        "#    ‚Ä¢ Example: radius, perimeter, and area are highly correlated\n",
        "#    ‚Ä¢ We might only need one of them for prediction\n",
        "#\n",
        "# 2. MULTICOLLINEARITY ISSUES:\n",
        "#    ‚Ä¢ Some algorithms (like linear regression) sensitive to multicollinearity\n",
        "#    ‚Ä¢ Can inflate variance of coefficient estimates\n",
        "#    ‚Ä¢ Makes interpretation of individual features difficult\n",
        "#\n",
        "# 3. MODEL EFFICIENCY:\n",
        "#    ‚Ä¢ Removing correlated features can:\n",
        "#      - Speed up training\n",
        "#      - Reduce model complexity\n",
        "#      - Improve interpretability\n",
        "#      - Sometimes improve performance\n",
        "#\n",
        "# 4. FEATURE IMPORTANCE:\n",
        "#    ‚Ä¢ Features highly correlated with target are most predictive\n",
        "#    ‚Ä¢ Helps prioritize which features to focus on\n",
        "#\n",
        "# CORRELATION WITH TARGET (DIAGNOSIS):\n",
        "# -------------------------------------\n",
        "# ‚Ä¢ Positive correlation with diagnosis:\n",
        "#   ‚Üí Higher feature values ‚Üí More likely BENIGN (diagnosis = 1)\n",
        "#   ‚Üí These features are LOWER in malignant tumors\n",
        "#\n",
        "# ‚Ä¢ Negative correlation with diagnosis:\n",
        "#   ‚Üí Higher feature values ‚Üí More likely MALIGNANT (diagnosis = 0)\n",
        "#   ‚Üí These features are HIGHER in malignant tumors\n",
        "#\n",
        "# EXPECTED PATTERNS:\n",
        "# ------------------\n",
        "# Based on medical knowledge, we expect:\n",
        "# ‚Ä¢ Size features (radius, perimeter, area) correlated with malignancy\n",
        "# ‚Ä¢ Irregularity features (concavity, concave points) correlated with malignancy\n",
        "# ‚Ä¢ Smoothness correlated with benign tumors\n",
        "# ‚Ä¢ Size features highly correlated with each other\n",
        "#\n",
        "# VISUALIZATION NOTES:\n",
        "# --------------------\n",
        "# ‚Ä¢ Heatmap colors:\n",
        "#   - Red/Hot colors: Positive correlation\n",
        "#   - Blue/Cool colors: Negative correlation\n",
        "#   - White: No correlation\n",
        "# ‚Ä¢ Diagonal: Always 1.0 (feature correlated with itself)\n",
        "# ‚Ä¢ Symmetric: correlation(A,B) = correlation(B,A)\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üîó CORRELATION ANALYSIS: UNDERSTANDING FEATURE RELATIONSHIPS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìä CALCULATING CORRELATION MATRIX...\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Calculate correlation matrix for mean features + diagnosis\n",
        "correlation_matrix = df[mean_features + ['diagnosis']].corr()\n",
        "\n",
        "print(\"‚úÖ Correlation matrix computed successfully!\")\n",
        "print(f\"   Matrix dimensions: {correlation_matrix.shape}\")\n",
        "print(f\"   Total correlations calculated: {correlation_matrix.shape[0] * correlation_matrix.shape[1]}\")\n",
        "\n",
        "# Find features most correlated with diagnosis\n",
        "diagnosis_correlations = correlation_matrix['diagnosis'].drop('diagnosis').sort_values(ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üéØ FEATURES MOST CORRELATED WITH DIAGNOSIS (BENIGN):\")\n",
        "print(\"-\" * 80)\n",
        "print(\"\\nPositive correlations (higher values ‚Üí more likely benign):\")\n",
        "print(diagnosis_correlations[diagnosis_correlations > 0].to_string())\n",
        "\n",
        "print(\"\\nNegative correlations (higher values ‚Üí more likely malignant):\")\n",
        "print(diagnosis_correlations[diagnosis_correlations < 0].to_string())\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üí° INTERPRETATION:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"\"\"\n",
        "TOP PREDICTORS OF MALIGNANCY (strongest negative correlations):\n",
        "1. {diagnosis_correlations.index[-1]}: {diagnosis_correlations.iloc[-1]:.3f}\n",
        "2. {diagnosis_correlations.index[-2]}: {diagnosis_correlations.iloc[-2]:.3f}\n",
        "3. {diagnosis_correlations.index[-3]}: {diagnosis_correlations.iloc[-3]:.3f}\n",
        "\n",
        "Clinical Meaning:\n",
        "‚Ä¢ Malignant tumors tend to have HIGHER values for these features\n",
        "‚Ä¢ These features show the strongest discriminative power\n",
        "‚Ä¢ Should be prioritized in feature selection\n",
        "\"\"\")\n",
        "\n",
        "# Find highly correlated feature pairs (multicollinearity detection)\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"‚ö†Ô∏è MULTICOLLINEARITY DETECTION:\")\n",
        "print(\"-\" * 80)\n",
        "print(\"\\nHighly correlated feature pairs (|correlation| > 0.9):\")\n",
        "\n",
        "high_corr_pairs = []\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i+1, len(correlation_matrix.columns)):\n",
        "        if abs(correlation_matrix.iloc[i, j]) > 0.9:\n",
        "            high_corr_pairs.append({\n",
        "                'Feature 1': correlation_matrix.columns[i],\n",
        "                'Feature 2': correlation_matrix.columns[j],\n",
        "                'Correlation': correlation_matrix.iloc[i, j]\n",
        "            })\n",
        "\n",
        "if high_corr_pairs:\n",
        "    high_corr_df = pd.DataFrame(high_corr_pairs).sort_values('Correlation', ascending=False)\n",
        "    print(high_corr_df.to_string(index=False))\n",
        "    print(f\"\\n‚ö†Ô∏è Found {len(high_corr_pairs)} highly correlated pairs\")\n",
        "    print(\"   Consider removing redundant features to reduce multicollinearity\")\n",
        "else:\n",
        "    print(\"‚úÖ No extreme multicollinearity detected (all |correlations| < 0.9)\")\n",
        "\n",
        "# Visualization: Correlation heatmap\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìä GENERATING CORRELATION HEATMAP...\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "plt.figure(figsize=(16, 14))\n",
        "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))  # Mask upper triangle\n",
        "sns.heatmap(correlation_matrix,\n",
        "            annot=True,\n",
        "            fmt='.2f',\n",
        "            cmap='coolwarm',\n",
        "            center=0,\n",
        "            square=True,\n",
        "            linewidths=1,\n",
        "            cbar_kws={\"shrink\": 0.8, \"label\": \"Correlation Coefficient\"},\n",
        "            vmin=-1,\n",
        "            vmax=1,\n",
        "            mask=mask)\n",
        "\n",
        "plt.title('Correlation Heatmap of Mean Features and Diagnosis\\n' +\n",
        "          'Red = Positive Correlation | Blue = Negative Correlation',\n",
        "          fontsize=16, fontweight='bold', pad=20)\n",
        "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
        "plt.yticks(rotation=0, fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üîç KEY CORRELATION INSIGHTS:\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\"\"\n",
        "1. SIZE FEATURES ARE HIGHLY CORRELATED:\n",
        "   ‚Ä¢ Radius, perimeter, and area show very strong correlations\n",
        "   ‚Ä¢ This makes sense: larger radius ‚Üí larger perimeter ‚Üí larger area\n",
        "   ‚Ä¢ Implication: We might only need one size feature for modeling\n",
        "\n",
        "2. TEXTURE AND IRREGULARITY FEATURES:\n",
        "   ‚Ä¢ Concavity and concave points are strongly correlated\n",
        "   ‚Ä¢ Both measure tumor boundary irregularity\n",
        "   ‚Ä¢ Malignant tumors show higher values for both\n",
        "\n",
        "3. DIAGNOSIS CORRELATIONS:\n",
        "   ‚Ä¢ Negative correlations indicate features HIGHER in malignant tumors\n",
        "   ‚Ä¢ These are our most important predictive features\n",
        "   ‚Ä¢ Focus on: concave points, perimeter, radius, area\n",
        "\n",
        "4. FEATURE REDUNDANCY:\n",
        "   ‚Ä¢ Some feature pairs are highly redundant (correlation > 0.9)\n",
        "   ‚Ä¢ Could perform feature selection to remove redundancy\n",
        "   ‚Ä¢ This would simplify models without losing information\n",
        "\n",
        "5. CLINICAL VALIDATION:\n",
        "   ‚Ä¢ Correlation patterns match medical knowledge\n",
        "   ‚Ä¢ Larger, more irregular tumors tend to be malignant\n",
        "   ‚Ä¢ Validates the quality and clinical relevance of our data\n",
        "\"\"\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 8: BOX PLOTS - OUTLIER DETECTION AND DISTRIBUTION ANALYSIS\n",
        "# ============================================================================\n",
        "#\n",
        "# SECTION PURPOSE:\n",
        "# ----------------\n",
        "# This cell uses box plots to visualize feature distributions and identify\n",
        "# outliers for both malignant and benign cases. Box plots are excellent\n",
        "# for understanding data spread and detecting unusual values.\n",
        "#\n",
        "# WHAT IS A BOX PLOT?\n",
        "# -------------------\n",
        "# A box plot (box-and-whisker plot) displays data distribution through quartiles:\n",
        "#\n",
        "#     Maximum (within 1.5 IQR) -------|\n",
        "#                                     |\n",
        "#     75th percentile (Q3) -----------+  ‚Üê Top of box\n",
        "#                                     |\n",
        "#     50th percentile (Median) -------+  ‚Üê Line inside box\n",
        "#                                     |\n",
        "#     25th percentile (Q1) -----------+  ‚Üê Bottom of box\n",
        "#                                     |\n",
        "#     Minimum (within 1.5 IQR) -------|\n",
        "#\n",
        "#     ‚Ä¢ = Outliers (beyond 1.5 √ó IQR from quartiles)\n",
        "#\n",
        "# KEY COMPONENTS:\n",
        "# ---------------\n",
        "# ‚Ä¢ BOX: Contains middle 50% of data (IQR = Q3 - Q1)\n",
        "# ‚Ä¢ LINE IN BOX: Median (50th percentile)\n",
        "# ‚Ä¢ WHISKERS: Extend to min/max within 1.5 √ó IQR\n",
        "# ‚Ä¢ DOTS: Outliers beyond whiskers\n",
        "#\n",
        "# WHY BOX PLOTS MATTER FOR ML:\n",
        "# -----------------------------\n",
        "#\n",
        "# 1. OUTLIER DETECTION:\n",
        "#    ‚Ä¢ Outliers can significantly impact model performance\n",
        "#    ‚Ä¢ Some algorithms (like SVM, Linear Regression) sensitive to outliers\n",
        "#    ‚Ä¢ Tree-based models (Random Forest, Decision Tree) more robust\n",
        "#\n",
        "# 2. DISTRIBUTION UNDERSTANDING:\n",
        "#    ‚Ä¢ Skewed data may need transformation\n",
        "#    ‚Ä¢ Symmetric distributions work better with some algorithms\n",
        "#    ‚Ä¢ Identifies features with heavy tails\n",
        "#\n",
        "# 3. CLASS SEPARATION:\n",
        "#    ‚Ä¢ Minimal overlap between boxes ‚Üí Good feature for classification\n",
        "#    ‚Ä¢ Large overlap ‚Üí Feature less discriminative\n",
        "#    ‚Ä¢ Different medians ‚Üí Feature can distinguish classes\n",
        "#\n",
        "# 4. SCALE DIFFERENCES:\n",
        "#    ‚Ä¢ Confirms need for feature scaling\n",
        "#    ‚Ä¢ Different features have vastly different ranges\n",
        "#\n",
        "# OUTLIER HANDLING STRATEGIES:\n",
        "# -----------------------------\n",
        "#\n",
        "# OPTION 1 - KEEP OUTLIERS:\n",
        "# ‚Ä¢ Use when: Outliers are valid measurements, not errors\n",
        "# ‚Ä¢ ML algorithms: Tree-based (Random Forest, Decision Trees)\n",
        "# ‚Ä¢ Our case: Medical measurements, outliers likely real\n",
        "#\n",
        "# OPTION 2 - REMOVE OUTLIERS:\n",
        "# ‚Ä¢ Use when: Outliers are measurement errors\n",
        "# ‚Ä¢ Risk: Losing potentially important cases\n",
        "# ‚Ä¢ Caution: In medical data, \"outliers\" might be critical cases\n",
        "#\n",
        "# OPTION 3 - TRANSFORM DATA:\n",
        "# ‚Ä¢ Log transformation for right-skewed data\n",
        "# ‚Ä¢ Square root transformation\n",
        "# ‚Ä¢ Box-Cox transformation\n",
        "#\n",
        "# OPTION 4 - USE ROBUST SCALERS:\n",
        "# ‚Ä¢ Robust Scaler: Uses median and IQR instead of mean and std\n",
        "# ‚Ä¢ Less sensitive to outliers\n",
        "# ‚Ä¢ Good compromise for our dataset\n",
        "#\n",
        "# MEDICAL CONTEXT:\n",
        "# ----------------\n",
        "# In cancer diagnosis:\n",
        "# ‚Ä¢ Outliers might represent unusual but real tumor characteristics\n",
        "# ‚Ä¢ Extreme values could be most diagnostically important\n",
        "# ‚Ä¢ Should NOT automatically remove outliers without domain expertise\n",
        "# ‚Ä¢ \"Worst\" features intentionally capture extreme values\n",
        "#\n",
        "# WHAT TO LOOK FOR:\n",
        "# -----------------\n",
        "# ‚úì Features with clear separation between malignant and benign boxes\n",
        "# ‚úì Presence of outliers (common in medical data)\n",
        "# ‚úì Skewness in distributions\n",
        "# ‚úì Features with minimal overlap (best for classification)\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üì¶ BOX PLOT ANALYSIS: OUTLIER DETECTION & DISTRIBUTION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìä UNDERSTANDING BOX PLOTS:\")\n",
        "print(\"-\" * 80)\n",
        "print(\"\"\"\n",
        "Box Plot Components:\n",
        "‚Ä¢ Box edges: 25th (Q1) and 75th (Q3) percentiles\n",
        "‚Ä¢ Line in box: Median (50th percentile)\n",
        "‚Ä¢ Whiskers: Extend to 1.5 √ó IQR from box edges\n",
        "‚Ä¢ Individual points: Outliers beyond whiskers\n",
        "\n",
        "What We're Looking For:\n",
        "1. Separation between Malignant and Benign boxes\n",
        "2. Presence and frequency of outliers\n",
        "3. Symmetry vs. skewness of distributions\n",
        "4. Features with minimal overlap (best predictors)\n",
        "\"\"\")\n",
        "\n",
        "# Count outliers for each feature\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üîç OUTLIER ANALYSIS:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "outlier_summary = []\n",
        "for feature in mean_features:\n",
        "    # Calculate outliers using IQR method\n",
        "    Q1 = df[feature].quantile(0.25)\n",
        "    Q3 = df[feature].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)]\n",
        "    outlier_pct = (len(outliers) / len(df)) * 100\n",
        "\n",
        "    outlier_summary.append({\n",
        "        'Feature': feature.replace('mean ', ''),\n",
        "        'Outlier Count': len(outliers),\n",
        "        'Outlier %': outlier_pct,\n",
        "        'Lower Bound': lower_bound,\n",
        "        'Upper Bound': upper_bound\n",
        "    })\n",
        "\n",
        "outlier_df = pd.DataFrame(outlier_summary).sort_values('Outlier Count', ascending=False)\n",
        "print(\"\\nFeatures sorted by number of outliers:\")\n",
        "print(outlier_df.to_string(index=False))\n",
        "\n",
        "total_outliers = outlier_df['Outlier Count'].sum()\n",
        "print(f\"\\nüìä Total outlier occurrences across all mean features: {total_outliers}\")\n",
        "print(f\"   Average outliers per feature: {total_outliers / len(mean_features):.1f}\")\n",
        "\n",
        "if outlier_df['Outlier %'].max() > 10:\n",
        "    print(\"\\n‚ö†Ô∏è Some features have >10% outliers - consider robust scaling\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ Outlier percentages are reasonable (<10% per feature)\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìä GENERATING BOX PLOTS BY DIAGNOSIS...\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Create box plots\n",
        "fig, axes = plt.subplots(2, 5, figsize=(24, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, feature in enumerate(mean_features):\n",
        "    # Create box plot grouped by diagnosis\n",
        "    data_to_plot = [df[df['diagnosis_label'] == 'Malignant'][feature],\n",
        "                    df[df['diagnosis_label'] == 'Benign'][feature]]\n",
        "\n",
        "    bp = axes[idx].boxplot(data_to_plot,\n",
        "                           labels=['Malignant', 'Benign'],\n",
        "                           patch_artist=True,\n",
        "                           widths=0.6,\n",
        "                           showmeans=True,\n",
        "                           meanprops=dict(marker='D', markerfacecolor='yellow',\n",
        "                                        markeredgecolor='black', markersize=8))\n",
        "\n",
        "    # Color the boxes\n",
        "    bp['boxes'][0].set_facecolor('#FF6B6B')\n",
        "    bp['boxes'][0].set_alpha(0.7)\n",
        "    bp['boxes'][1].set_facecolor('#4ECDC4')\n",
        "    bp['boxes'][1].set_alpha(0.7)\n",
        "\n",
        "    # Styling\n",
        "    axes[idx].set_title(feature.replace('mean ', '').title(),\n",
        "                        fontsize=12, fontweight='bold')\n",
        "    axes[idx].set_ylabel('Value', fontsize=10)\n",
        "    axes[idx].grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "    # Add feature statistics\n",
        "    mal_median = df[df['diagnosis_label'] == 'Malignant'][feature].median()\n",
        "    ben_median = df[df['diagnosis_label'] == 'Benign'][feature].median()\n",
        "    axes[idx].text(0.02, 0.98, f'M: {mal_median:.2f}\\nB: {ben_median:.2f}',\n",
        "                   transform=axes[idx].transAxes, fontsize=9,\n",
        "                   verticalalignment='top', bbox=dict(boxstyle='round',\n",
        "                   facecolor='wheat', alpha=0.5))\n",
        "\n",
        "plt.suptitle('Box Plots of Mean Features by Diagnosis\\n' +\n",
        "             'Red = Malignant | Teal = Benign | Yellow Diamond = Mean | Line in Box = Median',\n",
        "             fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üí° BOX PLOT INSIGHTS:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Identify features with best separation\n",
        "separation_scores = []\n",
        "for feature in mean_features:\n",
        "    mal_median = df[df['diagnosis'] == 0][feature].median()\n",
        "    ben_median = df[df['diagnosis'] == 1][feature].median()\n",
        "    pooled_iqr = df[feature].quantile(0.75) - df[feature].quantile(0.25)\n",
        "    separation = abs(mal_median - ben_median) / pooled_iqr if pooled_iqr > 0 else 0\n",
        "    separation_scores.append((feature.replace('mean ', ''), separation))\n",
        "\n",
        "separation_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(f\"\"\"\n",
        "1. BEST SEPARATING FEATURES (least box overlap):\n",
        "   Top 5 features with clearest malignant/benign distinction:\n",
        "   ‚Ä¢ {separation_scores[0][0]}: {separation_scores[0][1]:.2f} (highest separation)\n",
        "   ‚Ä¢ {separation_scores[1][0]}: {separation_scores[1][1]:.2f}\n",
        "   ‚Ä¢ {separation_scores[2][0]}: {separation_scores[2][1]:.2f}\n",
        "   ‚Ä¢ {separation_scores[3][0]}: {separation_scores[3][1]:.2f}\n",
        "   ‚Ä¢ {separation_scores[4][0]}: {separation_scores[4][1]:.2f}\n",
        "\n",
        "2. OUTLIER PATTERNS:\n",
        "   ‚Ä¢ Outliers present in most features (typical for medical data)\n",
        "   ‚Ä¢ Outliers appear in both malignant and benign cases\n",
        "   ‚Ä¢ NOT measurement errors - represent natural biological variation\n",
        "   ‚Ä¢ Recommendation: KEEP outliers, use robust ML algorithms\n",
        "\n",
        "3. DISTRIBUTION SHAPES:\n",
        "   ‚Ä¢ Some features show right-skewed distributions\n",
        "   ‚Ä¢ Size features (area, perimeter) particularly skewed\n",
        "   ‚Ä¢ Malignant tumors tend to have longer right tails\n",
        "   ‚Ä¢ Confirms need for standardization before modeling\n",
        "\n",
        "4. CLASS SEPARATION:\n",
        "   ‚Ä¢ Clear median differences in most features\n",
        "   ‚Ä¢ Malignant boxes generally higher for size/irregularity features\n",
        "   ‚Ä¢ Good news: Features show discriminative power\n",
        "   ‚Ä¢ Multiple features can distinguish between classes\n",
        "\n",
        "5. MODELING RECOMMENDATIONS:\n",
        "   ‚Ä¢ Use StandardScaler or RobustScaler for preprocessing\n",
        "   ‚Ä¢ Don't remove outliers - they're medically relevant\n",
        "   ‚Ä¢ Focus on top separating features for feature selection\n",
        "   ‚Ä¢ Ensemble methods (Random Forest) will handle outliers well\n",
        "\"\"\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 9: DATA PREPARATION FOR MACHINE LEARNING\n",
        "# ============================================================================\n",
        "#\n",
        "# SECTION PURPOSE:\n",
        "# ----------------\n",
        "# This critical cell prepares our data for machine learning by:\n",
        "# 1. Separating features (X) from target variable (y)\n",
        "# 2. Splitting data into training and testing sets\n",
        "# 3. Ensuring proper stratification for balanced representation\n",
        "#\n",
        "# WHY TRAIN-TEST SPLIT?\n",
        "# ----------------------\n",
        "# We NEVER train and test on the same data because:\n",
        "#\n",
        "# PROBLEM WITHOUT SPLIT:\n",
        "# ‚Ä¢ Model memorizes training data (overfitting)\n",
        "# ‚Ä¢ Perfect training performance, poor real-world performance\n",
        "# ‚Ä¢ Can't assess true generalization ability\n",
        "# ‚Ä¢ Like studying with exam answers - doesn't test real knowledge\n",
        "#\n",
        "# SOLUTION WITH SPLIT:\n",
        "# ‚Ä¢ Training set (80%): Model learns patterns from this data\n",
        "# ‚Ä¢ Testing set (20%): Model evaluated on unseen data\n",
        "# ‚Ä¢ Tests generalization: Can model handle new cases?\n",
        "# ‚Ä¢ Realistic performance estimate for real-world deployment\n",
        "#\n",
        "# TRAIN-TEST SPLIT RATIO:\n",
        "# -----------------------\n",
        "# Common splits:\n",
        "# ‚Ä¢ 80-20: Standard choice (our choice) - good balance\n",
        "# ‚Ä¢ 70-30: More test data for better evaluation estimate\n",
        "# ‚Ä¢ 90-10: More training data when dataset is small\n",
        "# ‚Ä¢ 60-20-20: Train-Validation-Test for hyperparameter tuning\n",
        "#\n",
        "# Our choice: 80-20 split\n",
        "# ‚Ä¢ 455 training samples (80% of 569)\n",
        "# ‚Ä¢ 114 testing samples (20% of 569)\n",
        "# ‚Ä¢ Adequate for both training and evaluation\n",
        "#\n",
        "# WHAT IS STRATIFICATION?\n",
        "# ------------------------\n",
        "# Stratification ensures training and test sets have the same class proportions\n",
        "# as the original dataset.\n",
        "#\n",
        "# WITHOUT STRATIFICATION:\n",
        "# ‚Ä¢ Random split might give unbalanced sets\n",
        "# ‚Ä¢ Example: Test set could have 70% benign (vs 63% in full data)\n",
        "# ‚Ä¢ Evaluation metrics become unreliable\n",
        "# ‚Ä¢ Model might not see enough of minority class\n",
        "#\n",
        "# WITH STRATIFICATION (stratify=y):\n",
        "# ‚Ä¢ Both sets maintain 63% benign, 37% malignant ratio\n",
        "# ‚Ä¢ More reliable evaluation\n",
        "# ‚Ä¢ Model sees representative sample during training\n",
        "# ‚Ä¢ Critical for imbalanced datasets\n",
        "#\n",
        "# RANDOM STATE PARAMETER:\n",
        "# -----------------------\n",
        "# random_state=42 ensures reproducibility:\n",
        "# ‚Ä¢ Same split every time we run the code\n",
        "# ‚Ä¢ Results are reproducible\n",
        "# ‚Ä¢ Others can verify our findings\n",
        "# ‚Ä¢ \"42\" is arbitrary (common choice from \"Hitchhiker's Guide to Galaxy\")\n",
        "#\n",
        "# FEATURE MATRIX (X) vs TARGET VECTOR (y):\n",
        "# -----------------------------------------\n",
        "# X (Features):\n",
        "# ‚Ä¢ Contains all 30 numeric measurements\n",
        "# ‚Ä¢ Input variables model uses for prediction\n",
        "# ‚Ä¢ Shape: (569, 30) - 569 samples, 30 features\n",
        "#\n",
        "# y (Target):\n",
        "# ‚Ä¢ Contains diagnosis labels\n",
        "# ‚Ä¢ What we're trying to predict\n",
        "# ‚Ä¢ Shape: (569,) - 569 labels\n",
        "# ‚Ä¢ Binary: 0 (Malignant) or 1 (Benign)\n",
        "#\n",
        "# DATA LEAKAGE WARNING:\n",
        "# ---------------------\n",
        "# CRITICAL: We must split data BEFORE any preprocessing!\n",
        "#\n",
        "# WRONG ORDER (causes data leakage):\n",
        "# 1. Scale entire dataset\n",
        "# 2. Split into train/test\n",
        "# Result: Test data influenced training ‚Üí Overoptimistic results\n",
        "#\n",
        "# CORRECT ORDER (what we're doing):\n",
        "# 1. Split into train/test\n",
        "# 2. Fit scaler on training data only\n",
        "# 3. Transform both sets using training scaler\n",
        "# Result: Test data truly unseen ‚Üí Realistic results\n",
        "#\n",
        "# MEDICAL CONTEXT:\n",
        "# ----------------\n",
        "# In clinical deployment:\n",
        "# ‚Ä¢ Training data: Historical patient cases with known outcomes\n",
        "# ‚Ä¢ Test data: Simulates new patients coming for diagnosis\n",
        "# ‚Ä¢ Model must work on patients it has never seen before\n",
        "# ‚Ä¢ Our train-test split simulates this real-world scenario\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üîß DATA PREPARATION FOR MACHINE LEARNING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìã STEP 1: SEPARATING FEATURES AND TARGET\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = df[data.feature_names]  # All 30 numeric features\n",
        "y = df['diagnosis']          # Target variable (0=Malignant, 1=Benign)\n",
        "\n",
        "print(f\"\\n‚úÖ Data separated successfully!\")\n",
        "print(f\"\\nFeature matrix (X):\")\n",
        "print(f\"   ‚Ä¢ Shape: {X.shape}\")\n",
        "print(f\"   ‚Ä¢ {X.shape[0]} samples (patients)\")\n",
        "print(f\"   ‚Ä¢ {X.shape[1]} features (measurements)\")\n",
        "print(f\"   ‚Ä¢ Memory usage: {X.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
        "\n",
        "print(f\"\\nTarget vector (y):\")\n",
        "print(f\"   ‚Ä¢ Shape: {y.shape}\")\n",
        "print(f\"   ‚Ä¢ {y.shape[0]} labels\")\n",
        "print(f\"   ‚Ä¢ Values: {y.unique()}\")\n",
        "print(f\"   ‚Ä¢ Class distribution: {y.value_counts().to_dict()}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìã STEP 2: TRAIN-TEST SPLIT\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "print(\"\\nSplit configuration:\")\n",
        "print(\"   ‚Ä¢ Train size: 80% of data\")\n",
        "print(\"   ‚Ä¢ Test size: 20% of data\")\n",
        "print(\"   ‚Ä¢ Stratification: YES (maintains class balance)\")\n",
        "print(\"   ‚Ä¢ Random state: 42 (for reproducibility)\")\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,      # 20% for testing\n",
        "    random_state=42,    # Reproducibility\n",
        "    stratify=y          # Maintain class distribution\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Data split completed!\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìä SPLIT SUMMARY:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "print(f\"\\nüéì TRAINING SET:\")\n",
        "print(f\"   ‚Ä¢ Total samples: {len(X_train)}\")\n",
        "print(f\"   ‚Ä¢ Features: {X_train.shape[1]}\")\n",
        "print(f\"   ‚Ä¢ Malignant cases: {(y_train == 0).sum()} ({(y_train == 0).sum()/len(y_train)*100:.1f}%)\")\n",
        "print(f\"   ‚Ä¢ Benign cases: {(y_train == 1).sum()} ({(y_train == 1).sum()/len(y_train)*100:.1f}%)\")\n",
        "print(f\"   ‚Ä¢ Purpose: Model learns patterns from this data\")\n",
        "\n",
        "print(f\"\\nüß™ TESTING SET:\")\n",
        "print(f\"   ‚Ä¢ Total samples: {len(X_test)}\")\n",
        "print(f\"   ‚Ä¢ Features: {X_test.shape[1]}\")\n",
        "print(f\"   ‚Ä¢ Malignant cases: {(y_test == 0).sum()} ({(y_test == 0).sum()/len(y_test)*100:.1f}%)\")\n",
        "print(f\"   ‚Ä¢ Benign cases: {(y_test == 1).sum()} ({(y_test == 1).sum()/len(y_test)*100:.1f}%)\")\n",
        "print(f\"   ‚Ä¢ Purpose: Evaluate model on unseen data\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"‚úÖ STRATIFICATION VERIFICATION:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "original_benign_pct = (y == 1).sum() / len(y) * 100\n",
        "train_benign_pct = (y_train == 1).sum() / len(y_train) * 100\n",
        "test_benign_pct = (y_test == 1).sum() / len(y_test) * 100\n",
        "\n",
        "print(f\"\\nBenign class percentage:\")\n",
        "print(f\"   ‚Ä¢ Original dataset: {original_benign_pct:.2f}%\")\n",
        "print(f\"   ‚Ä¢ Training set: {train_benign_pct:.2f}%\")\n",
        "print(f\"   ‚Ä¢ Testing set: {test_benign_pct:.2f}%\")\n",
        "print(f\"   ‚Ä¢ Difference (train-test): {abs(train_benign_pct - test_benign_pct):.2f}%\")\n",
        "\n",
        "if abs(train_benign_pct - test_benign_pct) < 2:\n",
        "    print(\"\\n   ‚úÖ Excellent stratification! Class distributions match closely.\")\n",
        "else:\n",
        "    print(\"\\n   ‚ö†Ô∏è Stratification deviation detected, but within acceptable range.\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üí° KEY POINTS ABOUT OUR DATA SPLIT:\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\"\"\n",
        "1. PROPER SEPARATION:\n",
        "   ‚Ä¢ Features (X) and target (y) cleanly separated\n",
        "   ‚Ä¢ No data leakage between input and output\n",
        "   ‚Ä¢ Ready for supervised learning\n",
        "\n",
        "2. ADEQUATE SAMPLE SIZES:\n",
        "   ‚Ä¢ 455 training samples: Sufficient for learning patterns\n",
        "   ‚Ä¢ 114 test samples: Adequate for reliable evaluation\n",
        "   ‚Ä¢ Both sets large enough for robust results\n",
        "\n",
        "3. MAINTAINED CLASS BALANCE:\n",
        "   ‚Ä¢ Stratification ensures fair representation\n",
        "   ‚Ä¢ Both sets have similar malignant/benign ratios\n",
        "   ‚Ä¢ Prevents bias in evaluation metrics\n",
        "\n",
        "4. REPRODUCIBILITY:\n",
        "   ‚Ä¢ random_state=42 ensures identical splits\n",
        "   ‚Ä¢ Results can be replicated\n",
        "   ‚Ä¢ Important for scientific validity\n",
        "\n",
        "5. PREVENTS OVERFITTING:\n",
        "   ‚Ä¢ Model never sees test data during training\n",
        "   ‚Ä¢ Test performance reflects real-world capability\n",
        "   ‚Ä¢ Honest assessment of model generalization\n",
        "\n",
        "6. NEXT STEPS:\n",
        "   ‚Ä¢ ‚úì Data is split\n",
        "   ‚Ä¢ ‚Üí Need to scale features (different ranges)\n",
        "   ‚Ä¢ ‚Üí Then ready for model training\n",
        "   ‚Ä¢ ‚Üí Test set stays untouched until final evaluation\n",
        "\"\"\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 10: FEATURE SCALING (STANDARDIZATION)\n",
        "# ============================================================================\n",
        "#\n",
        "# SECTION PURPOSE:\n",
        "# ----------------\n",
        "# This cell standardizes our features to have mean=0 and standard deviation=1.\n",
        "# This is one of the most critical preprocessing steps for many ML algorithms.\n",
        "#\n",
        "# WHY FEATURE SCALING IS ESSENTIAL:\n",
        "# ----------------------------------\n",
        "# Our features have vastly different scales:\n",
        "# ‚Ä¢ mean radius: ~6 to ~28\n",
        "# ‚Ä¢ mean area: ~143 to ~2501\n",
        "# ‚Ä¢ mean smoothness: ~0.05 to ~0.16\n",
        "# ‚Ä¢ mean fractal dimension: ~0.05 to ~0.10\n",
        "#\n",
        "# PROBLEMS WITHOUT SCALING:\n",
        "# ---------------------------\n",
        "# 1. DISTANCE-BASED ALGORITHMS:\n",
        "#    ‚Ä¢ KNN, SVM, K-Means affected by scale\n",
        "#    ‚Ä¢ Features with larger ranges dominate distance calculations\n",
        "#    ‚Ä¢ Example: \"area\" (range ~2400) dominates \"smoothness\" (range ~0.11)\n",
        "#    ‚Ä¢ Model ignores smaller-scale features even if they're important\n",
        "#\n",
        "# 2. GRADIENT-BASED ALGORITHMS:\n",
        "#    ‚Ä¢ Logistic Regression, Neural Networks use gradient descent\n",
        "#    ‚Ä¢ Different scales cause slow/unstable convergence\n",
        "#    ‚Ä¢ Learning rate optimized for large features may be too large for small ones\n",
        "#    ‚Ä¢ Can lead to overshooting and training instability\n",
        "#\n",
        "# 3. REGULARIZATION:\n",
        "#    ‚Ä¢ L1/L2 penalties penalize large-scale features more\n",
        "#    ‚Ä¢ Unfair comparison between features\n",
        "#    ‚Ä¢ Model selection biased toward small-scale features\n",
        "#\n",
        "# ALGORITHMS THAT DON'T NEED SCALING:\n",
        "# ------------------------------------\n",
        "# ‚Ä¢ Tree-based: Decision Trees, Random Forest, Gradient Boosting\n",
        "# ‚Ä¢ Rule-based: Decision rules independent of scale\n",
        "# ‚Ä¢ However, scaling doesn't hurt these algorithms either\n",
        "#\n",
        "# STANDARDIZATION vs NORMALIZATION:\n",
        "# ----------------------------------\n",
        "#\n",
        "# STANDARDIZATION (StandardScaler) - What we're using:\n",
        "# Formula: z = (x - mean) / std\n",
        "# Result: Mean = 0, Std = 1\n",
        "# When to use:\n",
        "#   ‚Ä¢ Data is roughly normally distributed\n",
        "#   ‚Ä¢ Contains outliers (less sensitive than normalization)\n",
        "#   ‚Ä¢ Works with negative values\n",
        "#   ‚Ä¢ Most common choice for general ML\n",
        "#\n",
        "# NORMALIZATION (MinMaxScaler):\n",
        "# Formula: x_scaled = (x - min) / (max - min)\n",
        "# Result: Range [0, 1]\n",
        "# When to use:\n",
        "#   ‚Ä¢ Bounded range needed (e.g., neural networks)\n",
        "#   ‚Ä¢ Data not normally distributed\n",
        "#   ‚Ä¢ No outliers (sensitive to outliers)\n",
        "#\n",
        "# ROBUST SCALER:\n",
        "# Uses median and IQR instead of mean and std\n",
        "# When to use:\n",
        "#   ‚Ä¢ Many outliers present\n",
        "#   ‚Ä¢ Want to preserve outlier information\n",
        "#   ‚Ä¢ More robust to extreme values\n",
        "#\n",
        "# CRITICAL: FIT ON TRAIN, TRANSFORM BOTH:\n",
        "# ----------------------------------------\n",
        "# CORRECT (what we're doing):\n",
        "# 1. scaler.fit(X_train) ‚Üí Learn mean/std from training data only\n",
        "# 2. X_train_scaled = scaler.transform(X_train) ‚Üí Apply to training\n",
        "# 3. X_test_scaled = scaler.transform(X_test) ‚Üí Apply same transformation to test\n",
        "#\n",
        "# WRONG (causes data leakage):\n",
        "# 1. scaler.fit(all_data) ‚Üí Test data influences scaling parameters\n",
        "# 2. Result: Test set not truly independent ‚Üí Overoptimistic results\n",
        "#\n",
        "# WHY THIS MATTERS:\n",
        "# ‚Ä¢ In real deployment, new patients haven't been seen before\n",
        "# ‚Ä¢ Scaling parameters must come only from training data\n",
        "# ‚Ä¢ Test data simulates future unseen patients\n",
        "# ‚Ä¢ Using test data in fit() = peeking at future information\n",
        "#\n",
        "# WHAT STANDARDIZATION DOES:\n",
        "# ---------------------------\n",
        "# Before: Features have different means and standard deviations\n",
        "# After: All features centered at 0 with unit variance\n",
        "#\n",
        "# Example transformation:\n",
        "# ‚Ä¢ Original: mean_radius = 15.5, mean = 14.1, std = 3.5\n",
        "# ‚Ä¢ Standardized: (15.5 - 14.1) / 3.5 = 0.4\n",
        "#\n",
        "# Benefits:\n",
        "# ‚úì Features on comparable scales\n",
        "# ‚úì No feature dominates due to scale\n",
        "# ‚úì Faster model convergence\n",
        "# ‚úì More stable gradients\n",
        "# ‚úì Improved model performance\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚öñÔ∏è FEATURE SCALING: STANDARDIZATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìã WHY WE NEED SCALING:\")\n",
        "print(\"-\" * 80)\n",
        "print(\"\"\"\n",
        "Our features have vastly different scales. Examples:\n",
        "‚Ä¢ mean radius: 6.98 to 28.11 (range ‚âà 21)\n",
        "‚Ä¢ mean area: 143.5 to 2501.0 (range ‚âà 2357)\n",
        "‚Ä¢ mean smoothness: 0.053 to 0.163 (range ‚âà 0.11)\n",
        "‚Ä¢ mean fractal dimension: 0.050 to 0.097 (range ‚âà 0.047)\n",
        "\n",
        "Without scaling:\n",
        "‚ùå Distance-based algorithms (KNN, SVM) biased by scale\n",
        "‚ùå Gradient descent converges slowly or fails\n",
        "‚ùå Large-scale features dominate small-scale features\n",
        "‚ùå Model performance suffers significantly\n",
        "\n",
        "With scaling:\n",
        "‚úÖ All features contribute equally\n",
        "‚úÖ Faster training convergence\n",
        "‚úÖ Better model performance\n",
        "‚úÖ More stable and reliable results\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìä BEFORE SCALING - SAMPLE STATISTICS:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Show statistics before scaling\n",
        "sample_features = ['mean radius', 'mean area', 'mean smoothness', 'mean fractal dimension']\n",
        "print(\"\\nTraining set statistics (before scaling):\")\n",
        "print(X_train[sample_features].describe().loc[['mean', 'std', 'min', 'max']].round(3))\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üîß APPLYING STANDARDIZATION:\")\n",
        "print(\"-\" * 80)\n",
        "print(\"\"\"\n",
        "Method: StandardScaler (Z-score normalization)\n",
        "Formula: z = (x - Œº) / œÉ\n",
        "  where: Œº = mean, œÉ = standard deviation\n",
        "Result: Mean = 0, Standard Deviation = 1\n",
        "\n",
        "Process:\n",
        "1. Fit scaler on TRAINING data only (learn Œº and œÉ)\n",
        "2. Transform training data using learned parameters\n",
        "3. Transform test data using SAME parameters (no fitting on test!)\n",
        "\"\"\")\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "print(\"\\n‚è≥ Step 1: Fitting scaler on training data...\")\n",
        "# Fit on training data only (learns mean and std)\n",
        "scaler.fit(X_train)\n",
        "print(\"   ‚úÖ Scaler fitted (learned mean and std from training data)\")\n",
        "\n",
        "print(\"\\n‚è≥ Step 2: Transforming training data...\")\n",
        "# Transform training data\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "print(\"   ‚úÖ Training data transformed\")\n",
        "\n",
        "print(\"\\n‚è≥ Step 3: Transforming test data...\")\n",
        "# Transform test data using same parameters\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "print(\"   ‚úÖ Test data transformed (using training parameters)\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìä AFTER SCALING - VERIFICATION:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Convert back to DataFrame for easier viewing\n",
        "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
        "\n",
        "print(\"\\nTraining set statistics (after scaling):\")\n",
        "print(X_train_scaled_df[sample_features].describe().loc[['mean', 'std', 'min', 'max']].round(3))\n",
        "\n",
        "print(\"\\nTest set statistics (after scaling):\")\n",
        "print(X_test_scaled_df[sample_features].describe().loc[['mean', 'std', 'min', 'max']].round(3))\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üîç DETAILED COMPARISON - EXAMPLE FEATURE:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "example_feature = 'mean radius'\n",
        "print(f\"\\nFeature: {example_feature}\")\n",
        "print(f\"\\nBEFORE SCALING:\")\n",
        "print(f\"   Training set:\")\n",
        "print(f\"   ‚Ä¢ Mean: {X_train[example_feature].mean():.4f}\")\n",
        "print(f\"   ‚Ä¢ Std:  {X_train[example_feature].std():.4f}\")\n",
        "print(f\"   ‚Ä¢ Min:  {X_train[example_feature].min():.4f}\")\n",
        "print(f\"   ‚Ä¢ Max:  {X_train[example_feature].max():.4f}\")\n",
        "print(f\"   ‚Ä¢ Range: {X_train[example_feature].max() - X_train[example_feature].min():.4f}\")\n",
        "\n",
        "print(f\"\\nAFTER SCALING:\")\n",
        "print(f\"   Training set:\")\n",
        "print(f\"   ‚Ä¢ Mean: {X_train_scaled_df[example_feature].mean():.4f} (‚âà 0)\")\n",
        "print(f\"   ‚Ä¢ Std:  {X_train_scaled_df[example_feature].std():.4f} (‚âà 1)\")\n",
        "print(f\"   ‚Ä¢ Min:  {X_train_scaled_df[example_feature].min():.4f}\")\n",
        "print(f\"   ‚Ä¢ Max:  {X_train_scaled_df[example_feature].max():.4f}\")\n",
        "print(f\"   ‚Ä¢ Range: {X_train_scaled_df[example_feature].max() - X_train_scaled_df[example_feature].min():.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìä VISUALIZATION: BEFORE VS AFTER SCALING\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Visualize scaling effect\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "\n",
        "# Before scaling - Box plot\n",
        "axes[0, 0].boxplot([X_train[feat] for feat in sample_features], labels=sample_features)\n",
        "axes[0, 0].set_title('Before Scaling - Box Plots\\n(Note: Different scales make comparison difficult)',\n",
        "                     fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_ylabel('Original Values', fontsize=11)\n",
        "axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "axes[0, 0].grid(alpha=0.3)\n",
        "\n",
        "# After scaling - Box plot\n",
        "axes[0, 1].boxplot([X_train_scaled_df[feat] for feat in sample_features], labels=sample_features)\n",
        "axes[0, 1].set_title('After Scaling - Box Plots\\n(All features now on comparable scale)',\n",
        "                     fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_ylabel('Standardized Values (z-scores)', fontsize=11)\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "axes[0, 1].axhline(y=0, color='r', linestyle='--', alpha=0.5, label='Mean (0)')\n",
        "axes[0, 1].grid(alpha=0.3)\n",
        "axes[0, 1].legend()\n",
        "\n",
        "# Before scaling - Distribution\n",
        "for feat in sample_features[:2]:  # Show 2 features\n",
        "    axes[1, 0].hist(X_train[feat], bins=30, alpha=0.5, label=feat)\n",
        "axes[1, 0].set_title('Before Scaling - Feature Distributions\\n(Different scales, different ranges)',\n",
        "                     fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Original Values', fontsize=11)\n",
        "axes[1, 0].set_ylabel('Frequency', fontsize=11)\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(alpha=0.3)\n",
        "\n",
        "# After scaling - Distribution\n",
        "for feat in sample_features[:2]:  # Show same 2 features\n",
        "    axes[1, 1].hist(X_train_scaled_df[feat], bins=30, alpha=0.5, label=feat)\n",
        "axes[1, 1].set_title('After Scaling - Feature Distributions\\n(Centered at 0, comparable scales)',\n",
        "                     fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Standardized Values (z-scores)', fontsize=11)\n",
        "axes[1, 1].set_ylabel('Frequency', fontsize=11)\n",
        "axes[1, 1].axvline(x=0, color='r', linestyle='--', alpha=0.5, label='Mean')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ SCALING COMPLETE - KEY TAKEAWAYS:\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\"\"\n",
        "1. TRANSFORMATION APPLIED:\n",
        "   ‚Ä¢ All {X_train.shape[1]} features standardized\n",
        "   ‚Ä¢ Training set: {X_train_scaled.shape} ‚Üí {X_train_scaled.nbytes / 1024:.2f} KB\n",
        "   ‚Ä¢ Test set: {X_test_scaled.shape} ‚Üí {X_test_scaled.nbytes / 1024:.2f} KB\n",
        "\n",
        "2. VERIFICATION PASSED:\n",
        "   ‚úÖ Training set mean ‚âà 0 (actual: {X_train_scaled_df.mean().mean():.6f})\n",
        "   ‚úÖ Training set std ‚âà 1 (actual: {X_train_scaled_df.std().mean():.6f})\n",
        "   ‚úÖ Test set transformed with same parameters\n",
        "   ‚úÖ No data leakage (test data not used in fitting)\n",
        "\n",
        "3. FEATURE SCALE COMPARISON:\n",
        "   Before: Features ranged from 0.05 to 2501\n",
        "   After: Features range approximately from -3 to +3 standard deviations\n",
        "   Interpretation: Values represent # of std deviations from mean\n",
        "\n",
        "4. READY FOR MACHINE LEARNING:\n",
        "   ‚úÖ Features on comparable scales\n",
        "   ‚úÖ No single feature dominates due to scale\n",
        "   ‚úÖ Distance metrics meaningful\n",
        "   ‚úÖ Gradient descent will converge efficiently\n",
        "   ‚úÖ Model performance will be optimized\n",
        "\n",
        "5. ALGORITHMS THAT BENEFIT:\n",
        "   ‚Ä¢ Logistic Regression ‚≠ê\n",
        "   ‚Ä¢ K-Nearest Neighbors (KNN) ‚≠ê‚≠ê‚≠ê\n",
        "   ‚Ä¢ Support Vector Machine (SVM) ‚≠ê‚≠ê‚≠ê\n",
        "   ‚Ä¢ Neural Networks ‚≠ê‚≠ê‚≠ê\n",
        "   ‚Ä¢ Decision Trees (not needed but doesn't hurt)\n",
        "   ‚Ä¢ Random Forest (not needed but doesn't hurt)\n",
        "\n",
        "6. NEXT STEP:\n",
        "   Ready to train machine learning models! üöÄ\n",
        "   We now have properly preprocessed data for optimal model performance.\n",
        "\"\"\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 11: MODEL 1 - LOGISTIC REGRESSION\n",
        "# ============================================================================\n",
        "#\n",
        "# SECTION PURPOSE:\n",
        "# ----------------\n",
        "# Train and evaluate a Logistic Regression model for binary classification\n",
        "# of breast tumors. This is often the first model to try due to its\n",
        "# simplicity, interpretability, and effectiveness.\n",
        "#\n",
        "# WHAT IS LOGISTIC REGRESSION?\n",
        "# -----------------------------\n",
        "# Despite its name, Logistic Regression is a CLASSIFICATION algorithm:\n",
        "# ‚Ä¢ Uses logistic (sigmoid) function to predict probabilities\n",
        "# ‚Ä¢ Output: Probability between 0 and 1\n",
        "# ‚Ä¢ Decision: If P(benign) > 0.5 ‚Üí Predict Benign, else Malignant\n",
        "#\n",
        "# Mathematical Foundation:\n",
        "# P(y=1|X) = 1 / (1 + e^(-z))\n",
        "# where z = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ... + Œ≤‚Çôx‚Çô\n",
        "#\n",
        "# HOW IT WORKS:\n",
        "# -------------\n",
        "# 1. Computes weighted sum of features (linear combination)\n",
        "# 2. Applies sigmoid function to squash output to [0, 1]\n",
        "# 3. Interprets output as probability\n",
        "# 4. Uses threshold (default 0.5) to make binary decision\n",
        "#\n",
        "# ADVANTAGES:\n",
        "# -----------\n",
        "# ‚úÖ Simple and interpretable (linear decision boundary)\n",
        "# ‚úÖ Fast to train, efficient for large datasets\n",
        "# ‚úÖ Provides probability estimates (not just predictions)\n",
        "# ‚úÖ Coefficients show feature importance and direction\n",
        "# ‚úÖ Works well when classes are linearly separable\n",
        "# ‚úÖ Less prone to overfitting than complex models\n",
        "# ‚úÖ Baseline model for binary classification\n",
        "#\n",
        "# DISADVANTAGES:\n",
        "# --------------\n",
        "# ‚ùå Assumes linear relationship (can't model complex patterns)\n",
        "# ‚ùå Sensitive to outliers\n",
        "# ‚ùå Requires feature scaling (which we did!)\n",
        "# ‚ùå May underperform with non-linear decision boundaries\n",
        "# ‚ùå Assumes feature independence\n",
        "#\n",
        "# WHEN TO USE LOGISTIC REGRESSION:\n",
        "# ---------------------------------\n",
        "# ‚Ä¢ Binary classification problems ‚úì (our case)\n",
        "# ‚Ä¢ Need probability estimates ‚úì\n",
        "# ‚Ä¢ Want interpretable model ‚úì\n",
        "# ‚Ä¢ Have linearly separable classes ‚úì (somewhat)\n",
        "# ‚Ä¢ Need fast training/prediction ‚úì\n",
        "# ‚Ä¢ Want to understand feature contributions ‚úì\n",
        "#\n",
        "# HYPERPARAMETERS:\n",
        "# ----------------\n",
        "# ‚Ä¢ max_iter: Maximum iterations for convergence (10000 = generous)\n",
        "# ‚Ä¢ random_state: For reproducibility\n",
        "# ‚Ä¢ C: Regularization strength (default=1.0, smaller = more regularization)\n",
        "# ‚Ä¢ penalty: Type of regularization ('l1', 'l2', 'elasticnet', 'none')\n",
        "# ‚Ä¢ solver: Algorithm for optimization ('lbfgs', 'liblinear', 'sag', etc.)\n",
        "#\n",
        "# EVALUATION METRICS EXPLAINED:\n",
        "# ------------------------------\n",
        "#\n",
        "# 1. ACCURACY:\n",
        "#    Formula: (TP + TN) / Total\n",
        "#    Meaning: Overall correctness\n",
        "#    When good: Balanced datasets\n",
        "#    Limitation: Misleading for imbalanced data\n",
        "#\n",
        "# 2. PRECISION:\n",
        "#    Formula: TP / (TP + FP)\n",
        "#    Meaning: Of predicted positives, how many were actually positive?\n",
        "#    Medical context: Of predicted benign, how many were truly benign?\n",
        "#    High precision: Few false alarms\n",
        "#\n",
        "# 3. RECALL (Sensitivity):\n",
        "#    Formula: TP / (TP + FN)\n",
        "#    Meaning: Of actual positives, how many did we catch?\n",
        "#    Medical context: Of actual malignancies, how many did we detect?\n",
        "#    High recall: Few missed cancers (CRITICAL in medical diagnosis)\n",
        "#\n",
        "# 4. F1-SCORE:\n",
        "#    Formula: 2 √ó (Precision √ó Recall) / (Precision + Recall)\n",
        "#    Meaning: Harmonic mean of precision and recall\n",
        "#    When good: Need balance between precision and recall\n",
        "#    Range: 0 to 1 (higher is better)\n",
        "#\n",
        "# 5. ROC-AUC:\n",
        "#    Meaning: Area Under Receiver Operating Characteristic curve\n",
        "#    Range: 0.5 (random) to 1.0 (perfect)\n",
        "#    Interpretation:\n",
        "#      ‚Ä¢ 0.90-1.00: Excellent\n",
        "#      ‚Ä¢ 0.80-0.90: Good\n",
        "#      ‚Ä¢ 0.70-0.80: Fair\n",
        "#      ‚Ä¢ 0.60-0.70: Poor\n",
        "#      ‚Ä¢ 0.50-0.60: Fail\n",
        "#\n",
        "# MEDICAL CONTEXT - METRIC PRIORITIES:\n",
        "# -------------------------------------\n",
        "# In cancer diagnosis:\n",
        "# 1. RECALL (most important): Must catch cancer cases!\n",
        "#    ‚Ä¢ False Negative (missed cancer) = Life-threatening\n",
        "#    ‚Ä¢ Cost: Delayed treatment, disease progression\n",
        "#\n",
        "# 2. PRECISION (important): Avoid unnecessary stress\n",
        "#    ‚Ä¢ False Positive (false alarm) = Stress, unnecessary procedures\n",
        "#    ‚Ä¢ Cost: Emotional distress, additional biopsies\n",
        "#\n",
        "# 3. Balance: Need high recall without too many false positives\n",
        "#    ‚Ä¢ F1-Score helps find this balance\n",
        "#    ‚Ä¢ ROC-AUC shows overall discriminative ability\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ü§ñ MODEL 1: LOGISTIC REGRESSION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìã ABOUT LOGISTIC REGRESSION:\")\n",
        "print(\"-\" * 80)\n",
        "print(\"\"\"\n",
        "Type: Linear classifier for binary classification\n",
        "Method: Uses sigmoid function to predict probabilities\n",
        "Output: Probability P(Benign) ‚àà [0, 1]\n",
        "Decision: P > 0.5 ‚Üí Benign, P ‚â§ 0.5 ‚Üí Malignant\n",
        "\n",
        "Key Characteristics:\n",
        "‚úì Simple and interpretable (linear decision boundary)\n",
        "‚úì Fast training and prediction\n",
        "‚úì Provides probability estimates\n",
        "‚úì Good baseline model for comparison\n",
        "‚úì Works well with scaled features\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"‚è≥ TRAINING LOGISTIC REGRESSION MODEL...\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Initialize and train the model\n",
        "lr_model = LogisticRegression(\n",
        "    random_state=42,     # For reproducibility\n",
        "    max_iter=10000,      # Maximum iterations for convergence\n",
        "    solver='lbfgs'       # Optimization algorithm\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "lr_model.fit(X_train_scaled, y_train)\n",
        "print(\"‚úÖ Model trained successfully!\")\n",
        "\n",
        "print(f\"\\n   Model parameters:\")\n",
        "print(f\"   ‚Ä¢ Number of iterations: {lr_model.n_iter_[0]}\")\n",
        "print(f\"   ‚Ä¢ Convergence: {'Yes' if lr_model.n_iter_[0] < 10000 else 'No (max iterations reached)'}\")\n",
        "print(f\"   ‚Ä¢ Number of features used: {lr_model.coef_.shape[1]}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üîÆ MAKING PREDICTIONS...\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Make predictions on test set\n",
        "y_pred_lr = lr_model.predict(X_test_scaled)\n",
        "y_pred_proba_lr = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "print(f\"‚úÖ Predictions generated!\")\n",
        "print(f\"   ‚Ä¢ Test samples: {len(y_test)}\")\n",
        "print(f\"   ‚Ä¢ Predicted Benign: {(y_pred_lr == 1).sum()}\")\n",
        "print(f\"   ‚Ä¢ Predicted Malignant: {(y_pred_lr == 0).sum()}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìä MODEL PERFORMANCE METRICS:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Calculate metrics\n",
        "lr_accuracy = accuracy_score(y_test, y_pred_lr)\n",
        "lr_precision = precision_score(y_test, y_pred_lr)\n",
        "lr_recall = recall_score(y_test, y_pred_lr)\n",
        "lr_f1 = f1_score(y_test, y_pred_lr)\n",
        "lr_auc = roc_auc_score(y_test, y_pred_proba_lr)\n",
        "\n",
        "print(f\"\\n{'Metric':<20} {'Score':<10} {'Interpretation'}\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'Accuracy':<20} {lr_accuracy:<10.4f} Overall correctness\")\n",
        "print(f\"{'Precision':<20} {lr_precision:<10.4f} Accuracy of positive predictions\")\n",
        "print(f\"{'Recall (Sensitivity)':<20} {lr_recall:<10.4f} Ability to find all positives\")\n",
        "print(f\"{'F1-Score':<20} {lr_f1:<10.4f} Balance of precision & recall\")\n",
        "print(f\"{'ROC-AUC':<20} {lr_auc:<10.4f} Overall discriminative ability\")\n",
        "\n",
        "# Performance assessment\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üí° PERFORMANCE ASSESSMENT:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "if lr_accuracy > 0.95:\n",
        "    acc_rating = \"‚≠ê‚≠ê‚≠ê Excellent\"\n",
        "elif lr_accuracy > 0.90:\n",
        "    acc_rating = \"‚≠ê‚≠ê Very Good\"\n",
        "elif lr_accuracy > 0.85:\n",
        "    acc_rating = \"‚≠ê Good\"\n",
        "else:\n",
        "    acc_rating = \"‚ö†Ô∏è Needs Improvement\"\n",
        "\n",
        "if lr_recall > 0.95:\n",
        "    rec_rating = \"‚≠ê‚≠ê‚≠ê Excellent (very few missed cancers)\"\n",
        "elif lr_recall > 0.90:\n",
        "    rec_rating = \"‚≠ê‚≠ê Very Good (few missed cancers)\"\n",
        "elif lr_recall > 0.85:\n",
        "    rec_rating = \"‚≠ê Good (some missed cancers)\"\n",
        "else:\n",
        "    rec_rating = \"‚ö†Ô∏è Concerning (many missed cancers)\"\n",
        "\n",
        "print(f\"\\nAccuracy: {acc_rating}\")\n",
        "print(f\"Recall: {rec_rating}\")\n",
        "print(f\"F1-Score: {'‚≠ê‚≠ê‚≠ê Excellent balance' if lr_f1 > 0.95 else '‚≠ê‚≠ê Good balance' if lr_f1 > 0.90 else '‚≠ê Fair balance'}\")\n",
        "print(f\"ROC-AUC: {'‚≠ê‚≠ê‚≠ê Excellent discrimination' if lr_auc > 0.95 else '‚≠ê‚≠ê Good discrimination' if lr_auc > 0.90 else '‚≠ê Fair discrimination'}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üîç TOP PREDICTIVE FEATURES:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Feature importance (absolute coefficients)\n",
        "feature_importance_lr = pd.DataFrame({\n",
        "    'Feature': data.feature_names,\n",
        "    'Coefficient': lr_model.coef_[0],\n",
        "    'Abs_Coefficient': np.abs(lr_model.coef_[0])\n",
        "}).sort_values('Abs_Coefficient', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 most influential features:\")\n",
        "print(feature_importance_lr.head(10)[['Feature', 'Coefficient']].to_string(index=False))\n",
        "\n",
        "print(\"\\nüí° Coefficient interpretation:\")\n",
        "print(\"   ‚Ä¢ Positive coefficient ‚Üí Higher value ‚Üí More likely BENIGN\")\n",
        "print(\"   ‚Ä¢ Negative coefficient ‚Üí Higher value ‚Üí More likely MALIGNANT\")\n",
        "print(\"   ‚Ä¢ Larger |coefficient| ‚Üí Stronger influence on prediction\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ LOGISTIC REGRESSION SUMMARY:\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\"\"\n",
        "Model Performance:\n",
        "‚Ä¢ Accuracy: {lr_accuracy:.2%} - Correctly classified {int(lr_accuracy * len(y_test))}/{len(y_test)} test cases\n",
        "‚Ä¢ Recall: {lr_recall:.2%} - Detected {int(lr_recall * (y_test == 1).sum())}/{(y_test == 1).sum()} benign cases\n",
        "‚Ä¢ Precision: {lr_precision:.2%} - {int(lr_precision * (y_pred_lr == 1).sum())}/{(y_pred_lr == 1).sum()} predicted benign were correct\n",
        "\n",
        "Clinical Implications:\n",
        "‚Ä¢ False Negatives: {((y_test == 1) & (y_pred_lr == 0)).sum()} (missed benign cases - low risk)\n",
        "‚Ä¢ False Positives: {((y_test == 0) & (y_pred_lr == 1)).sum()} (false malignant alarms - moderate concern)\n",
        "‚Ä¢ True Positives: {((y_test == 1) & (y_pred_lr == 1)).sum()} (correct benign predictions)\n",
        "‚Ä¢ True Negatives: {((y_test == 0) & (y_pred_lr == 0)).sum()} (correct malignant predictions)\n",
        "\n",
        "Model Strengths:\n",
        "‚úì Simple and interpretable\n",
        "‚úì Fast predictions (suitable for real-time use)\n",
        "‚úì Provides probability estimates for risk assessment\n",
        "‚úì Strong baseline performance\n",
        "\n",
        "Next Steps:\n",
        "‚Üí Compare with other algorithms (KNN, Random Forest, SVM)\n",
        "‚Üí Look for improvements in recall and precision\n",
        "‚Üí Consider ensemble methods for better performance\n",
        "\"\"\")\n",
        "print(\"=\" * 80)(\"\\n\" + \"=\" * 80)\n",
        "print(\"üí° KEY INSIGHTS FROM CLASS DISTRIBUTION:\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\"\"\n",
        "1. Dataset Balance: The dataset has a {imbalance_ratio:.2f}:1 ratio of\n",
        "   Benign to Malignant cases.\n",
        "\n",
        "2. Clinical Relevance: This distribution is actually quite realistic for\n",
        "   breast cancer screening programs, where benign findings are more common\n",
        "   than malignant tumors.\n",
        "\n",
        "3. Modeling Implications:\n",
        "   ‚Ä¢ Our dataset is reasonably balanced ({diagnosis_percentages.values[1]:.1f}% minority class)\n",
        "   ‚Ä¢ We can use standard accuracy metrics, but should also monitor:\n",
        "     - Recall (sensitivity): To minimize false negatives\n",
        "     - Precision: To minimize false positives\n",
        "     - F1-Score: Balanced measure of both\n",
        "\n",
        "4. Medical Context: In cancer detection, missing a malignant case\n",
        "   (false negative) is typically considered worse than a false positive,\n",
        "   so we'll pay special attention to RECALL scores.\n",
        "\"\"\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 6: FEATURE ANALYSIS - UNDERSTANDING THE MEASUREMENTS\n",
        "# ============================================================================\n",
        "#\n",
        "# SECTION PURPOSE:\n",
        "# ----------------\n",
        "# This cell provides a comprehensive analysis of the 30 features in our dataset.\n",
        "# Each feature represents a different measurement of tumor characteristics\n",
        "# derived from cell nuclei in biopsy images.\n",
        "#\n",
        "# FEATURE ORGANIZATION:\n",
        "# ---------------------\n",
        "# The 30 features are organized into 3 groups of 10 measurements each:\n",
        "#\n",
        "# 1. MEAN VALUES (10 features):\n",
        "#    - Average of measurements across all cells in the image\n",
        "#    - Suffix: \"_mean\"\n",
        "#    - Example: \"mean radius\", \"mean texture\"\n",
        "#\n",
        "# 2. STANDARD ERROR (10 features):\n",
        "#    - Standard error of measurements (variability measure)\n",
        "#    - Suffix: \"_se\"\n",
        "#    - Example: \"radius error\", \"texture error\"\n",
        "#\n",
        "# 3. WORST VALUES (10 features):\n",
        "#    - Mean of the three largest values\n",
        "#    - Suffix: \"_worst\"\n",
        "#    - Example: \"worst radius\", \"worst texture\"\n",
        "#\n",
        "# THE 10 CORE MEASUREMENTS:\n",
        "# --------------------------\n",
        "# Each group contains these 10 measurements:\n",
        "#\n",
        "# 1. RADIUS:\n",
        "#    - Mean distance from center to points on perimeter\n",
        "#    - Larger radius ‚Üí Larger tumor\n",
        "#    - Medical significance: Size is a key diagnostic indicator\n",
        "#\n",
        "# 2. TEXTURE:\n",
        "#    - Standard deviation of gray-scale values\n",
        "#    - Higher texture ‚Üí More irregular cell appearance\n",
        "#    - Medical significance: Malignant cells often more irregular\n",
        "#\n",
        "# 3. PERIMETER:\n",
        "#    - Total boundary length of the tumor\n",
        "#    - Related to radius but captures shape complexity\n",
        "#    - Medical significance: Irregular perimeters suggest malignancy\n",
        "#\n",
        "# 4. AREA:\n",
        "#    - Total area enclosed by tumor perimeter\n",
        "#    - Directly related to tumor size\n",
        "#    - Medical significance: Larger tumors more concerning\n",
        "#\n",
        "# 5. SMOOTHNESS:\n",
        "#    - Local variation in radius lengths\n",
        "#    - Smoother ‚Üí More uniform, regular shape\n",
        "#    - Medical significance: Benign tumors typically smoother\n",
        "#\n",
        "# 6. COMPACTNESS:\n",
        "#    - Perimeter¬≤ / Area - 1.0\n",
        "#    - Measures how compact vs. spread out the tumor is\n",
        "#    - Medical significance: Malignant cells less compact\n",
        "#\n",
        "# 7. CONCAVITY:\n",
        "#    - Severity of concave portions of contour\n",
        "#    - Higher values ‚Üí More indentations in tumor boundary\n",
        "#    - Medical significance: Malignant tumors more irregular\n",
        "#\n",
        "# 8. CONCAVE POINTS:\n",
        "#    - Number of concave portions of contour\n",
        "#    - Counts distinct indentations\n",
        "#    - Medical significance: Strong indicator of malignancy\n",
        "#\n",
        "# 9. SYMMETRY:\n",
        "#    - Measures symmetry of the tumor\n",
        "#    - Higher values ‚Üí More asymmetric\n",
        "#    - Medical significance: Malignant tumors less symmetric\n",
        "#\n",
        "# 10. FRACTAL DIMENSION:\n",
        "#     - \"Coastline approximation\" - 1\n",
        "#     - Measures complexity of the boundary\n",
        "#     - Medical significance: Complex boundaries suggest malignancy\n",
        "#\n",
        "# WHY THREE VERSIONS OF EACH MEASUREMENT?\n",
        "# ----------------------------------------\n",
        "# ‚Ä¢ MEAN: Overall average characteristic\n",
        "# ‚Ä¢ ERROR: Variability within the sample (uncertainty measure)\n",
        "# ‚Ä¢ WORST: Most severe measurements (often most diagnostic)\n",
        "#\n",
        "# CLINICAL INTERPRETATION:\n",
        "# ------------------------\n",
        "# Medical professionals look for:\n",
        "# ‚úì Large radius/perimeter/area ‚Üí Concerning\n",
        "# ‚úì High texture/concavity ‚Üí Irregular cells ‚Üí Concerning\n",
        "# ‚úì Low smoothness/symmetry ‚Üí Irregular shape ‚Üí Concerning\n",
        "# ‚úì High \"worst\" values ‚Üí Most concerning areas ‚Üí Diagnostic\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üî¨ COMPREHENSIVE FEATURE ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Categorize features by type\n",
        "mean_features = [col for col in df.columns if 'mean' in col]\n",
        "se_features = [col for col in df.columns if 'error' in col or 'se' in col]\n",
        "worst_features = [col for col in df.columns if 'worst' in col]\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìã FEATURE ORGANIZATION:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"\\n1. MEAN FEATURES ({len(mean_features)} features):\")\n",
        "print(\"   These represent the average measurements across all cells:\")\n",
        "for i, feature in enumerate(mean_features, 1):\n",
        "    print(f\"   {i:2d}. {feature}\")\n",
        "\n",
        "print(f\"\\n2. STANDARD ERROR FEATURES ({len(se_features)} features):\")\n",
        "print(\"   These represent the variability/uncertainty in measurements:\")\n",
        "for i, feature in enumerate(se_features, 1):\n",
        "    print(f\"   {i:2d}. {feature}\")\n",
        "\n",
        "print(f\"\\n3. WORST FEATURES ({len(worst_features)} features):\")\n",
        "print(\"   These represent the most extreme measurements:\")\n",
        "for i, feature in enumerate(worst_features, 1):\n",
        "    print(f\"   {i:2d}. {feature}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìä FEATURE VALUE RANGES:\")\n",
        "print(\"-\" * 80)\n",
        "print(\"\\nUnderstanding the scale of measurements:\")\n",
        "print(df[mean_features].describe().loc[['min', 'max']].round(2))\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üí° KEY OBSERVATIONS:\")\n",
        "print(\"-\" * 80)\n",
        "print(\"\"\"\n",
        "1. SCALE DIFFERENCES:\n",
        "   ‚Ä¢ Features have vastly different scales\n",
        "   ‚Ä¢ Example: 'mean area' ranges from ~143 to ~2501\n",
        "   ‚Ä¢ Example: 'mean smoothness' ranges from ~0.05 to ~0.16\n",
        "   ‚Ä¢ Implication: MUST scale features before ML modeling\n",
        "\n",
        "2. PHYSICAL MEANING:\n",
        "   ‚Ä¢ Size features (radius, perimeter, area) are correlated\n",
        "   ‚Ä¢ Shape features (smoothness, symmetry) describe regularity\n",
        "   ‚Ä¢ Texture features describe cell appearance variability\n",
        "\n",
        "3. DIAGNOSTIC RELEVANCE:\n",
        "   ‚Ä¢ \"Worst\" features often most important for diagnosis\n",
        "   ‚Ä¢ They capture the most abnormal regions of the tumor\n",
        "   ‚Ä¢ Medical professionals focus on worst-case characteristics\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìä GENERATING FEATURE DISTRIBUTION VISUALIZATIONS...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Visualization of mean features distribution by diagnosis\n",
        "fig, axes = plt.subplots(2, 5, figsize=(22, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, feature in enumerate(mean_features):\n",
        "    # Separate data by diagnosis\n",
        "    malignant_data = df[df['diagnosis'] == 0][feature]\n",
        "    benign_data = df[df['diagnosis'] == 1][feature]\n",
        "\n",
        "    # Create overlapping histograms\n",
        "    axes[idx].hist(malignant_data, bins=25, alpha=0.6, label='Malignant',\n",
        "                   color='#FF6B6B', edgecolor='black', linewidth=0.5)\n",
        "    axes[idx].hist(benign_data, bins=25, alpha=0.6, label='Benign',\n",
        "                   color='#4ECDC4', edgecolor='black', linewidth=0.5)\n",
        "\n",
        "    # Styling\n",
        "    axes[idx].set_title(feature.replace('mean ', '').title(),\n",
        "                        fontsize=11, fontweight='bold')\n",
        "    axes[idx].set_xlabel('Value', fontsize=9)\n",
        "    axes[idx].set_ylabel('Frequency', fontsize=9)\n",
        "    axes[idx].legend(fontsize=8, loc='upper right')\n",
        "    axes[idx].grid(alpha=0.3, linestyle='--')\n",
        "\n",
        "    # Add statistical annotations\n",
        "    mal_mean = malignant_data.mean()\n",
        "    ben_mean = benign_data.mean()\n",
        "    axes[idx].axvline(mal_mean, color='#FF6B6B', linestyle='--',\n",
        "                      linewidth=2, alpha=0.7, label=f'M Œº={mal_mean:.1f}')\n",
        "    axes[idx].axvline(ben_mean, color='#4ECDC4', linestyle='--',\n",
        "                      linewidth=2, alpha=0.7, label=f'B Œº={ben_mean:.1f}')\n",
        "\n",
        "plt.suptitle('Distribution of Mean Features by Diagnosis\\n' +\n",
        "             'Red = Malignant | Teal = Benign | Dashed lines = Mean values',\n",
        "             fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print"
      ],
      "metadata": {
        "id": "CiJegDow4tle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u3JLOoJs4vOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# ü©∫ BREAST CANCER PREDICTION: MALIGNANT VS BENIGN CLASSIFICATION üß¨\n",
        "# ============================================================================\n",
        "#\n",
        "# PROJECT OVERVIEW:\n",
        "# -----------------\n",
        "# This comprehensive notebook demonstrates a complete machine learning workflow\n",
        "# for predicting whether a breast tumor is malignant (cancerous) or benign\n",
        "# (non-cancerous) based on physical characteristics extracted from biopsy images.\n",
        "#\n",
        "# DATASET: Breast Cancer Wisconsin (Diagnostic) Dataset\n",
        "# - Source: UCI Machine Learning Repository\n",
        "# - Samples: 569 breast tumor cases\n",
        "# - Features: 30 numeric measurements of tumor characteristics\n",
        "# - Target: Binary classification (Malignant vs Benign)\n",
        "#\n",
        "# WORKFLOW STAGES:\n",
        "# 1. Data Loading & Exploration\n",
        "# 2. Exploratory Data Analysis (EDA)\n",
        "# 3. Data Preprocessing & Feature Engineering\n",
        "# 4. Multiple ML Model Training\n",
        "# 5. Model Evaluation & Comparison\n",
        "# 6. Final Recommendations\n",
        "#\n",
        "# CLINICAL SIGNIFICANCE:\n",
        "# Early and accurate detection of breast cancer is crucial for successful\n",
        "# treatment. This machine learning approach can assist medical professionals\n",
        "# in making more informed diagnostic decisions.\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 1: IMPORT NECESSARY LIBRARIES\n",
        "# ============================================================================\n",
        "#\n",
        "# SECTION PURPOSE:\n",
        "# ----------------\n",
        "# In this cell, we import all the essential Python libraries needed for our\n",
        "# complete machine learning pipeline. Each library serves a specific purpose\n",
        "# in our analysis workflow.\n",
        "#\n",
        "# LIBRARIES BREAKDOWN:\n",
        "# --------------------\n",
        "#\n",
        "# üìä DATA MANIPULATION:\n",
        "# - numpy: Numerical computing, array operations, mathematical functions\n",
        "# - pandas: Data manipulation, DataFrame operations, data analysis\n",
        "#\n",
        "# üìà DATA VISUALIZATION:\n",
        "# - matplotlib.pyplot: Creating static, animated, and interactive visualizations\n",
        "# - seaborn: Statistical data visualization built on matplotlib, prettier plots\n",
        "#\n",
        "# ü§ñ MACHINE LEARNING - CORE:\n",
        "# - sklearn.datasets: Access to built-in datasets including breast cancer data\n",
        "# - sklearn.model_selection: Tools for splitting data and cross-validation\n",
        "# - sklearn.preprocessing: Data preprocessing tools like scaling and encoding\n",
        "#\n",
        "# üéØ MACHINE LEARNING - ALGORITHMS:\n",
        "# - LogisticRegression: Linear model for binary classification\n",
        "# - DecisionTreeClassifier: Tree-based model with interpretable rules\n",
        "# - RandomForestClassifier: Ensemble of decision trees for robust predictions\n",
        "# - SVC: Support Vector Classifier for complex decision boundaries\n",
        "# - KNeighborsClassifier: Instance-based learning using nearest neighbors\n",
        "#\n",
        "# üìè MACHINE LEARNING - EVALUATION:\n",
        "# - Various metrics: accuracy, precision, recall, F1-score for model assessment\n",
        "# - confusion_matrix: Visual representation of classification results\n",
        "# - ROC curves & AUC: Evaluate model performance across different thresholds\n",
        "#\n",
        "# WHY EACH METRIC MATTERS:\n",
        "# - Accuracy: Overall correctness of predictions\n",
        "# - Precision: Of all positive predictions, how many were actually positive?\n",
        "# - Recall: Of all actual positives, how many did we correctly identify?\n",
        "# - F1-Score: Harmonic mean of precision and recall (balanced metric)\n",
        "# - ROC-AUC: Model's ability to distinguish between classes\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, confusion_matrix, classification_report,\n",
        "                             roc_curve, roc_auc_score)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set visualization style for better-looking plots\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"‚úÖ ALL LIBRARIES IMPORTED SUCCESSFULLY!\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nüì¶ Libraries loaded:\")\n",
        "print(\"   ‚úì NumPy - Numerical computing\")\n",
        "print(\"   ‚úì Pandas - Data manipulation\")\n",
        "print(\"   ‚úì Matplotlib & Seaborn - Data visualization\")\n",
        "print(\"   ‚úì Scikit-learn - Machine learning algorithms and tools\")\n",
        "print(\"\\nüöÄ Ready to begin breast cancer prediction analysis!\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 2: LOAD AND PREPARE THE BREAST CANCER DATASET\n",
        "# ============================================================================\n",
        "#\n",
        "# SECTION PURPOSE:\n",
        "# ----------------\n",
        "# This cell loads the Breast Cancer Wisconsin dataset from scikit-learn's\n",
        "# built-in datasets and prepares it for analysis by converting it into a\n",
        "# pandas DataFrame with proper column names and labels.\n",
        "#\n",
        "# ABOUT THE DATASET:\n",
        "# ------------------\n",
        "# The Breast Cancer Wisconsin (Diagnostic) Dataset contains features computed\n",
        "# from digitized images of fine needle aspirate (FNA) of breast masses. These\n",
        "# features describe characteristics of cell nuclei present in the images.\n",
        "#\n",
        "# DATA COLLECTION METHOD:\n",
        "# 1. A fine needle aspirate (FNA) is taken from a breast mass\n",
        "# 2. The sample is digitized and processed\n",
        "# 3. Computer vision algorithms extract features from cell nuclei\n",
        "# 4. Medical experts provide the diagnosis (malignant or benign)\n",
        "#\n",
        "# DATASET STRUCTURE:\n",
        "# - 569 instances (patient cases)\n",
        "# - 30 real-valued features (measurements)\n",
        "# - 2 classes: Malignant (0) and Benign (1)\n",
        "# - No missing values (complete dataset)\n",
        "#\n",
        "# TARGET VARIABLE ENCODING:\n",
        "# - 0 = Malignant (M) ‚Üí Cancer is present, requires treatment\n",
        "# - 1 = Benign (B) ‚Üí No cancer, but monitoring may be needed\n",
        "#\n",
        "# WHY THIS MATTERS:\n",
        "# In medical diagnosis, correctly identifying malignant tumors (high recall)\n",
        "# is critical to ensure patients receive timely treatment. However, we also\n",
        "# want to avoid false positives (high precision) to prevent unnecessary\n",
        "# stress and medical procedures.\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìä LOADING BREAST CANCER WISCONSIN DATASET\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Load the dataset from scikit-learn's built-in datasets\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Create a pandas DataFrame for easier manipulation and analysis\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "\n",
        "# Add the target variable (diagnosis) to our DataFrame\n",
        "df['diagnosis'] = data.target\n",
        "\n",
        "# Create human-readable labels for better interpretation\n",
        "# Map 0 ‚Üí 'Malignant' (cancerous), 1 ‚Üí 'Benign' (non-cancerous)\n",
        "df['diagnosis_label'] = df['diagnosis'].map({0: 'Malignant', 1: 'Benign'})\n",
        "\n",
        "print(\"\\n‚úÖ Dataset loaded successfully!\")\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìã DATASET OVERVIEW:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"   ‚Ä¢ Total number of samples: {len(df)}\")\n",
        "print(f\"   ‚Ä¢ Number of features: {len(data.feature_names)}\")\n",
        "print(f\"   ‚Ä¢ Dataset dimensions: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
        "print(f\"   ‚Ä¢ Number of malignant cases: {(df['diagnosis'] == 0).sum()}\")\n",
        "print(f\"   ‚Ä¢ Number of benign cases: {(df['diagnosis'] == 1).sum()}\")\n",
        "print(f\"   ‚Ä¢ Class balance ratio: {(df['diagnosis'] == 1).sum() / len(df) * 100:.1f}% benign\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìù DATASET DESCRIPTION:\")\n",
        "print(\"-\" * 80)\n",
        "print(data.DESCR[:500] + \"...\")  # Print first 500 characters of description\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 3: INITIAL DATA EXPLORATION - VIEWING THE DATA\n",
        "# ============================================================================\n",
        "#\n",
        "# SECTION PURPOSE:\n",
        "# ----------------\n",
        "# This cell performs initial exploration of our dataset to understand its\n",
        "# structure, data types, and basic characteristics. This is a critical first\n",
        "# step in any data science project.\n",
        "#\n",
        "# WHY DATA EXPLORATION MATTERS:\n",
        "# -----------------------------\n",
        "# Before building any machine learning model, we need to:\n",
        "# 1. Understand what data we're working with\n",
        "# 2. Identify data types and potential issues\n",
        "# 3. Get familiar with feature names and values\n",
        "# 4. Check for any obvious problems or anomalies\n",
        "#\n",
        "# WHAT WE'RE EXAMINING:\n",
        "# ---------------------\n",
        "# ‚Ä¢ First few rows: Get a feel for the actual data values\n",
        "# ‚Ä¢ Data types: Ensure all features are numeric (required for ML)\n",
        "# ‚Ä¢ Memory usage: Understand dataset size\n",
        "# ‚Ä¢ Feature names: Familiarize ourselves with what we're measuring\n",
        "#\n",
        "# THINGS TO LOOK FOR:\n",
        "# -------------------\n",
        "# ‚úì Are all features numeric? (Yes, required for our ML models)\n",
        "# ‚úì Do values seem reasonable? (No obvious errors)\n",
        "# ‚úì Are there any unexpected patterns?\n",
        "# ‚úì Do feature names make clinical sense?\n",
        "#\n",
        "# UNDERSTANDING THE OUTPUT:\n",
        "# -------------------------\n",
        "# - .head(): Shows first 5 rows of data\n",
        "# - .info(): Provides data types, non-null counts, memory usage\n",
        "# - .describe(): Statistical summary (mean, std, min, max, quartiles)\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üîç INITIAL DATA EXPLORATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìã FIRST 5 ROWS OF THE DATASET\")\n",
        "print(\"-\" * 80)\n",
        "print(\"\\nThis gives us a glimpse of the actual data values:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìä DATASET INFORMATION (DATA TYPES & STRUCTURE)\")\n",
        "print(\"-\" * 80)\n",
        "print(\"\\nDetailed information about each column:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìà STATISTICAL SUMMARY OF ALL FEATURES\")\n",
        "print(\"-\" * 80)\n",
        "print(\"\\nDescriptive statistics for each numeric feature:\")\n",
        "print(df.describe().round(2))\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìù INTERPRETATION GUIDE:\")\n",
        "print(\"-\" * 80)\n",
        "print(\"\"\"\n",
        "‚Ä¢ count: Number of non-null values (should be 569 for all)\n",
        "‚Ä¢ mean: Average value - center of the distribution\n",
        "‚Ä¢ std: Standard deviation - measure of spread/variability\n",
        "‚Ä¢ min: Minimum value observed\n",
        "‚Ä¢ 25%: First quartile (25% of data is below this value)\n",
        "‚Ä¢ 50%: Median (middle value when sorted)\n",
        "‚Ä¢ 75%: Third quartile (75% of data is below this value)\n",
        "‚Ä¢ max: Maximum value observed\n",
        "\n",
        "KEY OBSERVATIONS:\n",
        "‚úì All features have 569 non-null values ‚Üí No missing data!\n",
        "‚úì All features are numeric (float64) ‚Üí Ready for machine learning\n",
        "‚úì Different features have vastly different scales ‚Üí Will need scaling\n",
        "‚úì Some features show high variability (large std) ‚Üí Normal for medical data\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 4: DATA QUALITY CHECK - MISSING VALUES ANALYSIS\n",
        "# ============================================================================\n",
        "#\n",
        "# SECTION PURPOSE:\n",
        "# ----------------\n",
        "# This cell thoroughly checks for missing values in our dataset. Missing data\n",
        "# is one of the most common problems in real-world datasets and can significantly\n",
        "# impact model performance if not handled properly.\n",
        "#\n",
        "# WHY MISSING VALUES MATTER:\n",
        "# --------------------------\n",
        "# Missing values can occur due to:\n",
        "# ‚Ä¢ Data collection errors\n",
        "# ‚Ä¢ Equipment malfunction\n",
        "# ‚Ä¢ Human error during data entry\n",
        "# ‚Ä¢ Privacy concerns (data intentionally omitted)\n",
        "# ‚Ä¢ Technical issues during data transfer\n",
        "#\n",
        "# IMPACT ON MACHINE LEARNING:\n",
        "# ----------------------------\n",
        "# Most ML algorithms cannot handle missing values and will either:\n",
        "# 1. Throw an error and refuse to run\n",
        "# 2. Produce incorrect results\n",
        "# 3. Automatically drop rows/columns with missing values\n",
        "#\n",
        "# COMMON STRATEGIES FOR HANDLING MISSING DATA:\n",
        "# ---------------------------------------------\n",
        "# If we find missing values, we can:\n",
        "# 1. DELETE: Remove rows or columns with missing values\n",
        "#    - Use when: Very few missing values (<5% of data)\n",
        "#    - Pros: Simple, no assumptions made\n",
        "#    - Cons: Lose potentially valuable data\n",
        "#\n",
        "# 2. IMPUTE - MEAN/MEDIAN/MODE:\n",
        "#    - Use when: Data is missing at random\n",
        "#    - Pros: Retains all samples\n",
        "#    - Cons: Can distort distributions\n",
        "#\n",
        "# 3. IMPUTE - ADVANCED METHODS:\n",
        "#    - Use algorithms like KNN or regression to predict missing values\n",
        "#    - Pros: More accurate than simple imputation\n",
        "#    - Cons: More complex, computationally expensive\n",
        "#\n",
        "# 4. CREATE INDICATOR VARIABLES:\n",
        "#    - Add binary column indicating if value was missing\n",
        "#    - Use when: Missingness itself is informative\n",
        "#\n",
        "# GOOD NEWS FOR OUR DATASET:\n",
        "# ---------------------------\n",
        "# The Wisconsin Breast Cancer dataset is a well-curated research dataset\n",
        "# with NO missing values! This is rare in real-world scenarios but makes\n",
        "# our analysis cleaner and more straightforward.\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üîç DATA QUALITY CHECK: MISSING VALUES ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Check for missing values in each column\n",
        "missing_values = df.isnull().sum()\n",
        "total_cells = np.product(df.shape)\n",
        "total_missing = missing_values.sum()\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìä MISSING VALUES SUMMARY:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"\\n   ‚Ä¢ Total cells in dataset: {total_cells:,}\")\n",
        "print(f\"   ‚Ä¢ Total missing values: {total_missing}\")\n",
        "print(f\"   ‚Ä¢ Percentage of missing data: {(total_missing / total_cells) * 100:.2f}%\")\n",
        "\n",
        "if total_missing == 0:\n",
        "    print(\"\\n   ‚úÖ EXCELLENT! No missing values found in any column!\")\n",
        "    print(\"   ‚úÖ Dataset is complete and ready for machine learning!\")\n",
        "    print(\"\\n   This is a high-quality dataset - no imputation needed!\")\n",
        "else:\n",
        "    print(\"\\n   ‚ö†Ô∏è WARNING: Missing values detected!\")\n",
        "    print(\"\\n   Columns with missing values:\")\n",
        "    print(\"-\" * 80)\n",
        "    missing_df = pd.DataFrame({\n",
        "        'Column': missing_values[missing_values > 0].index,\n",
        "        'Missing Count': missing_values[missing_values > 0].values,\n",
        "        'Percentage': (missing_values[missing_values > 0].values / len(df) * 100).round(2)\n",
        "    })\n",
        "    print(missing_df.to_string(index=False))\n",
        "\n",
        "    print(\"\\n   üìù RECOMMENDED ACTIONS:\")\n",
        "    print(\"   \" + \"-\" * 76)\n",
        "    for col, missing_pct in zip(missing_df['Column'], missing_df['Percentage']):\n",
        "        if missing_pct < 5:\n",
        "            print(f\"   ‚Ä¢ {col}: {missing_pct}% missing ‚Üí Consider removing rows\")\n",
        "        elif missing_pct < 30:\n",
        "            print(f\"   ‚Ä¢ {col}: {missing_pct}% missing ‚Üí Consider imputation\")\n",
        "        else:\n",
        "            print(f\"   ‚Ä¢ {col}: {missing_pct}% missing ‚Üí Consider removing column\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üí° DATA QUALITY ASSESSMENT:\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\"\"\n",
        "‚úì All 569 samples are complete\n",
        "‚úì All 30 features have valid measurements\n",
        "‚úì No data cleaning required for missing values\n",
        "‚úì Ready to proceed with exploratory data analysis\n",
        "\n",
        "This clean dataset allows us to focus on feature engineering and\n",
        "model building without worrying about data imputation strategies!\n",
        "\"\"\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 5: TARGET VARIABLE DISTRIBUTION ANALYSIS\n",
        "# ============================================================================\n",
        "#\n",
        "# SECTION PURPOSE:\n",
        "# ----------------\n",
        "# This cell analyzes the distribution of our target variable (diagnosis).\n",
        "# Understanding class distribution is crucial because it affects:\n",
        "# 1. Model training and performance\n",
        "# 2. Evaluation metric selection\n",
        "# 3. Potential need for balancing techniques\n",
        "#\n",
        "# WHY CLASS DISTRIBUTION MATTERS:\n",
        "# -------------------------------\n",
        "# BALANCED DATASET (50:50 ratio):\n",
        "# ‚Ä¢ Models learn both classes equally well\n",
        "# ‚Ä¢ Standard metrics (accuracy) work well\n",
        "# ‚Ä¢ No special techniques needed\n",
        "#\n",
        "# IMBALANCED DATASET (e.g., 90:10 ratio):\n",
        "# ‚Ä¢ Models may become biased toward majority class\n",
        "# ‚Ä¢ Accuracy can be misleading (e.g., 90% by always predicting majority)\n",
        "# ‚Ä¢ May need: oversampling, undersampling, SMOTE, or class weights\n",
        "# ‚Ä¢ Should focus on: precision, recall, F1-score, not just accuracy\n",
        "#\n",
        "# CLASS IMBALANCE IN MEDICAL DIAGNOSIS:\n",
        "# --------------------------------------\n",
        "# In medical datasets, imbalance is common because:\n",
        "# ‚Ä¢ Diseases are often rare in the general population\n",
        "# ‚Ä¢ More benign cases than malignant in screening programs\n",
        "# ‚Ä¢ Cost of false negatives (missing cancer) is very high\n",
        "#\n",
        "# WHAT TO LOOK FOR:\n",
        "# -----------------\n",
        "# ‚Ä¢ Ratio between classes (is one significantly larger?)\n",
        "# ‚Ä¢ Absolute numbers (do we have enough samples of minority class?)\n",
        "# ‚Ä¢ Consider if imbalance reflects real-world prevalence\n",
        "#\n",
        "# EVALUATION METRIC IMPLICATIONS:\n",
        "# --------------------------------\n",
        "# ‚Ä¢ Balanced dataset ‚Üí Accuracy is fine\n",
        "# ‚Ä¢ Imbalanced dataset ‚Üí Focus on precision/recall/F1-score\n",
        "# ‚Ä¢ Medical context ‚Üí Prioritize RECALL (don't miss cancer cases)\n",
        "#\n",
        "# VISUALIZATIONS INCLUDED:\n",
        "# ------------------------\n",
        "# 1. Count plot: Shows absolute numbers of each class\n",
        "# 2. Pie chart: Shows proportional distribution\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üéØ TARGET VARIABLE DISTRIBUTION ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Calculate class distribution\n",
        "diagnosis_counts = df['diagnosis_label'].value_counts()\n",
        "diagnosis_percentages = df['diagnosis_label'].value_counts(normalize=True) * 100\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìä CLASS DISTRIBUTION (ABSOLUTE COUNTS):\")\n",
        "print(\"-\" * 80)\n",
        "for label, count in diagnosis_counts.items():\n",
        "    print(f\"   ‚Ä¢ {label:12s}: {count:3d} cases\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìä CLASS DISTRIBUTION (PERCENTAGES):\")\n",
        "print(\"-\" * 80)\n",
        "for label, pct in diagnosis_percentages.items():\n",
        "    print(f\"   ‚Ä¢ {label:12s}: {pct:5.2f}%\")\n",
        "\n",
        "# Calculate imbalance ratio\n",
        "majority_count = diagnosis_counts.values[0]\n",
        "minority_count = diagnosis_counts.values[1]\n",
        "imbalance_ratio = majority_count / minority_count\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"‚öñÔ∏è CLASS BALANCE ANALYSIS:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"   ‚Ä¢ Majority class: {diagnosis_counts.index[0]} ({majority_count} samples)\")\n",
        "print(f\"   ‚Ä¢ Minority class: {diagnosis_counts.index[1]} ({minority_count} samples)\")\n",
        "print(f\"   ‚Ä¢ Imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
        "\n",
        "if imbalance_ratio < 1.5:\n",
        "    balance_status = \"‚úÖ WELL BALANCED\"\n",
        "    recommendation = \"Standard ML algorithms will work well without modifications.\"\n",
        "elif imbalance_ratio < 3:\n",
        "    balance_status = \"‚ö†Ô∏è SLIGHT IMBALANCE\"\n",
        "    recommendation = \"Consider monitoring precision and recall separately. May use class weights.\"\n",
        "else:\n",
        "    balance_status = \"‚ùå SIGNIFICANT IMBALANCE\"\n",
        "    recommendation = \"Should use: class weights, SMOTE, or focus on F1-score/AUC metrics.\"\n",
        "\n",
        "print(f\"\\n   Status: {balance_status}\")\n",
        "print(f\"   Recommendation: {recommendation}\")\n",
        "\n",
        "# Visualization\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìä GENERATING VISUALIZATIONS...\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Count plot with detailed annotations\n",
        "bars = axes[0].bar(range(len(diagnosis_counts)), diagnosis_counts.values,\n",
        "                   color=['#FF6B6B', '#4ECDC4'], alpha=0.8, edgecolor='black', linewidth=2)\n",
        "axes[0].set_xticks(range(len(diagnosis_counts)))\n",
        "axes[0].set_xticklabels(diagnosis_counts.index, fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('Number of Cases', fontsize=13, fontweight='bold')\n",
        "axes[0].set_title('Distribution of Breast Cancer Diagnosis\\n(Absolute Counts)',\n",
        "                  fontsize=14, fontweight='bold', pad=20)\n",
        "axes[0].grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, (bar, count, pct) in enumerate(zip(bars, diagnosis_counts.values, diagnosis_percentages.values)):\n",
        "    height = bar.get_height()\n",
        "    axes[0].text(bar.get_x() + bar.get_width()/2., height + 5,\n",
        "                f'{count}\\n({pct:.1f}%)',\n",
        "                ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Pie chart with enhanced styling\n",
        "colors = ['#FF6B6B', '#4ECDC4']\n",
        "explode = (0.05, 0.05)  # Slightly separate both slices\n",
        "wedges, texts, autotexts = axes[1].pie(diagnosis_counts.values,\n",
        "                                        labels=diagnosis_counts.index,\n",
        "                                        autopct='%1.1f%%',\n",
        "                                        colors=colors,\n",
        "                                        explode=explode,\n",
        "                                        startangle=90,\n",
        "                                        textprops={'fontsize': 12, 'fontweight': 'bold'},\n",
        "                                        shadow=True)\n",
        "axes[1].set_title('Proportion of Malignant vs Benign Cases\\n(Percentage Distribution)',\n",
        "                  fontsize=14, fontweight='bold', pad=20)\n",
        "\n",
        "# Enhance autotext\n",
        "for autotext in autotexts:\n",
        "    autotext.set_color('white')\n",
        "    autotext.set_fontsize(13)\n",
        "    autotext.set_weight('bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üí° KEY INSIGHTS FROM CLASS DISTRIBUTION:\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\"\"\n",
        "1. Dataset Balance: The dataset has a {imbalance_ratio:.2f}:1 ratio of\n",
        "   Benign to Malignant cases.\n",
        "\n",
        "2. Clinical Relevance: This distribution is actually quite realistic for\n",
        "   breast cancer screening programs, where benign findings are more common\n",
        "   than malignant tumors.\n",
        "\n",
        "3. Modeling Implications:\n",
        "   ‚Ä¢ Our dataset is reasonably balanced ({diagnosis_percentages.values[1]:.1f}% minority class)\n",
        "   ‚Ä¢ We can use standard accuracy metrics, but should also monitor:\n",
        "     - Recall (sensitivity): To minimize false negatives\n",
        "     - Precision: To minimize false positives\n",
        "     - F1-Score: Balanced measure of both\n",
        "\n",
        "4. Medical Context: In cancer detection, missing a malignant case\n",
        "   (false negative) is typically considered worse than a false positive,\n",
        "   so we'll pay special attention to RECALL scores.\n",
        "\"\"\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 6: FEATURE ANALYSIS - UNDERSTANDING THE MEASUREMENTS\n",
        "# ============================================================================\n",
        "#\n",
        "# SECTION PURPOSE:\n",
        "# ----------------\n",
        "# This cell provides a comprehensive analysis of the 30 features in our dataset.\n",
        "# Each feature represents a different measurement of tumor characteristics\n",
        "# derived from cell nuclei in biopsy images.\n",
        "#\n",
        "# FEATURE ORGANIZATION:\n",
        "# ---------------------\n",
        "# The 30 features are organized into 3 groups of 10 measurements each:\n",
        "#\n",
        "# 1. MEAN VALUES (10 features):\n",
        "#    - Average of measurements across all cells in the image\n",
        "#    - Suffix: \"_mean\"\n",
        "#    - Example: \"mean radius\", \"mean texture\"\n",
        "#\n",
        "# 2. STANDARD ERROR (10 features):\n",
        "#    - Standard error of measurements (variability measure)\n",
        "#    - Suffix: \"_se\"\n",
        "#    - Example: \"radius error\", \"texture error\"\n",
        "#\n",
        "# 3. WORST VALUES (10 features):\n",
        "#    - Mean of the three largest values\n",
        "#    - Suffix: \"_worst\"\n",
        "#    - Example: \"worst radius\", \"worst texture\"\n",
        "#\n",
        "# THE 10 CORE MEASUREMENTS:\n",
        "# --------------------------\n",
        "# Each group contains these 10 measurements:\n",
        "#\n",
        "# 1. RADIUS:\n",
        "#    - Mean distance from center to points on perimeter\n",
        "#    - Larger radius ‚Üí Larger tumor\n",
        "#    - Medical significance: Size is a key diagnostic indicator\n",
        "#\n",
        "# 2. TEXTURE:\n",
        "#    - Standard deviation of gray-scale values\n",
        "#    - Higher texture ‚Üí More irregular cell appearance\n",
        "#    - Medical significance: Malignant cells often more irregular\n",
        "#\n",
        "# 3. PERIMETER:\n",
        "#    - Total boundary length of the tumor\n",
        "#    - Related to radius but captures shape complexity\n",
        "#    - Medical significance: Irregular perimeters suggest malignancy\n",
        "#\n",
        "# 4. AREA:\n",
        "#    - Total area enclosed by tumor perimeter\n",
        "#    - Directly related to tumor size\n",
        "#    - Medical significance: Larger tumors more concerning\n",
        "#\n",
        "# 5. SMOOTHNESS:\n",
        "#    - Local variation in radius lengths\n",
        "#    - Smoother ‚Üí More uniform, regular shape\n",
        "#    - Medical significance: Benign tumors typically smoother\n",
        "#\n",
        "# 6. COMPACTNESS:\n",
        "#    - Perimeter¬≤ / Area - 1.0\n",
        "#    - Measures how compact vs. spread out the tumor is\n",
        "#    - Medical significance: Malignant cells less compact\n",
        "#\n",
        "# 7. CONCAVITY:\n",
        "#    - Severity of concave portions of contour\n",
        "#    - Higher values ‚Üí More indentations in tumor boundary\n",
        "#    - Medical significance: Malignant tumors more irregular\n",
        "#\n",
        "# 8. CONCAVE POINTS:\n",
        "#    - Number of concave portions of contour\n",
        "#    - Counts distinct indentations\n",
        "#    - Medical significance: Strong indicator of malignancy\n",
        "#\n",
        "# 9. SYMMETRY:\n",
        "#    - Measures symmetry of the tumor\n",
        "#    - Higher values ‚Üí More asymmetric\n",
        "#    - Medical significance: Malignant tumors less symmetric\n",
        "#\n",
        "# 10. FRACTAL DIMENSION:\n",
        "#     - \"Coastline approximation\" - 1\n",
        "#     - Measures complexity of the boundary\n",
        "#     - Medical significance: Complex boundaries suggest malignancy\n",
        "#\n",
        "# WHY THREE VERSIONS OF EACH MEASUREMENT?\n",
        "# ----------------------------------------\n",
        "# ‚Ä¢ MEAN: Overall average characteristic\n",
        "# ‚Ä¢ ERROR: Variability within the sample (uncertainty measure)\n",
        "# ‚Ä¢ WORST: Most severe measurements (often most diagnostic)\n",
        "#\n",
        "# CLINICAL INTERPRETATION:\n",
        "# ------------------------\n",
        "# Medical professionals look for:\n",
        "# ‚úì Large radius/perimeter/area ‚Üí Concerning\n",
        "# ‚úì High texture/concavity ‚Üí Irregular cells ‚Üí Concerning\n",
        "# ‚úì Low smoothness/symmetry ‚Üí Irregular shape ‚Üí Concerning\n",
        "# ‚úì High \"worst\" values ‚Üí Most concerning areas ‚Üí Diagnostic\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üî¨ COMPREHENSIVE FEATURE ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Categorize features by type\n",
        "mean_features = [col for col in df.columns if 'mean' in col]\n",
        "se_features = [col for col in df.columns if 'error' in col or 'se' in col]\n",
        "worst_features = [col for col in df.columns if 'worst' in col]\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìã FEATURE ORGANIZATION:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"\\n1. MEAN FEATURES ({len(mean_features)} features):\")\n",
        "print(\"   These represent the average measurements across all cells:\")\n",
        "for i, feature in enumerate(mean_features, 1):\n",
        "    print(f\"   {i:2d}. {feature}\")\n",
        "\n",
        "print(f\"\\n2. STANDARD ERROR FEATURES ({len(se_features)} features):\")\n",
        "print(\"   These represent the variability/uncertainty in measurements:\")\n",
        "for i, feature in enumerate(se_features, 1):\n",
        "    print(f\"   {i:2d}. {feature}\")\n",
        "\n",
        "print(f\"\\n3. WORST FEATURES ({len(worst_features)} features):\")\n",
        "print(\"   These represent the most extreme measurements:\")\n",
        "for i, feature in enumerate(worst_features, 1):\n",
        "    print(f\"   {i:2d}. {feature}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìä FEATURE VALUE RANGES:\")\n",
        "print(\"-\" * 80)\n",
        "print(\"\\nUnderstanding the scale of measurements:\")\n",
        "print(df[mean_features].describe().loc[['min', 'max']].round(2))\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üí° KEY OBSERVATIONS:\")\n",
        "print(\"-\" * 80)\n",
        "print(\"\"\"\n",
        "1. SCALE DIFFERENCES:\n",
        "   ‚Ä¢ Features have vastly different scales\n",
        "   ‚Ä¢ Example: 'mean area' ranges from ~143 to ~2501\n",
        "   ‚Ä¢ Example: 'mean smoothness' ranges from ~0.05 to ~0.16\n",
        "   ‚Ä¢ Implication: MUST scale features before ML modeling\n",
        "\n",
        "2. PHYSICAL MEANING:\n",
        "   ‚Ä¢ Size features (radius, perimeter, area) are correlated\n",
        "   ‚Ä¢ Shape features (smoothness, symmetry) describe regularity\n",
        "   ‚Ä¢ Texture features describe cell appearance variability\n",
        "\n",
        "3. DIAGNOSTIC RELEVANCE:\n",
        "   ‚Ä¢ \"Worst\" features often most important for diagnosis\n",
        "   ‚Ä¢ They capture the most abnormal regions of the tumor\n",
        "   ‚Ä¢ Medical professionals focus on worst-case characteristics\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìä GENERATING FEATURE DISTRIBUTION VISUALIZATIONS...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Visualization of mean features distribution by diagnosis\n",
        "fig, axes = plt.subplots(2, 5, figsize=(22, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, feature in enumerate(mean_features):\n",
        "    # Separate data by diagnosis\n",
        "    malignant_data = df[df['diagnosis'] == 0][feature]\n",
        "    benign_data = df[df['diagnosis'] == 1][feature]\n",
        "\n",
        "    # Create overlapping histograms\n",
        "    axes[idx].hist(malignant_data, bins=25, alpha=0.6, label='Malignant',\n",
        "                   color='#FF6B6B', edgecolor='black', linewidth=0.5)\n",
        "    axes[idx].hist(benign_data, bins=25, alpha=0.6, label='Benign',\n",
        "                   color='#4ECDC4', edgecolor='black', linewidth=0.5)\n",
        "\n",
        "    # Styling\n",
        "    axes[idx].set_title(feature.replace('mean ', '').title(),\n",
        "                        fontsize=11, fontweight='bold')\n",
        "    axes[idx].set_xlabel('Value', fontsize=9)\n",
        "    axes[idx].set_ylabel('Frequency', fontsize=9)\n",
        "    axes[idx].legend(fontsize=8, loc='upper right')\n",
        "    axes[idx].grid(alpha=0.3, linestyle='--')\n",
        "\n",
        "    # Add statistical annotations\n",
        "    mal_mean = malignant_data.mean()\n",
        "    ben_mean = benign_data.mean()\n",
        "    axes[idx].axvline(mal_mean, color='#FF6B6B', linestyle='--',\n",
        "                      linewidth=2, alpha=0.7, label=f'M Œº={mal_mean:.1f}')\n",
        "    axes[idx].axvline(ben_mean, color='#4ECDC4', linestyle='--',\n",
        "                      linewidth=2, alpha=0.7, label=f'B Œº={ben_mean:.1f}')\n",
        "\n",
        "plt.suptitle('Distribution of Mean Features by Diagnosis\\n' +\n",
        "             'Red = Malignant | Teal = Benign | Dashed lines = Mean values',\n",
        "             fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print"
      ],
      "metadata": {
        "id": "qBQGLIKWMNYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLonJCBNX7CK"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FEATURE SELECTION FOR MODELING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get all encoded features and numerical features\n",
        "encoded_features = [col for col in df_encoded.columns if col.endswith('_encoded')]\n",
        "original_numerical = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak', 'FastingBS']\n",
        "engineered_numerical = ['HR_Achievement_Pct', 'Simple_Risk_Score']\n",
        "\n",
        "# Combine features\n",
        "feature_cols = [col for col in original_numerical if col in df_encoded.columns]\n",
        "feature_cols.extend([col for col in encoded_features if col in df_encoded.columns])\n",
        "feature_cols.extend([col for col in engineered_numerical if col in df_encoded.columns])\n",
        "\n",
        "# Remove target if accidentally included\n",
        "if target_col in feature_cols:\n",
        "    feature_cols.remove(target_col)\n",
        "\n",
        "print(f\"Selected {len(feature_cols)} features for modeling:\")\n",
        "for i, col in enumerate(feature_cols, 1):\n",
        "    print(f\"  {i:2d}. {col}\")\n",
        "\n",
        "# Create feature matrix and target vector\n",
        "X = df_encoded[feature_cols].fillna(0)  # Fill any remaining NaN with 0\n",
        "y = df_encoded[target_col]\n",
        "\n",
        "print(f\"\\n‚úì Feature matrix (X): {X.shape[0]} patients √ó {X.shape[1]} features\")\n",
        "print(f\"‚úì Target vector (y): {y.shape[0]} patients\")\n",
        "print(f\"‚úì Class distribution:\")\n",
        "print(f\"   - Class 0 (No Disease): {(y == 0).sum()} ({(y == 0).sum()/len(y)*100:.1f}%)\")\n",
        "print(f\"   - Class 1 (Disease):    {(y == 1).sum()} ({(y == 1).sum()/len(y)*100:.1f}%)\")\n",
        "\n",
        "# ### 5.2 Feature Importance Analysis (Preliminary)\n",
        "\n",
        "\"\"\"\n",
        "Use Random Forest to identify most important features before modeling\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PRELIMINARY FEATURE IMPORTANCE ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Train Random Forest for feature importance\n",
        "rf_temp = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "rf_temp.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_cols,\n",
        "    'Importance': rf_temp.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 15 Most Important Features:\")\n",
        "print(feature_importance_df.head(15).to_string(index=False))\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(12, 8))\n",
        "top_15 = feature_importance_df.head(15)\n",
        "plt.barh(range(len(top_15)), top_15['Importance'], color='steelblue',\n",
        "        edgecolor='black', alpha=0.8)\n",
        "plt.yticks(range(len(top_15)), top_15['Feature'])\n",
        "plt.xlabel('Importance Score')\n",
        "plt.title('Top 15 Most Important Features (Random Forest)', fontsize=14, fontweight='bold')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---\n",
        "# ## 6. Data Splitting and Scaling\n",
        "# ### 6.1 Train-Test Split\n",
        "\n",
        "\"\"\"\n",
        "Split data into training and testing sets with stratification\n",
        "to maintain class distribution\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAIN-TEST SPLIT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"‚úì Training set: {len(X_train)} patients ({len(X_train)/len(X)*100:.1f}%)\")\n",
        "print(f\"‚úì Test set: {len(X_test)} patients ({len(X_test)/len(X)*100:.1f}%)\")\n",
        "print(f\"‚úì Features: {X_train.shape[1]}\")\n",
        "\n",
        "print(f\"\\nClass distribution in training set:\")\n",
        "print(f\"   - Class 0: {(y_train == 0).sum()} ({(y_train == 0).sum()/len(y_train)*100:.1f}%)\")\n",
        "print(f\"   - Class 1: {(y_train == 1).sum()} ({(y_train == 1).sum()/len(y_train)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nClass distribution in test set:\")\n",
        "print(f\"   - Class 0: {(y_test == 0).sum()} ({(y_test == 0).sum()/len(y_test)*100:.1f}%)\")\n",
        "print(f\"   - Class 1: {(y_test == 1).sum()} ({(y_test == 1).sum()/len(y_test)*100:.1f}%)\")\n",
        "\n",
        "# ### 6.2 Feature Scaling\n",
        "\n",
        "\"\"\"\n",
        "Standardize features to have zero mean and unit variance.\n",
        "Critical for distance-based algorithms like SVM and KNN.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FEATURE SCALING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"‚úì Features scaled using StandardScaler\")\n",
        "print(f\"  Training set: Mean ‚âà 0, Std ‚âà 1 for all features\")\n",
        "\n",
        "# Convert back to DataFrame for easier handling\n",
        "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=feature_cols, index=X_train.index)\n",
        "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=feature_cols, index=X_test.index)\n",
        "\n",
        "# ---\n",
        "# ## 7. Model Training and Evaluation\n",
        "# ### 7.1 Model 1: Logistic Regression\n",
        "\n",
        "\"\"\"\n",
        "Baseline Model: Logistic Regression\n",
        "- Simple, interpretable, fast\n",
        "- Good for linear relationships\n",
        "- Provides probability estimates\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL 1: LOGISTIC REGRESSION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "lr_model = LogisticRegression(random_state=42, max_iter=1000, solver='lbfgs')\n",
        "lr_model.fit(X_train_scaled_df, y_train)\n",
        "\n",
        "y_pred_lr = lr_model.predict(X_test_scaled_df)\n",
        "y_pred_proba_lr = lr_model.predict_proba(X_test_scaled_df)[:, 1]\n",
        "\n",
        "# Evaluate\n",
        "lr_accuracy = accuracy_score(y_test, y_pred_lr)\n",
        "lr_precision = precision_score(y_test, y_pred_lr)\n",
        "lr_recall = recall_score(y_test, y_pred_lr)\n",
        "lr_f1 = f1_score(y_test, y_pred_lr)\n",
        "lr_auc = roc_auc_score(y_test, y_pred_proba_lr)\n",
        "\n",
        "print(f\"\\nPerformance Metrics:\")\n",
        "print(f\"  ‚Ä¢ Accuracy:  {lr_accuracy:.4f} ({lr_accuracy*100:.2f}%)\")\n",
        "print(f\"  ‚Ä¢ Precision: {lr_precision:.4f} (of predicted positives, {lr_precision*100:.1f}% are correct)\")\n",
        "print(f\"  ‚Ä¢ Recall:    {lr_recall:.4f} (detected {lr_recall*100:.1f}% of actual disease cases)\")\n",
        "print(f\"  ‚Ä¢ F1-Score:  {lr_f1:.4f} (harmonic mean of precision and recall)\")\n",
        "print(f\"  ‚Ä¢ ROC-AUC:   {lr_auc:.4f} (area under ROC curve)\")\n",
        "\n",
        "# ### 7.2 Model 2: Random Forest Classifier\n",
        "\n",
        "\"\"\"\n",
        "Ensemble Model: Random Forest\n",
        "- Handles non-linear relationships\n",
        "- Robust to outliers\n",
        "- Provides feature importance\n",
        "- Less prone to overfitting\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL 2: RANDOM FOREST CLASSIFIER\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    min_samples_split=10,\n",
        "    min_samples_leaf=4,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate\n",
        "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
        "rf_precision = precision_score(y_test, y_pred_rf)\n",
        "rf_recall = recall_score(y_test, y_pred_rf)\n",
        "rf_f1 = f1_score(y_test, y_pred_rf)\n",
        "rf_auc = roc_auc_score(y_test, y_pred_proba_rf)\n",
        "\n",
        "print(f\"\\nPerformance Metrics:\")\n",
        "print(f\"  ‚Ä¢ Accuracy:  {rf_accuracy:.4f} ({rf_accuracy*100:.2f}%)\")\n",
        "print(f\"  ‚Ä¢ Precision: {rf_precision:.4f} (of predicted positives, {rf_precision*100:.1f}% are correct)\")\n",
        "print(f\"  ‚Ä¢ Recall:    {rf_recall:.4f} (detected {rf_recall*100:.1f}% of actual disease cases)\")\n",
        "print(f\"  ‚Ä¢ F1-Score:  {rf_f1:.4f}\")\n",
        "print(f\"  ‚Ä¢ ROC-AUC:   {rf_auc:.4f}\")\n",
        "\n",
        "# ### 7.3 Model 3: Support Vector Machine (SVM)\n",
        "\n",
        "\"\"\"\n",
        "Support Vector Machine with RBF Kernel\n",
        "- Effective in high-dimensional spaces\n",
        "- Good for non-linear classification\n",
        "- Memory efficient\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL 3: SUPPORT VECTOR MACHINE (SVM)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "svm_model = SVC(kernel='rbf', probability=True, random_state=42, C=1.0, gamma='scale')\n",
        "svm_model.fit(X_train_scaled_df, y_train)\n",
        "\n",
        "y_pred_svm = svm_model.predict(X_test_scaled_df)\n",
        "y_pred_proba_svm = svm_model.predict_proba(X_test_scaled_df)[:, 1]\n",
        "\n",
        "# Evaluate\n",
        "svm_accuracy = accuracy_score(y_test, y_pred_svm)\n",
        "svm_precision = precision_score(y_test, y_pred_svm)\n",
        "svm_recall = recall_score(y_test, y_pred_svm)\n",
        "svm_f1 = f1_score(y_test, y_pred_svm)\n",
        "svm_auc = roc_auc_score(y_test, y_pred_proba_svm)\n",
        "\n",
        "print(f\"\\nPerformance Metrics:\")\n",
        "print(f\"  ‚Ä¢ Accuracy:  {svm_accuracy:.4f} ({svm_accuracy*100:.2f}%)\")\n",
        "print(f\"  ‚Ä¢ Precision: {svm_precision:.4f} (of predicted positives, {svm_precision*100:.1f}% are correct)\")\n",
        "print(f\"  ‚Ä¢ Recall:    {svm_recall:.4f} (detected {svm_recall*100:.1f}% of actual disease cases)\")\n",
        "print(f\"  ‚Ä¢ F1-Score:  {svm_f1:.4f}\")\n",
        "print(f\"  ‚Ä¢ ROC-AUC:   {svm_auc:.4f}\")\n",
        "\n",
        "# ### 7.4 Model 4: Gradient Boosting Classifier\n",
        "\n",
        "\"\"\"\n",
        "Gradient Boosting: Advanced Ensemble Method\n",
        "- Sequential learning\n",
        "- Often achieves high accuracy\n",
        "- Handles complex patterns\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL 4: GRADIENT BOOSTING CLASSIFIER\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "gb_model = GradientBoostingClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=5,\n",
        "    min_samples_split=10,\n",
        "    random_state=42\n",
        ")\n",
        "gb_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_gb = gb_model.predict(X_test)\n",
        "y_pred_proba_gb = gb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate\n",
        "gb_accuracy = accuracy_score(y_test, y_pred_gb)\n",
        "gb_precision = precision_score(y_test, y_pred_gb)\n",
        "gb_recall = recall_score(y_test, y_pred_gb)\n",
        "gb_f1 = f1_score(y_test, y_pred_gb)\n",
        "gb_auc = roc_auc_score(y_test, y_pred_proba_gb)\n",
        "\n",
        "print(f\"\\nPerformance Metrics:\")\n",
        "print(f\"  ‚Ä¢ Accuracy:  {gb_accuracy:.4f} ({gb_accuracy*100:.2f}%)\")\n",
        "print(f\"  ‚Ä¢ Precision: {gb_precision:.4f} (of predicted positives, {gb_precision*100:.1f}% are correct)\")\n",
        "print(f\"  ‚Ä¢ Recall:    {gb_recall:.4f} (detected {gb_recall*100:.1f}% of actual disease cases)\")\n",
        "print(f\"  ‚Ä¢ F1-Score:  {gb_f1:.4f}\")\n",
        "print(f\"  ‚Ä¢ ROC-AUC:   {gb_auc:.4f}\")\n",
        "\n",
        "# ### 7.5 Model 5: K-Nearest Neighbors (KNN)\n",
        "\n",
        "\"\"\"\n",
        "K-Nearest Neighbors\n",
        "- Non-parametric, instance-based learning\n",
        "- Simple intuition: similar patients have similar outcomes\n",
        "- No training phase (lazy learning)\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL 5: K-NEAREST NEIGHBORS (KNN)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5, weights='distance', metric='euclidean')\n",
        "knn_model.fit(X_train_scaled_df, y_train)\n",
        "\n",
        "y_pred_knn = knn_model.predict(X_test_scaled_df)\n",
        "y_pred_proba_knn = knn_model.predict_proba(X_test_scaled_df)[:, 1]\n",
        "\n",
        "# Evaluate\n",
        "knn_accuracy = accuracy_score(y_test, y_pred_knn)\n",
        "knn_precision = precision_score(y_test, y_pred_knn)\n",
        "knn_recall = recall_score(y_test, y_pred_knn)\n",
        "knn_f1 = f1_score(y_test, y_pred_knn)\n",
        "knn_auc = roc_auc_score(y_test, y_pred_proba_knn)\n",
        "\n",
        "print(f\"\\nPerformance Metrics:\")\n",
        "print(f\"  ‚Ä¢ Accuracy:  {knn_accuracy:.4f} ({knn_accuracy*100:.2f}%)\")\n",
        "print(f\"  ‚Ä¢ Precision: {knn_precision:.4f} (of predicted positives, {knn_precision*100:.1f}% are correct)\")\n",
        "print(f\"  ‚Ä¢ Recall:    {knn_recall:.4f} (detected {knn_recall*100:.1f}% of actual disease cases)\")\n",
        "print(f\"  ‚Ä¢ F1-Score:  {knn_f1:.4f}\")\n",
        "print(f\"  ‚Ä¢ ROC-AUC:   {knn_auc:.4f}\")\n",
        "\n",
        "# ### 7.6 Model 6: Naive Bayes\n",
        "\n",
        "\"\"\"\n",
        "Gaussian Naive Bayes\n",
        "- Probabilistic classifier\n",
        "- Fast and efficient\n",
        "- Works well with small datasets\n",
        "- Assumes feature independence\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL 6: NAIVE BAYES (GAUSSIAN)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train_scaled_df, y_train)\n",
        "\n",
        "y_pred_nb = nb_model.predict(X_test_scaled_df)\n",
        "y_pred_proba_nb = nb_model.predict_proba(X_test_scaled_df)[:, 1]\n",
        "\n",
        "# Evaluate\n",
        "nb_accuracy = accuracy_score(y_test, y_pred_nb)\n",
        "nb_precision = precision_score(y_test, y_pred_nb)\n",
        "nb_recall = recall_score(y_test, y_pred_nb)\n",
        "nb_f1 = f1_score(y_test, y_pred_nb)\n",
        "nb_auc = roc_auc_score(y_test, y_pred_proba_nb)\n",
        "\n",
        "print(f\"\\nPerformance Metrics:\")\n",
        "print(f\"  ‚Ä¢ Accuracy:  {nb_accuracy:.4f} ({nb_accuracy*100:.2f}%)\")\n",
        "print(f\"  ‚Ä¢ Precision: {nb_precision:.4f} (of predicted positives, {nb_precision*100:.1f}% are correct)\")\n",
        "print(f\"  ‚Ä¢ Recall:    {nb_recall:.4f} (detected {nb_recall*100:.1f}% of actual disease cases)\")\n",
        "print(f\"  ‚Ä¢ F1-Score:  {nb_f1:.4f}\")\n",
        "print(f\"  ‚Ä¢ ROC-AUC:   {nb_auc:.4f}\")\n",
        "\n",
        "# ---\n",
        "# ## 8. Model Comparison and Analysis\n",
        "# ### 8.1 Comprehensive Model Comparison\n",
        "\n",
        "\"\"\"\n",
        "Compare all models across multiple metrics\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPREHENSIVE MODEL PERFORMANCE COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': ['Logistic Regression', 'Random Forest', 'SVM',\n",
        "              'Gradient Boosting', 'KNN', 'Naive Bayes'],\n",
        "    'Accuracy': [lr_accuracy, rf_accuracy, svm_accuracy, gb_accuracy, knn_accuracy, nb_accuracy],\n",
        "    'Precision': [lr_precision, rf_precision, svm_precision, gb_precision, knn_precision, nb_precision],\n",
        "    'Recall': [lr_recall, rf_recall, svm_recall, gb_recall, knn_recall, nb_recall],\n",
        "    'F1-Score': [lr_f1, rf_f1, svm_f1, gb_f1, knn_f1, nb_f1],\n",
        "    'ROC-AUC': [lr_auc, rf_auc, svm_auc, gb_auc, knn_auc, nb_auc]\n",
        "})\n",
        "\n",
        "# Sort by F1-Score\n",
        "comparison_df = comparison_df.sort_values('F1-Score', ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"\\nModel Performance Summary (Ranked by F1-Score):\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
        "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6', '#1abc9c']\n",
        "\n",
        "for idx, metric in enumerate(metrics):\n",
        "    axes[idx].bar(range(len(comparison_df)), comparison_df[metric],\n",
        "                 color=colors, edgecolor='black', alpha=0.8)\n",
        "    axes[idx].set_xticks(range(len(comparison_df)))\n",
        "    axes[idx].set_xticklabels(comparison_df['Model'], rotation=45, ha='right')\n",
        "    axes[idx].set_title(f'{metric} Comparison', fontweight='bold', fontsize=12)\n",
        "    axes[idx].set_ylabel(metric)\n",
        "    axes[idx].set_ylim([0, 1.1])\n",
        "    axes[idx].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Add value labels\n",
        "    for i, v in enumerate(comparison_df[metric]):\n",
        "        axes[idx].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold', fontsize=9)\n",
        "\n",
        "# Overall performance radar chart\n",
        "axes[5].remove()\n",
        "ax_radar = fig.add_subplot(2, 3, 6, projection='polar')\n",
        "\n",
        "# Get best model for radar chart\n",
        "best_model_idx = comparison_df['F1-Score'].idxmax()\n",
        "best_model_data = comparison_df.iloc[best_model_idx]\n",
        "\n",
        "categories = metrics\n",
        "values = best_model_data[metrics].values\n",
        "angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
        "values = np.concatenate((values, [values[0]]))\n",
        "angles += angles[:1]\n",
        "\n",
        "ax_radar.plot(angles, values, 'o-', linewidth=2, color='red', label=best_model_data['Model'])\n",
        "ax_radar.fill(angles, values, alpha=0.25, color='red')\n",
        "ax_radar.set_xticks(angles[:-1])\n",
        "ax_radar.set_xticklabels(categories)\n",
        "ax_radar.set_ylim(0, 1)\n",
        "ax_radar.set_title('Best Model Performance\\nRadar Chart', fontweight='bold', pad=20)\n",
        "ax_radar.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
        "ax_radar.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Select best model\n",
        "best_model_name = comparison_df.iloc[0]['Model']\n",
        "best_f1 = comparison_df.iloc[0]['F1-Score']\n",
        "best_auc = comparison_df.iloc[0]['ROC-AUC']\n",
        "\n",
        "print(f\"\\nüèÜ BEST PERFORMING MODEL: {best_model_name}\")\n",
        "print(f\"   ‚Ä¢ F1-Score: {best_f1:.4f}\")\n",
        "print(f\"   ‚Ä¢ ROC-AUC:  {best_auc:.4f}\")\n",
        "print(f\"   ‚Ä¢ Accuracy: {comparison_df.iloc[0]['Accuracy']:.4f}\")\n",
        "\n",
        "# Map to actual model and predictions\n",
        "model_map = {\n",
        "    'Logistic Regression': (lr_model, y_pred_lr, y_pred_proba_lr),\n",
        "    'Random Forest': (rf_model, y_pred_rf, y_pred_proba_rf),\n",
        "    'SVM': (svm_model, y_pred_svm, y_pred_proba_svm),\n",
        "    'Gradient Boosting': (gb_model, y_pred_gb, y_pred_proba_gb),\n",
        "    'KNN': (knn_model, y_pred_knn, y_pred_proba_knn),\n",
        "    'Naive Bayes': (nb_model, y_pred_nb, y_pred_proba_nb)\n",
        "}\n",
        "best_model, y_pred_best, y_pred_proba_best = model_map[best_model_name]\n",
        "\n",
        "# ### 8.2 Confusion Matrix Analysis\n",
        "\n",
        "\"\"\"\n",
        "Detailed breakdown of predictions for all models\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CONFUSION MATRIX ANALYSIS - ALL MODELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "predictions = [\n",
        "    ('Logistic Regression', y_pred_lr),\n",
        "    ('Random Forest', y_pred_rf),\n",
        "    ('SVM', y_pred_svm),\n",
        "    ('Gradient Boosting', y_pred_gb),\n",
        "    ('KNN', y_pred_knn),\n",
        "    ('Naive Bayes', y_pred_nb)\n",
        "]\n",
        "\n",
        "for idx, (model_name, y_pred) in enumerate(predictions):\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[idx],\n",
        "               xticklabels=['No Disease', 'Disease'],\n",
        "               yticklabels=['No Disease', 'Disease'])\n",
        "    axes[idx].set_title(f'{model_name}\\nConfusion Matrix', fontweight='bold')\n",
        "    axes[idx].set_ylabel('Actual')\n",
        "    axes[idx].set_xlabel('Predicted')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Detailed metrics for best model\n",
        "cm_best = confusion_matrix(y_test, y_pred_best)\n",
        "tn, fp, fn, tp = cm_best.ravel()\n",
        "\n",
        "print(f\"\\nDetailed Analysis - {best_model_name}:\")\n",
        "print(f\"  ‚Ä¢ True Positives (TP):  {tp} - Correctly identified disease cases\")\n",
        "print(f\"  ‚Ä¢ True Negatives (TN):  {tn} - Correctly identified healthy patients\")\n",
        "print(f\"  ‚Ä¢ False Positives (FP): {fp} - Healthy patients incorrectly flagged\")\n",
        "print(f\"  ‚Ä¢ False Negatives (FN): {fn} - Disease cases missed\")\n",
        "print(f\"\\n  ‚Ä¢ Sensitivity (Recall): {tp/(tp+fn):.4f} - {tp/(tp+fn)*100:.1f}% of disease cases detected\")\n",
        "print(f\"  ‚Ä¢ Specificity:          {tn/(tn+fp):.4f} - {tn/(tn+fp)*100:.1f}% of healthy correctly identified\")\n",
        "\n",
        "# ### 8.3 ROC Curve Comparison\n",
        "\n",
        "\"\"\"\n",
        "ROC curves show trade-off between true positive and false positive rates\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ROC CURVE ANALYSIS - ALL MODELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plot ROC curves\n",
        "roc_data = [\n",
        "    ('Logistic Regression', y_pred_proba_lr, lr_auc),\n",
        "    ('Random Forest', y_pred_proba_rf, rf_auc),\n",
        "    ('SVM', y_pred_proba_svm, svm_auc),\n",
        "    ('Gradient Boosting', y_pred_proba_gb, gb_auc),\n",
        "    ('KNN', y_pred_proba_knn, knn_auc),\n",
        "    ('Naive Bayes', y_pred_proba_nb, nb_auc)\n",
        "]\n",
        "\n",
        "for model_name, y_proba, auc_score in roc_data:\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "    plt.plot(fpr, tpr, linewidth=2, label=f'{model_name} (AUC = {auc_score:.4f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier (AUC = 0.5000)')\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate (Sensitivity)', fontsize=12)\n",
        "plt.title('ROC Curves - All Models', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right', fontsize=10)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"üìä ROC-AUC Interpretation:\")\n",
        "print(f\"   ‚Ä¢ AUC = 1.0: Perfect classifier\")\n",
        "print(f\"   ‚Ä¢ AUC = 0.9-1.0: Excellent\")\n",
        "print(f\"   ‚Ä¢ AUC = 0.8-0.9: Good\")\n",
        "print(f\"   ‚Ä¢ AUC = 0.7-0.8: Fair\")\n",
        "print(f\"   ‚Ä¢ AUC = 0.5: Random guess\")\n",
        "\n",
        "# ### 8.4 Precision-Recall Curve\n",
        "\n",
        "\"\"\"\n",
        "Precision-Recall curves are useful for imbalanced datasets\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PRECISION-RECALL CURVE ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "for model_name, y_proba, _ in roc_data:\n",
        "    precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
        "    pr_auc = auc(recall, precision)\n",
        "    plt.plot(recall, precision, linewidth=2, label=f'{model_name} (AP = {pr_auc:.4f})')\n",
        "\n",
        "plt.xlabel('Recall (Sensitivity)', fontsize=12)\n",
        "plt.ylabel('Precision', fontsize=12)\n",
        "plt.title('Precision-Recall Curves - All Models', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='best', fontsize=10)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ### 8.5 Classification Report - Best Model\n",
        "\n",
        "\"\"\"\n",
        "Detailed classification metrics for the best model\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"CLASSIFICATION REPORT - {best_model_name.upper()}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(classification_report(y_test, y_pred_best,\n",
        "                          target_names=['No Disease', 'Heart Disease'],\n",
        "                          digits=4))\n",
        "\n",
        "# ---\n",
        "# ## 9. Cross-Validation\n",
        "# ### 9.1 K-Fold Cross-Validation for All Models\n",
        "\n",
        "\"\"\"\n",
        "Perform k-fold cross-validation to assess model stability\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"5-FOLD CROSS-VALIDATION ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "cv_results = {}\n",
        "\n",
        "# Models to cross-validate\n",
        "models_cv = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
        "    'SVM': SVC(kernel='rbf', probability=True, random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
        "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
        "    'Naive Bayes': GaussianNB()\n",
        "}\n",
        "\n",
        "for model_name, model in models_cv.items():\n",
        "    # Use scaled data for distance-based models\n",
        "    if model_name in ['Logistic Regression', 'SVM', 'KNN', 'Naive Bayes']:\n",
        "        X_cv = X_train_scaled_df\n",
        "    else:\n",
        "        X_cv = X_train\n",
        "\n",
        "    cv_scores = cross_val_score(model, X_cv, y_train, cv=cv, scoring='f1')\n",
        "    cv_results[model_name] = cv_scores\n",
        "\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    print(f\"  CV Scores: {cv_scores}\")\n",
        "    print(f\"  Mean F1:   {cv_scores.mean():.4f}\")\n",
        "    print(f\"  Std Dev:   {cv_scores.std():.4f}\")\n",
        "\n",
        "# Visualize CV results\n",
        "plt.figure(figsize=(14, 6))\n",
        "cv_df = pd.DataFrame(cv_results)\n",
        "\n",
        "bp = plt.boxplot([cv_df[col] for col in cv_df.columns],\n",
        "                labels=cv_df.columns,\n",
        "                patch_artist=True,\n",
        "                showmeans=True)\n",
        "\n",
        "# Color the boxes\n",
        "for patch, color in zip(bp['boxes'], colors):\n",
        "    patch.set_facecolor(color)\n",
        "    patch.set_alpha(0.7)\n",
        "\n",
        "plt.xlabel('Model', fontsize=12)\n",
        "plt.ylabel('F1-Score', fontsize=12)\n",
        "plt.title('5-Fold Cross-Validation Results', fontsize=14, fontweight='bold')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---\n",
        "# ## 10. Key Insights and Clinical Findings\n",
        "# ### 10.1 Feature Importance - Clinical Insights\n",
        "\n",
        "\"\"\"\n",
        "Analyze which clinical factors are most predictive of heart disease\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)# Heart Failure Prediction Using Machine Learning\n",
        "# ================================================\n",
        "# Data Exploration, Model Comparison, and Insights\n",
        "#\n",
        "# Objective: Develop a machine learning pipeline to predict heart failure\n",
        "# using clinical data, compare multiple models, and derive actionable insights\n",
        "\n",
        "# ## 1. Project Setup and Environment Configuration\n",
        "# ### 1.1 Import Required Libraries\n",
        "\n",
        "\"\"\"\n",
        "This notebook implements a complete machine learning pipeline for heart failure prediction.\n",
        "We'll use:\n",
        "- Data manipulation: pandas, numpy\n",
        "- Visualization: matplotlib, seaborn\n",
        "- Machine Learning: scikit-learn\n",
        "- Statistical analysis: scipy\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "\n",
        "# Machine Learning Libraries\n",
        "from sklearn.model_selection import (train_test_split, cross_val_score,\n",
        "                                    StratifiedKFold, GridSearchCV, learning_curve)\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# ML Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                            f1_score, roc_auc_score, roc_curve, confusion_matrix,\n",
        "                            classification_report, precision_recall_curve, auc)\n",
        "\n",
        "# Feature Selection\n",
        "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
        "\n",
        "# Configuration\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette(\"Set2\")\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"HEART FAILURE PREDICTION USING MACHINE LEARNING\")\n",
        "print(\"Data Exploration, Model Comparison, and Clinical Insights\")\n",
        "print(\"=\"*80)\n",
        "print(\"‚úì All libraries imported successfully\")\n",
        "print(f\"‚úì Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# ### 1.2 Configure Display Settings\n",
        "\n",
        "\"\"\"\n",
        "Set up display options for better readability and professional visualizations\n",
        "\"\"\"\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['font.size'] = 10\n",
        "plt.rcParams['axes.titlesize'] = 14\n",
        "plt.rcParams['axes.labelsize'] = 11\n",
        "\n",
        "print(\"‚úì Display and visualization settings configured\")\n",
        "\n",
        "# ---\n",
        "# ## 2. Data Loading and Initial Exploration\n",
        "# ### 2.1 Load the Heart Failure Dataset\n",
        "\n",
        "\"\"\"\n",
        "Load the clinical heart failure dataset.\n",
        "This dataset contains various medical and demographic features\n",
        "used to predict the presence of heart disease.\n",
        "\"\"\"\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('heart_failure_data.csv')\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DATASET LOADED SUCCESSFULLY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Total Patients: {len(df):,}\")\n",
        "print(f\"Total Features: {len(df.columns)}\")\n",
        "print(f\"Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "# ### 2.2 Display Sample Data\n",
        "\n",
        "\"\"\"\n",
        "Examine the first and last few rows to understand data structure\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FIRST 10 PATIENT RECORDS\")\n",
        "print(\"=\"*80)\n",
        "print(df.head(10))\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"LAST 5 PATIENT RECORDS\")\n",
        "print(\"=\"*80)\n",
        "print(df.tail())\n",
        "\n",
        "# ### 2.3 Dataset Structure and Information\n",
        "\n",
        "\"\"\"\n",
        "Comprehensive overview of dataset structure, data types, and completeness\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DATASET STRUCTURE AND INFORMATION\")\n",
        "print(\"=\"*80)\n",
        "df.info()\n",
        "\n",
        "# ### 2.4 Feature Descriptions\n",
        "\n",
        "\"\"\"\n",
        "Understanding each clinical feature in the dataset:\n",
        "\"\"\"\n",
        "\n",
        "feature_descriptions = {\n",
        "    'Age': 'Age of the patient (years)',\n",
        "    'Sex': 'Sex of the patient (M: Male, F: Female)',\n",
        "    'ChestPainType': 'Type of chest pain (TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic)',\n",
        "    'RestingBP': 'Resting blood pressure (mm Hg)',\n",
        "    'Cholesterol': 'Serum cholesterol (mm/dl)',\n",
        "    'FastingBS': 'Fasting blood sugar > 120 mg/dl (1: Yes, 0: No)',\n",
        "    'RestingECG': 'Resting electrocardiogram results (Normal, ST, LVH)',\n",
        "    'MaxHR': 'Maximum heart rate achieved (60-202)',\n",
        "    'ExerciseAngina': 'Exercise-induced angina (Y: Yes, N: No)',\n",
        "    'Oldpeak': 'ST depression induced by exercise relative to rest',\n",
        "    'ST_Slope': 'Slope of peak exercise ST segment (Up, Flat, Down)',\n",
        "    'HeartDisease': 'Target variable - Presence of heart disease (1: Yes, 0: No)'\n",
        "}\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CLINICAL FEATURE DESCRIPTIONS\")\n",
        "print(\"=\"*80)\n",
        "for feature, description in feature_descriptions.items():\n",
        "    # Check for similar column names (case-insensitive, space handling)\n",
        "    matching_cols = [col for col in df.columns if feature.lower().replace('_', '').replace(' ', '')\n",
        "                    == col.lower().replace('_', '').replace(' ', '')]\n",
        "    if matching_cols:\n",
        "        print(f\"  ‚Ä¢ {matching_cols[0]:20s}: {description}\")\n",
        "\n",
        "# ### 2.5 Statistical Summary\n",
        "\n",
        "\"\"\"\n",
        "Statistical overview of numerical features:\n",
        "- Central tendency (mean, median)\n",
        "- Spread (std, min, max)\n",
        "- Distribution characteristics\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STATISTICAL SUMMARY OF NUMERICAL FEATURES\")\n",
        "print(\"=\"*80)\n",
        "print(df.describe())\n",
        "\n",
        "# ### 2.6 Target Variable Analysis\n",
        "\n",
        "\"\"\"\n",
        "Analyze the distribution of heart disease (target variable).\n",
        "Understanding class balance is critical for model development.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TARGET VARIABLE ANALYSIS: HEART DISEASE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Find the target column (might be named differently)\n",
        "target_col = None\n",
        "for col in df.columns:\n",
        "    if 'heart' in col.lower() and 'disease' in col.lower():\n",
        "        target_col = col\n",
        "        break\n",
        "    elif col.lower() in ['target', 'output', 'class']:\n",
        "        target_col = col\n",
        "        break\n",
        "\n",
        "if target_col:\n",
        "    target_counts = df[target_col].value_counts().sort_index()\n",
        "    target_pct = (target_counts / len(df) * 100).round(2)\n",
        "\n",
        "    print(f\"Target Variable: '{target_col}'\")\n",
        "    print(\"\\nClass Distribution:\")\n",
        "    print(f\"  ‚Ä¢ No Heart Disease (0): {target_counts.get(0, 0):,} patients ({target_pct.get(0, 0)}%)\")\n",
        "    print(f\"  ‚Ä¢ Heart Disease (1):    {target_counts.get(1, 0):,} patients ({target_pct.get(1, 0)}%)\")\n",
        "\n",
        "    # Check for class imbalance\n",
        "    if len(target_counts) == 2:\n",
        "        imbalance_ratio = max(target_counts) / min(target_counts)\n",
        "        print(f\"\\n  Class Balance Ratio: {imbalance_ratio:.2f}:1\")\n",
        "\n",
        "        if imbalance_ratio > 1.5:\n",
        "            print(f\"  ‚ö† Moderate class imbalance detected\")\n",
        "        elif imbalance_ratio > 2:\n",
        "            print(f\"  ‚ö† Significant class imbalance detected\")\n",
        "        else:\n",
        "            print(f\"  ‚úì Classes are well balanced\")\n",
        "\n",
        "    # Visualize target distribution\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Bar chart\n",
        "    axes[0].bar(['No Disease', 'Heart Disease'], target_counts.values,\n",
        "               color=['#2ecc71', '#e74c3c'], edgecolor='black', alpha=0.8)\n",
        "    axes[0].set_title('Heart Disease Distribution', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_ylabel('Number of Patients')\n",
        "    axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Add value labels\n",
        "    for i, v in enumerate(target_counts.values):\n",
        "        axes[0].text(i, v + 10, f'{v:,}\\n({target_pct.values[i]}%)',\n",
        "                    ha='center', fontweight='bold')\n",
        "\n",
        "    # Pie chart\n",
        "    axes[1].pie(target_counts.values, labels=['No Disease', 'Heart Disease'],\n",
        "               autopct='%1.1f%%', colors=['#2ecc71', '#e74c3c'],\n",
        "               startangle=90, explode=[0, 0.1])\n",
        "    axes[1].set_title('Class Distribution', fontsize=14, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"‚ö† Warning: Target variable not found. Please specify the correct column name.\")\n",
        "\n",
        "# ---\n",
        "# ## 3. Data Preprocessing\n",
        "# ### 3.1 Missing Values Analysis\n",
        "\n",
        "\"\"\"\n",
        "Identify and quantify missing data across all features.\n",
        "Missing data can significantly impact model performance.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MISSING VALUES ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "missing_data = pd.DataFrame({\n",
        "    'Column': df.columns,\n",
        "    'Missing_Count': df.isnull().sum(),\n",
        "    'Missing_Percentage': (df.isnull().sum() / len(df) * 100).round(2),\n",
        "    'Data_Type': df.dtypes\n",
        "})\n",
        "\n",
        "missing_data = missing_data[missing_data['Missing_Count'] > 0].sort_values(\n",
        "    'Missing_Percentage', ascending=False)\n",
        "\n",
        "if len(missing_data) > 0:\n",
        "    print(\"Columns with Missing Values:\")\n",
        "    print(missing_data.to_string(index=False))\n",
        "\n",
        "    # Visualize missing data\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.barh(missing_data['Column'], missing_data['Missing_Percentage'],\n",
        "            color='coral', edgecolor='black', alpha=0.8)\n",
        "    plt.xlabel('Missing Percentage (%)')\n",
        "    plt.title('Missing Data by Feature', fontsize=14, fontweight='bold')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.grid(axis='x', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"‚úì Excellent! No missing values found in the dataset\")\n",
        "\n",
        "# ### 3.2 Handle Missing Values\n",
        "\n",
        "\"\"\"\n",
        "Strategy for handling missing values:\n",
        "1. Drop columns with >50% missing data\n",
        "2. Impute numerical features with median\n",
        "3. Impute categorical features with mode\n",
        "4. Remove rows with missing target variable\n",
        "\"\"\"\n",
        "\n",
        "df_clean = df.copy()\n",
        "initial_rows = len(df_clean)\n",
        "initial_cols = len(df_clean.columns)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"HANDLING MISSING VALUES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Remove columns with >50% missing\n",
        "if len(missing_data) > 0:\n",
        "    high_missing = missing_data[missing_data['Missing_Percentage'] > 50]['Column'].tolist()\n",
        "    if len(high_missing) > 0:\n",
        "        df_clean = df_clean.drop(columns=high_missing)\n",
        "        print(f\"‚úì Dropped {len(high_missing)} columns with >50% missing: {high_missing}\")\n",
        "\n",
        "# Remove rows with missing target\n",
        "if target_col and df_clean[target_col].isnull().sum() > 0:\n",
        "    df_clean = df_clean.dropna(subset=[target_col])\n",
        "    print(f\"‚úì Removed {initial_rows - len(df_clean)} rows with missing target variable\")\n",
        "\n",
        "# Impute numerical features with median\n",
        "numerical_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
        "if target_col in numerical_cols:\n",
        "    numerical_cols.remove(target_col)\n",
        "\n",
        "for col in numerical_cols:\n",
        "    if df_clean[col].isnull().sum() > 0:\n",
        "        median_val = df_clean[col].median()\n",
        "        df_clean[col].fillna(median_val, inplace=True)\n",
        "        print(f\"‚úì Imputed '{col}' missing values with median: {median_val:.2f}\")\n",
        "\n",
        "# Impute categorical features with mode\n",
        "categorical_cols = df_clean.select_dtypes(include=['object']).columns.tolist()\n",
        "for col in categorical_cols:\n",
        "    if df_clean[col].isnull().sum() > 0:\n",
        "        mode_val = df_clean[col].mode()[0]\n",
        "        df_clean[col].fillna(mode_val, inplace=True)\n",
        "        print(f\"‚úì Imputed '{col}' missing values with mode: {mode_val}\")\n",
        "\n",
        "print(f\"\\n‚úì Final dataset: {len(df_clean):,} patients √ó {len(df_clean.columns)} features\")\n",
        "print(f\"‚úì Data retention: {len(df_clean)/initial_rows*100:.1f}% of original rows\")\n",
        "\n",
        "# ### 3.3 Check for Duplicates\n",
        "\n",
        "\"\"\"\n",
        "Identify and remove duplicate patient records\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DUPLICATE RECORDS CHECK\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "duplicates = df_clean.duplicated().sum()\n",
        "print(f\"Duplicate rows found: {duplicates}\")\n",
        "\n",
        "if duplicates > 0:\n",
        "    df_clean = df_clean.drop_duplicates()\n",
        "    print(f\"‚úì Removed {duplicates} duplicate records\")\n",
        "    print(f\"‚úì Dataset now has {len(df_clean):,} unique patient records\")\n",
        "else:\n",
        "    print(\"‚úì No duplicate records found\")\n",
        "\n",
        "# ### 3.4 Data Type Corrections\n",
        "\n",
        "\"\"\"\n",
        "Ensure all features have appropriate data types\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DATA TYPE VALIDATION AND CORRECTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Binary features should be 0/1 integers\n",
        "binary_features = ['FastingBS', target_col] if target_col else ['FastingBS']\n",
        "for col in binary_features:\n",
        "    if col in df_clean.columns:\n",
        "        df_clean[col] = df_clean[col].astype(int)\n",
        "        print(f\"‚úì Converted '{col}' to integer type\")\n",
        "\n",
        "# Ensure numerical features are numeric\n",
        "for col in numerical_cols:\n",
        "    if col in df_clean.columns and df_clean[col].dtype == 'object':\n",
        "        try:\n",
        "            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
        "            print(f\"‚úì Converted '{col}' to numeric type\")\n",
        "        except:\n",
        "            print(f\"‚ö† Could not convert '{col}' to numeric\")\n",
        "\n",
        "print(f\"\\n‚úì Data type validation completed\")\n",
        "\n",
        "# ### 3.5 Encode Categorical Variables\n",
        "\n",
        "\"\"\"\n",
        "Convert categorical text features to numerical format for machine learning.\n",
        "We'll use Label Encoding for binary categories and create dummy variables for multi-class.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ENCODING CATEGORICAL VARIABLES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "df_encoded = df_clean.copy()\n",
        "label_encoders = {}\n",
        "\n",
        "# Get categorical columns\n",
        "categorical_columns = df_encoded.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "print(f\"Found {len(categorical_columns)} categorical columns: {categorical_columns}\")\n",
        "\n",
        "for col in categorical_columns:\n",
        "    unique_values = df_encoded[col].nunique()\n",
        "    print(f\"\\n  ‚Ä¢ {col}: {unique_values} unique values\")\n",
        "    print(f\"    Values: {df_encoded[col].unique().tolist()}\")\n",
        "\n",
        "    # Use Label Encoding\n",
        "    le = LabelEncoder()\n",
        "    df_encoded[col + '_encoded'] = le.fit_transform(df_encoded[col].astype(str))\n",
        "    label_encoders[col] = le\n",
        "\n",
        "    # Create mapping\n",
        "    mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "    print(f\"    Mapping: {mapping}\")\n",
        "\n",
        "print(f\"\\n‚úì Categorical encoding completed\")\n",
        "print(f\"‚úì Total features: {len(df_encoded.columns)}\")\n",
        "\n",
        "# ### 3.6 Feature Engineering\n",
        "\n",
        "\"\"\"\n",
        "Create additional features that might improve model performance:\n",
        "1. Age groups\n",
        "2. Blood pressure categories\n",
        "3. Cholesterol categories\n",
        "4. Heart rate zones\n",
        "5. Risk scores\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FEATURE ENGINEERING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. Age groups\n",
        "if 'Age' in df_encoded.columns:\n",
        "    df_encoded['Age_Group'] = pd.cut(df_encoded['Age'],\n",
        "                                     bins=[0, 40, 55, 70, 100],\n",
        "                                     labels=['Young', 'Middle', 'Senior', 'Elderly'])\n",
        "    df_encoded['Age_Group_encoded'] = LabelEncoder().fit_transform(df_encoded['Age_Group'].astype(str))\n",
        "    print(\"‚úì Created Age_Group: Young (<40), Middle (40-55), Senior (55-70), Elderly (70+)\")\n",
        "\n",
        "# 2. Blood Pressure categories\n",
        "if 'RestingBP' in df_encoded.columns:\n",
        "    def categorize_bp(bp):\n",
        "        if bp < 120:\n",
        "            return 'Normal'\n",
        "        elif bp < 130:\n",
        "            return 'Elevated'\n",
        "        elif bp < 140:\n",
        "            return 'High_Stage1'\n",
        "        else:\n",
        "            return 'High_Stage2'\n",
        "\n",
        "    df_encoded['BP_Category'] = df_encoded['RestingBP'].apply(categorize_bp)\n",
        "    df_encoded['BP_Category_encoded'] = LabelEncoder().fit_transform(df_encoded['BP_Category'])\n",
        "    print(\"‚úì Created BP_Category: Normal, Elevated, High_Stage1, High_Stage2\")\n",
        "\n",
        "# 3. Cholesterol categories\n",
        "if 'Cholesterol' in df_encoded.columns:\n",
        "    # Filter out zero values (often missing data coded as 0)\n",
        "    df_encoded['Cholesterol_Valid'] = df_encoded['Cholesterol'].replace(0, np.nan)\n",
        "\n",
        "    def categorize_cholesterol(chol):\n",
        "        if pd.isna(chol):\n",
        "            return 'Unknown'\n",
        "        elif chol < 200:\n",
        "            return 'Desirable'\n",
        "        elif chol < 240:\n",
        "            return 'Borderline'\n",
        "        else:\n",
        "            return 'High'\n",
        "\n",
        "    df_encoded['Chol_Category'] = df_encoded['Cholesterol_Valid'].apply(categorize_cholesterol)\n",
        "    df_encoded['Chol_Category_encoded'] = LabelEncoder().fit_transform(df_encoded['Chol_Category'])\n",
        "    print(\"‚úì Created Chol_Category: Desirable (<200), Borderline (200-240), High (>240)\")\n",
        "\n",
        "# 4. Heart Rate zones\n",
        "if 'MaxHR' in df_encoded.columns and 'Age' in df_encoded.columns:\n",
        "    df_encoded['Max_HR_Expected'] = 220 - df_encoded['Age']\n",
        "    df_encoded['HR_Achievement_Pct'] = (df_encoded['MaxHR'] / df_encoded['Max_HR_Expected']) * 100\n",
        "    print(\"‚úì Created HR_Achievement_Pct: Percentage of maximum heart rate achieved\")\n",
        "\n",
        "# 5. Risk score (simple composite)\n",
        "if all(col in df_encoded.columns for col in ['Age', 'RestingBP', 'Cholesterol', 'FastingBS']):\n",
        "    df_encoded['Simple_Risk_Score'] = (\n",
        "        (df_encoded['Age'] > 55).astype(int) +\n",
        "        (df_encoded['RestingBP'] > 130).astype(int) +\n",
        "        (df_encoded['Cholesterol'] > 200).astype(int) +\n",
        "        df_encoded['FastingBS']\n",
        "    )\n",
        "    print(\"‚úì Created Simple_Risk_Score: Composite cardiovascular risk indicator (0-4)\")\n",
        "\n",
        "print(f\"\\n‚úì Feature engineering completed\")\n",
        "print(f\"‚úì Total features now: {len(df_encoded.columns)}\")\n",
        "\n",
        "# ---\n",
        "# ## 4. Exploratory Data Analysis (EDA)\n",
        "# ### 4.1 Univariate Analysis - Numerical Features\n",
        "\n",
        "\"\"\"\n",
        "Analyze the distribution of numerical features\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EXPLORATORY DATA ANALYSIS: NUMERICAL FEATURES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "numerical_features = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']\n",
        "available_num_features = [f for f in numerical_features if f in df_encoded.columns]\n",
        "\n",
        "if len(available_num_features) > 0:\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, feature in enumerate(available_num_features[:6]):\n",
        "        axes[idx].hist(df_encoded[feature], bins=30, color='steelblue',\n",
        "                      edgecolor='black', alpha=0.7)\n",
        "        axes[idx].set_title(f'Distribution of {feature}', fontweight='bold')\n",
        "        axes[idx].set_xlabel(feature)\n",
        "        axes[idx].set_ylabel('Frequency')\n",
        "        axes[idx].axvline(df_encoded[feature].mean(), color='red', linestyle='--',\n",
        "                         linewidth=2, label=f'Mean: {df_encoded[feature].mean():.1f}')\n",
        "        axes[idx].axvline(df_encoded[feature].median(), color='green', linestyle='--',\n",
        "                         linewidth=2, label=f'Median: {df_encoded[feature].median():.1f}')\n",
        "        axes[idx].legend(fontsize=8)\n",
        "        axes[idx].grid(alpha=0.3)\n",
        "\n",
        "    # Hide extra subplots\n",
        "    for idx in range(len(available_num_features), 6):\n",
        "        axes[idx].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ### 4.2 Univariate Analysis - Categorical Features\n",
        "\n",
        "\"\"\"\n",
        "Analyze the distribution of categorical features\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EXPLORATORY DATA ANALYSIS: CATEGORICAL FEATURES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "categorical_features = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n",
        "# Find matching columns (handle different naming conventions)\n",
        "available_cat_features = []\n",
        "for feat in categorical_features:\n",
        "    matching = [col for col in df_clean.columns if feat.lower().replace('_', '') in col.lower().replace('_', '')]\n",
        "    if matching:\n",
        "        available_cat_features.append(matching[0])\n",
        "\n",
        "if len(available_cat_features) > 0:\n",
        "    n_features = len(available_cat_features)\n",
        "    n_cols = min(3, n_features)\n",
        "    n_rows = (n_features + n_cols - 1) // n_cols\n",
        "\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(6*n_cols, 5*n_rows))\n",
        "    if n_features == 1:\n",
        "        axes = [axes]\n",
        "    else:\n",
        "        axes = axes.flatten()\n",
        "\n",
        "    for idx, feature in enumerate(available_cat_features):\n",
        "        feature_counts = df_clean[feature].value_counts()\n",
        "        axes[idx].bar(range(len(feature_counts)), feature_counts.values,\n",
        "                     color='coral', edgecolor='black', alpha=0.8)\n",
        "        axes[idx].set_xticks(range(len(feature_counts)))\n",
        "        axes[idx].set_xticklabels(feature_counts.index, rotation=45, ha='right')\n",
        "        axes[idx].set_title(f'Distribution of {feature}', fontweight='bold')\n",
        "        axes[idx].set_ylabel('Count')\n",
        "        axes[idx].grid(axis='y', alpha=0.3)\n",
        "\n",
        "        # Add value labels\n",
        "        for i, v in enumerate(feature_counts.values):\n",
        "            axes[idx].text(i, v + 5, str(v), ha='center', fontweight='bold')\n",
        "\n",
        "    # Hide extra subplots\n",
        "    for idx in range(n_features, len(axes)):\n",
        "        axes[idx].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ### 4.3 Bivariate Analysis - Features vs Heart Disease\n",
        "\n",
        "\"\"\"\n",
        "Analyze how each feature relates to heart disease outcome\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BIVARIATE ANALYSIS: FEATURES VS HEART DISEASE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if target_col:\n",
        "    # Numerical features vs target\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, feature in enumerate(available_num_features[:6]):\n",
        "        df_encoded.boxplot(column=feature, by=target_col, ax=axes[idx])\n",
        "        axes[idx].set_title(f'{feature} by Heart Disease Status', fontweight='bold')\n",
        "        axes[idx].set_xlabel('Heart Disease (0=No, 1=Yes)')\n",
        "        axes[idx].set_ylabel(feature)\n",
        "        plt.suptitle('')\n",
        "        axes[idx].grid(alpha=0.3)\n",
        "\n",
        "    for idx in range(len(available_num_features), 6):\n",
        "        axes[idx].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Statistical comparison\n",
        "    print(\"\\nMean Values by Heart Disease Status:\")\n",
        "    for feature in available_num_features:\n",
        "        no_disease_mean = df_encoded[df_encoded[target_col] == 0][feature].mean()\n",
        "        disease_mean = df_encoded[df_encoded[target_col] == 1][feature].mean()\n",
        "        difference = disease_mean - no_disease_mean\n",
        "        print(f\"  ‚Ä¢ {feature:15s}: No Disease={no_disease_mean:6.1f}, Disease={disease_mean:6.1f}, Diff={difference:+6.1f}\")\n",
        "\n",
        "# ### 4.4 Categorical Features vs Heart Disease\n",
        "\n",
        "\"\"\"\n",
        "Analyze relationship between categorical features and heart disease\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CATEGORICAL FEATURES VS HEART DISEASE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if target_col and len(available_cat_features) > 0:\n",
        "    n_features = min(4, len(available_cat_features))\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, feature in enumerate(available_cat_features[:4]):\n",
        "        crosstab = pd.crosstab(df_clean[feature], df_clean[target_col])\n",
        "        crosstab_pct = pd.crosstab(df_clean[feature], df_clean[target_col], normalize='index') * 100\n",
        "\n",
        "        crosstab.plot(kind='bar', ax=axes[idx], color=['#2ecc71', '#e74c3c'],\n",
        "                     edgecolor='black', alpha=0.8)\n",
        "        axes[idx].set_title(f'{feature} vs Heart Disease', fontweight='bold')\n",
        "        axes[idx].set_xlabel(feature)\n",
        "        axes[idx].set_ylabel('Count')\n",
        "        axes[idx].legend(['No Disease', 'Disease'])\n",
        "        axes[idx].tick_params(axis='x', rotation=45)\n",
        "        axes[idx].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ### 4.5 Correlation Analysis\n",
        "\n",
        "\"\"\"\n",
        "Analyze correlations between all numerical features\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CORRELATION ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Select numerical columns including encoded categoricals\n",
        "numerical_for_corr = df_encoded.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Remove non-predictive columns\n",
        "exclude_cols = [col for col in numerical_for_corr if 'Group' in col and '_encoded' not in col]\n",
        "numerical_for_corr = [col for col in numerical_for_corr if col not in exclude_cols]\n",
        "\n",
        "if len(numerical_for_corr) > 2:\n",
        "    # Calculate correlation matrix\n",
        "    corr_matrix = df_encoded[numerical_for_corr].corr()\n",
        "\n",
        "    # Visualize\n",
        "    plt.figure(figsize=(16, 14))\n",
        "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='RdYlGn', center=0,\n",
        "                square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8},\n",
        "                annot_kws={'size': 8})\n",
        "    plt.title('Correlation Matrix - All Numerical Features', fontsize=16, fontweight='bold', pad=20)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Features most correlated with target\n",
        "    if target_col in corr_matrix.columns:\n",
        "        target_corr = corr_matrix[target_col].drop(target_col).sort_values(ascending=False)\n",
        "\n",
        "        print(\"\\nTop 15 Features Correlated with Heart Disease:\")\n",
        "        print(\"\\nPositive Correlations (increase disease risk):\")\n",
        "        for feature, corr in target_corr[target_corr > 0].head(10).items():\n",
        "            print(f\"   ‚Ä¢ {feature:30s}: {corr:.4f}\")\n",
        "\n",
        "        print(\"\\nNegative Correlations (decrease disease risk):\")\n",
        "        for feature, corr in target_corr[target_corr < 0].head(10).items():\n",
        "            print(f\"   ‚Ä¢ {feature:30s}: {corr:.4f}\")\n",
        "\n",
        "        # Visualize top correlations\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        top_15_corr = pd.concat([target_corr.head(8), target_corr.tail(7)])\n",
        "        colors = ['green' if x > 0 else 'red' for x in top_15_corr.values]\n",
        "        plt.barh(range(len(top_15_corr)), top_15_corr.values, color=colors,\n",
        "                edgecolor='black', alpha=0.8)\n",
        "        plt.yticks(range(len(top_15_corr)), top_15_corr.index)\n",
        "        plt.xlabel('Correlation Coefficient')\n",
        "        plt.title('Top 15 Features Correlated with Heart Disease', fontsize=14, fontweight='bold')\n",
        "        plt.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
        "        plt.gca().invert_yaxis()\n",
        "        plt.grid(axis='x', alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# ---\n",
        "# ## 5. Feature Selection and Preparation\n",
        "# ### 5.1 Prepare Features for Modeling\n",
        "\n",
        "\"\"\"\n",
        "Select final features for machine learning models:\n",
        "- Use encoded versions of categorical features\n",
        "- Remove original categorical columns\n",
        "- Remove intermediate feature engineering columns\n",
        "- Separate features (X) and target (y)\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \""
      ]
    }
  ]
}